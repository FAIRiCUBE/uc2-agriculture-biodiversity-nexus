{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask - GeoPandas Example\n",
    "\n",
    "*Rob Knapen, Wageningen Environmental Research*\n",
    "<br>\n",
    "\n",
    "A notebook for trying out the Dask framework (as alternative to PySpark) with GeoPandas. This could be useful for processing large datasets of species observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "import geopandas as gp\n",
    "import dask_geopandas as dgp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Dask client\n",
    "Get a client for the dummy local Dask 'cluster', and the IP for the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client = Client()\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample species observation data\n",
    "As an example a dataset from the Dutch 'Nationale Databank Flora en Fauna' (ndff.nl). While we are hoping for this to be available as open data (soon), it is not yet. However, we have permission to use it for the FAIRiCUBE EU project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the NDFF datafile to process\n",
    "species_filename = \"../../../local/data/ndff/broedvogels_2016.csv\"\n",
    "\n",
    "# the columns to drop right away\n",
    "always_drop_cols_from_source = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a regular pandas dataframe\n",
    "species_df = pd.read_csv(species_filename, header='infer', sep=';', on_bad_lines='warn')\n",
    "\n",
    "# remove not needed columns\n",
    "if always_drop_cols_from_source:\n",
    "    species_df.drop(columns=always_drop_cols_from_source, inplace=True)\n",
    "\n",
    "# remove the crs prefix from the wkt data\n",
    "species_df['wkt_excl_crs'] = species_df['wkt'].map(lambda x: x.split(';')[1], na_action='ignore')\n",
    "species_df.drop(columns=['wkt'], inplace=True)\n",
    "species_df.rename(columns={'wkt_excl_crs': 'wkt'}, inplace=True)\n",
    "\n",
    "species_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a dask dataframe\n",
    "\n",
    "# read a Dask DataFrame\n",
    "species_dd = dd.read_csv(\n",
    "    species_filename,\n",
    "    header='infer',\n",
    "    sep=';',\n",
    "    on_bad_lines='warn',\n",
    "    dtype={ 'orig_abundance': 'object'} # because of '*' used as abundance value\n",
    ")\n",
    "\n",
    "# remove not needed columns, note that Dask DataFrames are immutable (unlike regular Pandas)\n",
    "if always_drop_cols_from_source:\n",
    "    species_dd = species_dd.drop(columns=always_drop_cols_from_source)\n",
    "\n",
    "# remove the crs prefix from the wkt data\n",
    "# note that map with a custom function needs additional meta info\n",
    "species_dd['wkt_excl_crs'] = species_dd['wkt'].map(\n",
    "    lambda x: x.split(';')[1],\n",
    "    na_action='ignore',\n",
    "    meta=pd.Series(dtype='str'))\n",
    "\n",
    "species_dd = species_dd.drop(columns=['wkt'])\n",
    "species_dd = species_dd.rename(columns={'wkt_excl_crs': 'wkt'})\n",
    "\n",
    "species_dd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is lazy, need to call compute to get the result from a task graph\n",
    "graph = species_dd['sci_name'].value_counts(sort=True, dropna=True)\n",
    "graph.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GeoPandas DataFrame\n",
    "The observations have spatial attributes, so lift them into a GeoPandas DataFrame to be able to process them.\n",
    "\n",
    "Note that there is a dask-geopandas package that bridges Dask with GeoPandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a GeoDataFrame, with the data using the Dutch RD coordinate reference system\n",
    "\n",
    "# note that we used the pandas dataframe\n",
    "gs = gp.GeoSeries.from_wkt(species_df['wkt'])\n",
    "species_gdf = gp.GeoDataFrame(species_df, geometry=gs, crs='EPSG:28992')\n",
    "\n",
    "# transform the dataset to the more common WGS84 (unprojected) CRS\n",
    "species_gdf.to_crs(crs=\"EPSG:4326\", inplace=True)\n",
    "species_gdf.drop(columns=['wkt'], inplace=True)\n",
    "\n",
    "species_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially select observations within an area of interest\n",
    "aoi_gdf = species_gdf.cx[4.0:4.1, 51.0:51.5]\n",
    "aoi_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# note that displaying large datasets in very time-consuming\n",
    "aoi_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dask GeoDataFrame\n",
    "Turn a regular geodataframe into a Dask geodataframe that support lazy graphs computed on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# (hides warning about sending large graph)\n",
    "\n",
    "# create a dask geodataframe\n",
    "species_gdd = dgp.from_geopandas(species_gdf, npartitions=4)\n",
    "species_gdd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# (hides warning about sending large graph)\n",
    "\n",
    "species_gdd['sci_name'].value_counts(sort=True, dropna=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# (hides warning about area calculation on non-projected data)\n",
    "\n",
    "species_gdd.geometry.area.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
