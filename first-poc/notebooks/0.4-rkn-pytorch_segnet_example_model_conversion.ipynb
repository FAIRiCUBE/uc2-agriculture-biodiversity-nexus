{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch EO Semantic Segmentation Example\n",
    "## Step 4: Trained Model Conversion\n",
    "*Rob Knapen, Wageningen Environmental Research*\n",
    "<br>\n",
    "\n",
    "This notebook illustrates two ways to convert a trained PyTorch model into a model that has no Python dependencies and can be used for inference in a different environment.\n",
    "\n",
    "The most common approaches are converting (tracing) the model into TorchScript, and use of the Open Neural Network Exchange (ONNX) format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# setup a path to the trained PyTorch model file\n",
    "content_root_folder = os.path.join('..')\n",
    "\n",
    "# need access to training data to get example inputs\n",
    "hdf5_file = os.path.join(content_root_folder, 'data', 'archives', 'crops_training_data_s2_10m_2018seasons_224x224x28_76cat.hdf5')\n",
    "codes_folder = os.path.join(content_root_folder, 'data', 'raw')\n",
    "codes_file = os.path.join(codes_folder, 'cell_value_to_crop_info_76_classes.csv')\n",
    "\n",
    "base_model_filename = 'rvo_crops_segnet_224x224x28_77classes_100epochs'\n",
    "\n",
    "# the trained model to convert\n",
    "pytorch_trained_model_file = os.path.join(content_root_folder, 'models', base_model_filename + '_model_full.pt')\n",
    "\n",
    "# the output TorchScript model\n",
    "torchscript_trained_model_file = os.path.join(content_root_folder, 'models', base_model_filename + '_model_full_traced.pt')\n",
    "\n",
    "# the output ONNX model\n",
    "onnx_trained_model_file = os.path.join(content_root_folder, 'models', base_model_filename + '_model_full.onnx')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# need the network class definition\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EncoderDecoder network for semantic segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda = 0):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        n_channels, int, number of input channel\n",
    "        encoder_conv_width, int list, size of the feature maps depth for the encoder after each conv\n",
    "        decoder_conv_width, int list, size of the feature maps depth for the decoder after each conv\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(SegNet, self).__init__() # necessary for all classes extending the module class\n",
    "\n",
    "        assert((encoder_conv_width[3] == encoder_conv_width[5]) \\\n",
    "               and (encoder_conv_width[1] == decoder_conv_width[1]))\n",
    "\n",
    "        self.maxpool=nn.MaxPool2d(2,2,return_indices=True) # maxpooling layer\n",
    "        self.unpool=nn.MaxUnpool2d(2,2) # unpooling layer\n",
    "\n",
    "        # encoder\n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, encoder_conv_width[0], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[0]),\n",
    "            nn.ReLU(True))\n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[0], encoder_conv_width[1], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[1]),\n",
    "            nn.ReLU(True))\n",
    "        self.c3 = nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[1], encoder_conv_width[2], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[2]),\n",
    "            nn.ReLU(True))\n",
    "        self.c4 = nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[2], encoder_conv_width[3], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[3]),\n",
    "            nn.ReLU(True))\n",
    "        self.c5 = nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[3], encoder_conv_width[4], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[4]),\n",
    "            nn.ReLU(True))\n",
    "        self.c6 = nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[4], encoder_conv_width[5], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(encoder_conv_width[5]),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        # decoder\n",
    "        self.c7=nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[5] + encoder_conv_width[3], decoder_conv_width[0], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(decoder_conv_width[0]),\n",
    "            nn.ReLU(True))\n",
    "        self.c8=nn.Sequential(\n",
    "            nn.Conv2d(decoder_conv_width[0], decoder_conv_width[1], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(decoder_conv_width[1]),\n",
    "            nn.ReLU(True))\n",
    "        self.c9=nn.Sequential(\n",
    "            nn.Conv2d(encoder_conv_width[1] + decoder_conv_width[1], decoder_conv_width[2], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(decoder_conv_width[2]),\n",
    "            nn.ReLU(True))\n",
    "        self.c10=nn.Sequential(\n",
    "            nn.Conv2d(decoder_conv_width[2], decoder_conv_width[3], 3, padding=1, padding_mode='reflect'),\n",
    "            nn.BatchNorm2d(decoder_conv_width[3]),\n",
    "            nn.ReLU(True)) # for regularisation could add dropout here, nn.Dropout()\n",
    "\n",
    "        # final classifying layer\n",
    "        self.classifier=nn.Conv2d(decoder_conv_width[3], n_class, 3, padding=1, padding_mode='reflect')\n",
    "\n",
    "        # weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        if cuda: # put the model on the GPU memory\n",
    "            self.cuda()\n",
    "\n",
    "    def init_weights(self,layer): # gaussian init for the conv layers\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        the function called to run inference\n",
    "        \"\"\"\n",
    "        # encoder\n",
    "        #level 1\n",
    "        x1 = self.c2(self.c1(input))\n",
    "        x2, indices_a_b = self.maxpool(x1)\n",
    "        #level 2\n",
    "        x3= self.c4(self.c3(x2))\n",
    "        x4, indices_b_c =self.maxpool(x3)\n",
    "        #level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        # decoder\n",
    "        #level 2\n",
    "        y4 = self.unpool(x5, indices_b_c, x2.size())\n",
    "        y3 = self.c8(self.c7(torch.cat((y4,x3), 1)))\n",
    "        # level 1\n",
    "        y2 = self.unpool(y3, indices_a_b, x1.size())\n",
    "        y1 = self.c10(self.c9(torch.cat((y2,x1), 1)))\n",
    "        # output\n",
    "        out = self.classifier(y1)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Attribute, Value:)\n",
      "('description', 'This data set is intended for training deep learning models for crop classification.')\n",
      "('ground_truth', 'Encoded crop types for fields according to the RVO registration in 2018.')\n",
      "('ground_truth_shape', '1,224,224; 76 classes, int16')\n",
      "('observations', '4 Bands (blue, green, red, nir) of 7 seasonal Sentinel 2 images, from 2018.')\n",
      "('observations_shape', '28,224,224; scaled [0-1], float32')\n",
      "('spatial_extent', 'Sample area around the Noordoost polder in The Netherlands.')\n",
      "('spatial_resolution', '10x10m grid cells')\n",
      "('title', 'Crop classification training data set')\n",
      "151 tiles for training, 43 tiles for testing\n"
     ]
    }
   ],
   "source": [
    "# need example data for the conversion\n",
    "data_file = h5py.File(hdf5_file, \"r\")\n",
    "\n",
    "print(\"(Attribute, Value:)\");\n",
    "for item in data_file.attrs.items():\n",
    "    print(item);\n",
    "\n",
    "train_obs = data_file['train_observation'][:]\n",
    "train_gt = data_file['train_groundtruth'][:]\n",
    "test_obs = data_file['test_observation'][:]\n",
    "test_gt = data_file['test_groundtruth'][:]\n",
    "n_train = train_obs.shape[0]\n",
    "n_test = test_obs.shape[0]\n",
    "\n",
    "print(\"%d tiles for training, %d tiles for testing\" % (n_train, n_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "SegNet(\n  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n  (c1): Sequential(\n    (0): Conv2d(28, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c2): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c3): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c4): Sequential(\n    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c5): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c6): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c7): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c8): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c9): Sequential(\n    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (c10): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (classifier): Conv2d(64, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the entire model (on CPU)\n",
    "model = torch.load(pytorch_trained_model_file, map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TorchScript Conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "SegNet(\n  original_name=SegNet\n  (maxpool): MaxPool2d(original_name=MaxPool2d)\n  (unpool): MaxUnpool2d(original_name=MaxUnpool2d)\n  (c1): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c2): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c3): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c4): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c5): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c6): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c7): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c8): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c9): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (c10): Sequential(\n    original_name=Sequential\n    (0): Conv2d(original_name=Conv2d)\n    (1): BatchNorm2d(original_name=BatchNorm2d)\n    (2): ReLU(original_name=ReLU)\n  )\n  (classifier): Conv2d(original_name=Conv2d)\n)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the trained model to TorchScript\n",
    "\n",
    "# create a tensor with example input data\n",
    "tile_index = 1\n",
    "tile = train_obs[tile_index,:,:,:].transpose(2,0,1)\n",
    "tile_t = torch.from_numpy(tile).unsqueeze(dim=0)\n",
    "\n",
    "# trace the model to a TorchScript version\n",
    "traced_model = torch.jit.trace(model, tile_t)\n",
    "traced_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('c1.0.weight',\n              tensor([[[[ 0.0543, -0.0334, -0.0157],\n                        [ 0.0239,  0.0966,  0.0349],\n                        [ 0.0629, -0.1413, -0.0657]],\n              \n                       [[ 0.1023, -0.0224, -0.0558],\n                        [-0.1220, -0.0497, -0.1010],\n                        [-0.1012, -0.0448, -0.0014]],\n              \n                       [[ 0.0176,  0.0130,  0.0839],\n                        [ 0.0128, -0.0361,  0.1166],\n                        [-0.0032,  0.0057, -0.0356]],\n              \n                       ...,\n              \n                       [[ 0.0121,  0.0290, -0.0348],\n                        [-0.0173,  0.0434, -0.0268],\n                        [-0.0860, -0.0691, -0.0090]],\n              \n                       [[-0.0570, -0.0615, -0.0435],\n                        [-0.0086, -0.0417,  0.0436],\n                        [ 0.0361, -0.0283,  0.0531]],\n              \n                       [[ 0.0164, -0.0293, -0.0069],\n                        [ 0.0131, -0.0651,  0.0535],\n                        [ 0.0341,  0.0294, -0.0240]]],\n              \n              \n                      [[[ 0.0448, -0.0967, -0.0265],\n                        [-0.0715,  0.0242,  0.0202],\n                        [ 0.0423,  0.0481,  0.0535]],\n              \n                       [[ 0.0719,  0.0383, -0.0285],\n                        [ 0.1618, -0.0959,  0.0162],\n                        [ 0.0090,  0.0092,  0.0514]],\n              \n                       [[-0.0331, -0.0411, -0.0069],\n                        [ 0.0711,  0.0650,  0.0143],\n                        [ 0.0457,  0.0011,  0.0248]],\n              \n                       ...,\n              \n                       [[-0.1467,  0.0957, -0.0148],\n                        [-0.1025, -0.0624, -0.0067],\n                        [ 0.0025, -0.0899, -0.1438]],\n              \n                       [[ 0.1443, -0.1546,  0.0166],\n                        [ 0.0858, -0.0064,  0.0729],\n                        [-0.0484, -0.0256, -0.0035]],\n              \n                       [[-0.0586, -0.0839,  0.0352],\n                        [ 0.0411, -0.0245, -0.1048],\n                        [ 0.0802,  0.0373,  0.1024]]],\n              \n              \n                      [[[-0.0167,  0.0541, -0.0440],\n                        [ 0.1732, -0.0344,  0.0494],\n                        [ 0.0272, -0.0279,  0.0062]],\n              \n                       [[ 0.0219, -0.0498, -0.0026],\n                        [-0.0564,  0.0451, -0.0490],\n                        [-0.0622,  0.1032,  0.0732]],\n              \n                       [[ 0.0279,  0.0834, -0.0880],\n                        [-0.0742,  0.0735,  0.1170],\n                        [-0.0240,  0.0745,  0.0751]],\n              \n                       ...,\n              \n                       [[ 0.0454,  0.0143, -0.0364],\n                        [ 0.0014,  0.0126, -0.0865],\n                        [-0.1015, -0.0595,  0.0401]],\n              \n                       [[-0.0382, -0.0434,  0.0209],\n                        [ 0.0042, -0.0866,  0.0444],\n                        [ 0.0041, -0.0871, -0.0180]],\n              \n                       [[-0.0166, -0.0026, -0.0426],\n                        [-0.0644,  0.1106,  0.0264],\n                        [-0.0466, -0.0650, -0.0006]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0703, -0.0538,  0.0099],\n                        [-0.0207, -0.0380, -0.0701],\n                        [-0.0404,  0.0450,  0.0866]],\n              \n                       [[-0.0291, -0.0240,  0.0858],\n                        [ 0.1147,  0.0266,  0.0735],\n                        [-0.0126, -0.0130, -0.0234]],\n              \n                       [[-0.0328,  0.0654,  0.1051],\n                        [-0.0120, -0.0826, -0.0018],\n                        [-0.0372,  0.0170, -0.0019]],\n              \n                       ...,\n              \n                       [[-0.0586, -0.1124, -0.1389],\n                        [ 0.0279, -0.0076, -0.0168],\n                        [-0.0517, -0.0726, -0.0453]],\n              \n                       [[ 0.0826,  0.0903,  0.0159],\n                        [-0.0192,  0.0511, -0.0061],\n                        [-0.0042,  0.0726,  0.1144]],\n              \n                       [[ 0.0247,  0.0303, -0.0336],\n                        [-0.0503,  0.0403, -0.0522],\n                        [ 0.0332,  0.0521,  0.0188]]],\n              \n              \n                      [[[ 0.0650, -0.0995,  0.0652],\n                        [ 0.0073, -0.0565,  0.0198],\n                        [-0.0871,  0.0138,  0.0383]],\n              \n                       [[-0.0354, -0.0368, -0.0783],\n                        [-0.1812, -0.0073, -0.1054],\n                        [-0.0075, -0.0838, -0.0245]],\n              \n                       [[-0.0388,  0.0246,  0.0390],\n                        [ 0.0527, -0.1049,  0.0088],\n                        [-0.0262,  0.0519,  0.0203]],\n              \n                       ...,\n              \n                       [[-0.0040, -0.0203, -0.0275],\n                        [ 0.0310,  0.0271,  0.1675],\n                        [ 0.0533, -0.0192,  0.0406]],\n              \n                       [[ 0.0531, -0.0576, -0.0634],\n                        [ 0.0176,  0.0871,  0.0310],\n                        [ 0.0296,  0.0642, -0.0778]],\n              \n                       [[ 0.0157, -0.0367, -0.0278],\n                        [-0.0375, -0.0771, -0.0365],\n                        [ 0.0674,  0.0284,  0.0387]]],\n              \n              \n                      [[[ 0.0089,  0.0471, -0.0487],\n                        [-0.0624, -0.0488, -0.0173],\n                        [ 0.1138, -0.0721, -0.0655]],\n              \n                       [[ 0.0009, -0.0202, -0.0709],\n                        [-0.0570,  0.0298,  0.0397],\n                        [ 0.0442,  0.0216, -0.0870]],\n              \n                       [[-0.0353,  0.0556, -0.0943],\n                        [ 0.0597,  0.0272,  0.0251],\n                        [ 0.0005, -0.0255, -0.0878]],\n              \n                       ...,\n              \n                       [[-0.1195, -0.0258, -0.1331],\n                        [-0.0677, -0.0022, -0.1024],\n                        [ 0.0236, -0.0926, -0.0203]],\n              \n                       [[ 0.0246,  0.0971,  0.0356],\n                        [ 0.1514,  0.0599,  0.1133],\n                        [ 0.0331,  0.0265, -0.0585]],\n              \n                       [[-0.0275, -0.0144, -0.1470],\n                        [-0.1246, -0.1054, -0.0229],\n                        [ 0.0148, -0.0695, -0.0096]]]])),\n             ('c1.0.bias',\n              tensor([ 0.0010, -0.0518,  0.0370,  0.0423, -0.0071, -0.0594,  0.0354,  0.0549,\n                       0.0258,  0.0104,  0.0187, -0.0019,  0.0013,  0.0077,  0.0225, -0.0145,\n                       0.0421, -0.0048,  0.0479, -0.0208,  0.0227, -0.0188,  0.0237, -0.0156,\n                       0.0195,  0.0258,  0.0331, -0.0212, -0.0120,  0.0340, -0.0367,  0.0652,\n                      -0.0064,  0.0504, -0.0248,  0.0234, -0.0603,  0.0018, -0.0403, -0.0248,\n                      -0.0229,  0.0122,  0.0181,  0.0637, -0.0590, -0.0456,  0.0521, -0.0312,\n                       0.0421, -0.0233, -0.0269,  0.0278,  0.0319,  0.0180, -0.0146,  0.0324,\n                      -0.0604,  0.0709, -0.0519, -0.0397,  0.0513,  0.0185, -0.0420, -0.0186])),\n             ('c1.1.weight',\n              tensor([1.0470, 0.9714, 1.0098, 0.9967, 1.0152, 1.0052, 0.9984, 1.0216, 0.9984,\n                      0.9934, 1.0265, 0.9673, 1.0240, 0.9822, 1.0246, 1.0146, 0.9866, 1.0087,\n                      1.0160, 1.0062, 0.9575, 1.0063, 1.0086, 1.0132, 1.0198, 0.9999, 0.9921,\n                      0.9936, 1.0232, 0.9865, 1.0104, 1.0009, 0.9793, 0.9884, 1.0169, 1.0203,\n                      1.0073, 0.9879, 1.0024, 0.9561, 0.9498, 0.9850, 1.0060, 0.9633, 0.9924,\n                      1.0132, 0.9729, 1.0049, 0.9927, 0.9936, 0.9998, 1.0380, 0.9899, 0.9944,\n                      0.9909, 0.9989, 0.9745, 1.0331, 1.0122, 0.9855, 1.0118, 0.9783, 0.9592,\n                      0.9878])),\n             ('c1.1.bias',\n              tensor([ 0.0165, -0.0117, -0.0081,  0.0110,  0.0712, -0.0397, -0.0164, -0.0203,\n                      -0.0024,  0.0153, -0.0054,  0.0002,  0.0049,  0.0305,  0.0405,  0.0265,\n                       0.0096,  0.0377, -0.0063, -0.0102, -0.0379, -0.0071,  0.0324, -0.0176,\n                       0.0318, -0.0078, -0.0139, -0.0104,  0.0124, -0.0063,  0.0069, -0.0109,\n                      -0.0199,  0.0035,  0.0609, -0.0121,  0.0181, -0.0088, -0.0010, -0.0180,\n                      -0.0099,  0.0040,  0.0038,  0.0078,  0.0594,  0.0413, -0.0061,  0.0210,\n                       0.0489, -0.0098, -0.0123,  0.0299, -0.0047,  0.0439,  0.0180, -0.0262,\n                       0.0012,  0.0174, -0.0151,  0.0222,  0.0305,  0.0227, -0.0354,  0.0161])),\n             ('c1.1.running_mean',\n              tensor([-0.0048, -0.1441,  0.0353,  0.0060, -0.0125, -0.1097,  0.2567,  0.1443,\n                       0.0154, -0.0419, -0.0221, -0.1319, -0.0297, -0.0211,  0.0459, -0.0707,\n                       0.0129, -0.0159,  0.0109,  0.3620, -0.0637,  0.0924, -0.0185,  0.0535,\n                       0.0691,  0.0622, -0.0010, -0.0742, -0.0348,  0.0436,  0.0719,  0.0281,\n                      -0.0323,  0.0257, -0.0892,  0.0054, -0.0682, -0.0228, -0.0774, -0.1127,\n                      -0.0611,  0.0098, -0.0245,  0.0848,  0.0023,  0.0067,  0.0149,  0.0088,\n                       0.0809, -0.1107, -0.0747,  0.0009,  0.0464,  0.0602,  0.0819, -0.0038,\n                      -0.1106,  0.0632, -0.1071, -0.0472,  0.0371,  0.0021, -0.1386, -0.0279])),\n             ('c1.1.running_var',\n              tensor([0.0004, 0.0022, 0.0014, 0.0034, 0.0014, 0.0034, 0.0046, 0.0014, 0.0015,\n                      0.0026, 0.0025, 0.0039, 0.0016, 0.0014, 0.0010, 0.0021, 0.0017, 0.0007,\n                      0.0024, 0.0125, 0.0019, 0.0017, 0.0020, 0.0010, 0.0016, 0.0009, 0.0036,\n                      0.0012, 0.0033, 0.0008, 0.0018, 0.0033, 0.0034, 0.0026, 0.0023, 0.0014,\n                      0.0020, 0.0015, 0.0022, 0.0022, 0.0022, 0.0007, 0.0016, 0.0019, 0.0013,\n                      0.0009, 0.0015, 0.0012, 0.0016, 0.0024, 0.0017, 0.0021, 0.0013, 0.0003,\n                      0.0021, 0.0032, 0.0016, 0.0026, 0.0016, 0.0018, 0.0010, 0.0005, 0.0013,\n                      0.0019])),\n             ('c1.1.num_batches_tracked', tensor(3700)),\n             ('c2.0.weight',\n              tensor([[[[-0.0211, -0.0291,  0.0552],\n                        [ 0.0234, -0.1340, -0.0770],\n                        [ 0.0091, -0.0963,  0.0014]],\n              \n                       [[ 0.0373, -0.0732, -0.0072],\n                        [-0.0590,  0.1132, -0.0618],\n                        [-0.0144,  0.0248, -0.0957]],\n              \n                       [[-0.1054,  0.0426,  0.0238],\n                        [-0.0354, -0.0285,  0.0740],\n                        [ 0.0716, -0.1064, -0.0580]],\n              \n                       ...,\n              \n                       [[-0.0085,  0.0358,  0.0028],\n                        [ 0.0218, -0.0633,  0.0183],\n                        [ 0.1048, -0.0359,  0.0490]],\n              \n                       [[ 0.0346,  0.0382, -0.0451],\n                        [-0.0050,  0.1012,  0.0261],\n                        [ 0.0612,  0.0591,  0.0521]],\n              \n                       [[-0.1751, -0.0169,  0.0974],\n                        [ 0.1282,  0.0714, -0.0404],\n                        [ 0.0910,  0.0801, -0.1251]]],\n              \n              \n                      [[[ 0.0748, -0.1138, -0.1317],\n                        [-0.0356, -0.0361, -0.1114],\n                        [ 0.0913, -0.0915,  0.0124]],\n              \n                       [[-0.1172,  0.0169,  0.0539],\n                        [ 0.0306,  0.0406,  0.0524],\n                        [ 0.0187, -0.0623, -0.0383]],\n              \n                       [[-0.0027,  0.0259,  0.0910],\n                        [-0.0375, -0.0441, -0.0071],\n                        [-0.0393,  0.0273, -0.0337]],\n              \n                       ...,\n              \n                       [[-0.0557,  0.0127, -0.0193],\n                        [-0.0695,  0.0487,  0.0620],\n                        [ 0.0767,  0.0247, -0.0779]],\n              \n                       [[ 0.0046,  0.0246,  0.0178],\n                        [ 0.0642,  0.0229,  0.0514],\n                        [ 0.1375,  0.0298,  0.0593]],\n              \n                       [[ 0.1057, -0.0815, -0.0161],\n                        [ 0.0700,  0.0302,  0.0578],\n                        [-0.0620, -0.0130,  0.0055]]],\n              \n              \n                      [[[ 0.0376, -0.1309,  0.0223],\n                        [-0.0619,  0.0358, -0.1746],\n                        [-0.0608, -0.0577, -0.0333]],\n              \n                       [[ 0.0786, -0.0417,  0.0166],\n                        [ 0.0947,  0.0326,  0.0708],\n                        [ 0.0380, -0.0204,  0.0314]],\n              \n                       [[ 0.0241, -0.0979,  0.0012],\n                        [ 0.0381, -0.0758, -0.1077],\n                        [-0.0967,  0.0626, -0.0479]],\n              \n                       ...,\n              \n                       [[ 0.0247, -0.0002,  0.0742],\n                        [ 0.0836, -0.0043,  0.0447],\n                        [ 0.0237,  0.0583,  0.0749]],\n              \n                       [[ 0.1200, -0.0501, -0.0156],\n                        [-0.0213,  0.0229,  0.0065],\n                        [ 0.0107,  0.1365, -0.1115]],\n              \n                       [[-0.0625,  0.0230, -0.0108],\n                        [ 0.0019, -0.0676,  0.0693],\n                        [ 0.0157,  0.0552,  0.0715]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0359, -0.0699,  0.0105],\n                        [-0.0737, -0.0394,  0.0746],\n                        [ 0.0132,  0.0070, -0.0753]],\n              \n                       [[-0.1175, -0.0839,  0.0163],\n                        [-0.0118,  0.0403, -0.0025],\n                        [-0.0107,  0.0110,  0.0185]],\n              \n                       [[-0.0574,  0.0299, -0.0179],\n                        [-0.0242,  0.0293,  0.0084],\n                        [ 0.0325,  0.0027, -0.0153]],\n              \n                       ...,\n              \n                       [[-0.0431,  0.1287, -0.0260],\n                        [ 0.0652, -0.0428, -0.0044],\n                        [-0.0071, -0.0777, -0.0564]],\n              \n                       [[-0.0002,  0.0222, -0.0103],\n                        [ 0.1029,  0.0195, -0.0056],\n                        [-0.0455,  0.0364, -0.0764]],\n              \n                       [[-0.0263,  0.0217, -0.0020],\n                        [ 0.0063,  0.1001,  0.1387],\n                        [-0.0235, -0.0567,  0.0209]]],\n              \n              \n                      [[[ 0.0013, -0.0011, -0.0066],\n                        [-0.0058,  0.0317, -0.0202],\n                        [ 0.0223, -0.0585, -0.0535]],\n              \n                       [[-0.0870,  0.0183, -0.2028],\n                        [-0.0310,  0.0296, -0.0481],\n                        [ 0.0078, -0.0678, -0.1148]],\n              \n                       [[ 0.0389, -0.0116,  0.0453],\n                        [-0.0332, -0.0878,  0.0035],\n                        [-0.0090,  0.0226, -0.0223]],\n              \n                       ...,\n              \n                       [[-0.0287, -0.1068, -0.0657],\n                        [-0.0087,  0.0702,  0.0684],\n                        [ 0.0542,  0.0122, -0.0211]],\n              \n                       [[ 0.0064, -0.0750, -0.0151],\n                        [ 0.0162, -0.0074, -0.0700],\n                        [ 0.0077,  0.0334, -0.0686]],\n              \n                       [[ 0.0179, -0.0121,  0.0100],\n                        [ 0.0134,  0.0056,  0.0110],\n                        [ 0.0076, -0.0344,  0.0614]]],\n              \n              \n                      [[[ 0.0130, -0.1494, -0.0371],\n                        [-0.0462, -0.0239, -0.0678],\n                        [-0.1480,  0.0411, -0.0693]],\n              \n                       [[-0.0405,  0.0231,  0.0130],\n                        [-0.0339,  0.0548, -0.0422],\n                        [ 0.1202, -0.0661, -0.1128]],\n              \n                       [[ 0.0643, -0.0351,  0.0879],\n                        [ 0.0350, -0.0166,  0.1229],\n                        [ 0.1066,  0.0332,  0.0287]],\n              \n                       ...,\n              \n                       [[ 0.0036, -0.0136,  0.0551],\n                        [ 0.0771,  0.0382, -0.0168],\n                        [ 0.0726,  0.0523,  0.0366]],\n              \n                       [[-0.0101, -0.0473, -0.0095],\n                        [-0.0017, -0.0122, -0.0293],\n                        [-0.1016, -0.0228, -0.0130]],\n              \n                       [[-0.0136,  0.0825,  0.1166],\n                        [ 0.0049,  0.0132, -0.0179],\n                        [-0.0486,  0.0102,  0.1177]]]])),\n             ('c2.0.bias',\n              tensor([-9.4469e-03, -1.1215e-02, -4.2832e-02,  3.0522e-02, -3.4475e-02,\n                       2.0285e-02, -8.9161e-03,  1.3968e-02,  3.4473e-02, -8.0694e-03,\n                      -3.3773e-02,  2.9231e-03, -1.8400e-03, -1.0418e-02, -1.7573e-02,\n                      -7.7026e-03, -9.4260e-03, -1.5848e-02, -1.0502e-02, -9.5962e-03,\n                       3.8965e-02, -5.7737e-03, -2.9620e-02, -4.3255e-02, -3.2955e-02,\n                      -4.0821e-02, -3.0015e-02,  5.7062e-03,  3.4778e-02, -2.9638e-02,\n                       1.6689e-02,  4.4430e-02, -9.1404e-03,  2.5369e-02, -1.3952e-02,\n                       2.3589e-02,  8.2799e-03, -3.4685e-03,  3.1834e-02,  5.4834e-03,\n                       7.7853e-03, -1.3672e-02,  3.2402e-02,  7.8723e-05,  4.4977e-03,\n                       3.3098e-02,  1.4823e-02,  1.6484e-02, -3.0239e-02,  2.3128e-02,\n                       6.7727e-03,  1.6316e-02,  1.7659e-02, -2.7675e-02,  2.7292e-02,\n                      -1.0075e-02, -1.4207e-02, -3.6841e-02,  6.8312e-03,  1.4491e-02,\n                      -2.6966e-02,  2.6780e-02,  1.2111e-02, -4.2730e-02])),\n             ('c2.1.weight',\n              tensor([0.9792, 1.0061, 0.9969, 0.9963, 1.0207, 1.0176, 0.9893, 0.9947, 0.9924,\n                      1.0049, 1.0079, 0.9742, 0.9794, 0.9985, 0.9970, 0.9871, 0.9546, 0.9719,\n                      0.9771, 0.9808, 1.0031, 0.9855, 1.0150, 0.9987, 0.9854, 0.9961, 1.0150,\n                      0.9896, 1.0010, 0.9908, 0.9994, 0.9909, 0.9947, 1.0048, 0.9732, 0.9946,\n                      0.9922, 1.0156, 1.0046, 1.0192, 0.9863, 1.0164, 1.0160, 0.9896, 0.9854,\n                      0.9855, 0.9790, 0.9819, 1.0124, 0.9806, 0.9835, 0.9904, 0.9932, 1.0146,\n                      0.9775, 0.9897, 0.9954, 0.9956, 1.0028, 1.0008, 1.0244, 1.0006, 1.0082,\n                      0.9933])),\n             ('c2.1.bias',\n              tensor([-2.1000e-02, -6.3592e-03,  4.1303e-03,  2.8183e-03,  1.7312e-02,\n                      -2.3946e-02,  3.1937e-03, -2.4926e-02, -8.6352e-03, -1.2493e-02,\n                      -7.5464e-03, -5.1081e-02, -4.6446e-02,  1.4580e-02, -2.7612e-03,\n                      -1.5989e-03, -4.6922e-02, -3.0382e-02, -1.6653e-02, -5.1163e-02,\n                       5.2818e-03, -2.4610e-02, -3.1562e-03,  2.6028e-03, -3.1070e-02,\n                       1.9815e-02,  3.8899e-03,  5.7227e-05, -8.5132e-03, -3.0208e-02,\n                       4.3707e-02, -2.3718e-03, -1.2558e-02, -3.7085e-03, -4.9879e-02,\n                      -6.2652e-03,  3.3845e-03,  2.4339e-02,  1.0587e-02,  2.6856e-02,\n                       1.0660e-02,  8.1825e-03, -1.2959e-02, -5.1964e-03, -8.5980e-03,\n                       4.0463e-02, -3.0068e-02, -4.4612e-02,  2.7505e-03, -2.8921e-02,\n                       3.8837e-03,  6.7556e-04, -2.6650e-02, -1.6770e-02, -6.9256e-03,\n                      -3.2912e-02,  3.2969e-02,  4.9351e-03, -1.8202e-02,  3.1485e-02,\n                       5.9353e-03, -2.2667e-02, -1.2837e-02,  3.6677e-03])),\n             ('c2.1.running_mean',\n              tensor([ 0.1037,  0.0570, -0.6120, -0.1774, -0.8901,  0.9675,  0.7867, -0.6251,\n                      -0.6177,  1.0809, -0.8514, -0.6101, -0.5449, -1.9643, -0.9919, -2.0829,\n                       0.7910, -0.5227,  0.1587, -0.3899,  0.3309,  1.6143,  0.7184, -0.6645,\n                       0.4343, -0.3417, -0.9242,  0.3701,  1.2105, -0.4728, -1.0263,  1.4184,\n                      -1.2405, -0.3028,  0.1530,  0.0892, -0.7729, -0.5375, -0.2789, -1.6069,\n                      -0.3503,  0.2912, -0.5728, -1.2430, -0.7415,  0.2615, -0.1848,  1.8934,\n                      -0.7306, -0.3952, -0.2027,  1.0596, -0.0430, -0.2335,  0.0572,  1.4786,\n                      -0.1655,  0.1075,  0.3702,  0.4894, -1.5096,  0.2042, -0.7720, -0.2172])),\n             ('c2.1.running_var',\n              tensor([1.1008, 1.3366, 1.5871, 0.8379, 1.1233, 1.4630, 1.1284, 2.2135, 1.4481,\n                      1.5853, 0.9658, 1.5299, 1.4703, 2.0495, 1.4083, 1.6904, 0.9771, 0.8948,\n                      0.9820, 1.0599, 1.3542, 1.6102, 1.8824, 1.5495, 1.0574, 0.7791, 2.2385,\n                      1.3447, 2.3544, 1.3071, 1.2999, 1.0685, 1.0607, 1.3036, 1.1186, 1.0202,\n                      2.0071, 1.5284, 0.9690, 1.6548, 0.6959, 2.0997, 1.9486, 1.9677, 1.1070,\n                      1.1967, 0.9059, 1.6258, 1.1532, 1.1755, 0.8340, 1.3485, 1.0158, 1.9151,\n                      0.9730, 1.3415, 1.0157, 1.6765, 0.9456, 1.4887, 1.3461, 1.5708, 1.3503,\n                      0.6923])),\n             ('c2.1.num_batches_tracked', tensor(3700)),\n             ('c3.0.weight',\n              tensor([[[[ 0.0112,  0.0912,  0.0342],\n                        [-0.0108,  0.0171,  0.0585],\n                        [ 0.0823, -0.0236,  0.0109]],\n              \n                       [[ 0.0069, -0.0091,  0.0296],\n                        [-0.0605, -0.0190, -0.0508],\n                        [-0.0568, -0.0713, -0.0075]],\n              \n                       [[ 0.0098,  0.0500, -0.0488],\n                        [ 0.0138, -0.0325, -0.0417],\n                        [-0.0052, -0.0331,  0.0032]],\n              \n                       ...,\n              \n                       [[ 0.0013,  0.0235,  0.0771],\n                        [ 0.0103,  0.0200,  0.0123],\n                        [-0.0907, -0.0203, -0.0468]],\n              \n                       [[-0.0371, -0.0259,  0.0504],\n                        [-0.0480,  0.0255,  0.0094],\n                        [-0.0258,  0.0288,  0.0169]],\n              \n                       [[-0.0476, -0.0667, -0.0632],\n                        [-0.0548, -0.0148,  0.0043],\n                        [-0.0673, -0.1089,  0.0125]]],\n              \n              \n                      [[[-0.0038, -0.0318, -0.0270],\n                        [-0.0234, -0.0555,  0.0591],\n                        [-0.0618, -0.0457, -0.0478]],\n              \n                       [[ 0.0240,  0.0335,  0.0091],\n                        [ 0.0136,  0.0493,  0.0586],\n                        [-0.0026,  0.0059,  0.0590]],\n              \n                       [[ 0.0179, -0.0273, -0.0337],\n                        [-0.0084, -0.0573, -0.0195],\n                        [ 0.0032,  0.0424, -0.0063]],\n              \n                       ...,\n              \n                       [[ 0.0256,  0.0190,  0.0251],\n                        [-0.0604, -0.0568,  0.0470],\n                        [ 0.0484,  0.0271, -0.0219]],\n              \n                       [[ 0.0109,  0.0421,  0.0563],\n                        [ 0.0273,  0.0136,  0.0583],\n                        [ 0.1142, -0.0580,  0.0429]],\n              \n                       [[-0.1181, -0.0388, -0.0229],\n                        [-0.0588, -0.0583,  0.0437],\n                        [ 0.0434, -0.0676,  0.0284]]],\n              \n              \n                      [[[-0.0052,  0.0513, -0.0098],\n                        [-0.0347,  0.0452,  0.0832],\n                        [-0.0027, -0.0377,  0.0017]],\n              \n                       [[-0.0480,  0.0072,  0.0230],\n                        [-0.0418, -0.0003,  0.0524],\n                        [-0.0485,  0.0457, -0.0536]],\n              \n                       [[-0.0328,  0.0508,  0.0186],\n                        [-0.0176,  0.0391, -0.0331],\n                        [ 0.0493, -0.0417, -0.0207]],\n              \n                       ...,\n              \n                       [[-0.0203,  0.0486,  0.0151],\n                        [ 0.0040, -0.0641,  0.0224],\n                        [ 0.0237, -0.0262, -0.0244]],\n              \n                       [[-0.0064,  0.0246,  0.0323],\n                        [ 0.0699, -0.0296,  0.0284],\n                        [-0.0082,  0.0797, -0.0074]],\n              \n                       [[-0.0011, -0.0113, -0.0099],\n                        [-0.0411, -0.0474, -0.0741],\n                        [ 0.0012, -0.0415, -0.0664]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0135,  0.0038,  0.0426],\n                        [ 0.0979,  0.0578, -0.0290],\n                        [-0.0323,  0.0082,  0.0184]],\n              \n                       [[-0.0598, -0.0632, -0.0065],\n                        [ 0.0865, -0.0072, -0.0647],\n                        [ 0.0142,  0.0125,  0.0313]],\n              \n                       [[-0.0457, -0.0803, -0.0463],\n                        [ 0.0040,  0.0022, -0.0060],\n                        [-0.0134,  0.0271,  0.0192]],\n              \n                       ...,\n              \n                       [[ 0.0582,  0.0254,  0.1075],\n                        [ 0.0502, -0.0370,  0.0220],\n                        [-0.0322, -0.0229, -0.0060]],\n              \n                       [[-0.0765, -0.0358, -0.0279],\n                        [-0.0155, -0.0146, -0.1407],\n                        [-0.0468, -0.0383, -0.0406]],\n              \n                       [[ 0.0569,  0.0729,  0.0355],\n                        [ 0.1009,  0.0246,  0.0327],\n                        [-0.0178, -0.0205,  0.0101]]],\n              \n              \n                      [[[ 0.0158,  0.0390,  0.0769],\n                        [ 0.0353,  0.0271,  0.0558],\n                        [-0.0228,  0.0268,  0.0082]],\n              \n                       [[-0.0053, -0.0182,  0.0095],\n                        [-0.0803, -0.0339, -0.0320],\n                        [-0.0223,  0.0067,  0.0108]],\n              \n                       [[ 0.0613,  0.0527,  0.0648],\n                        [-0.0071,  0.0056,  0.0101],\n                        [-0.0238,  0.0222,  0.0265]],\n              \n                       ...,\n              \n                       [[ 0.0020,  0.0832, -0.0111],\n                        [-0.0378,  0.0375,  0.0047],\n                        [-0.0170, -0.0246, -0.0566]],\n              \n                       [[-0.1317, -0.0818, -0.0378],\n                        [-0.0365, -0.0297, -0.0062],\n                        [ 0.0215, -0.0134, -0.0235]],\n              \n                       [[ 0.0532,  0.0142,  0.0239],\n                        [ 0.0237,  0.0846,  0.0014],\n                        [-0.0307, -0.0457, -0.0105]]],\n              \n              \n                      [[[-0.0267, -0.0909, -0.0510],\n                        [ 0.0322, -0.0670,  0.0406],\n                        [-0.0917, -0.0181,  0.0199]],\n              \n                       [[-0.0005, -0.0601, -0.0360],\n                        [-0.0563, -0.0834, -0.0459],\n                        [-0.0723, -0.0481,  0.0177]],\n              \n                       [[ 0.0857, -0.0101, -0.0376],\n                        [ 0.0130,  0.0953, -0.0625],\n                        [ 0.0310, -0.0076,  0.0036]],\n              \n                       ...,\n              \n                       [[ 0.0366, -0.0867, -0.0100],\n                        [-0.0254, -0.0109,  0.0178],\n                        [-0.0313,  0.0160,  0.0486]],\n              \n                       [[-0.0080,  0.0088,  0.0615],\n                        [ 0.0613, -0.0262,  0.0125],\n                        [ 0.0635,  0.0940,  0.1183]],\n              \n                       [[-0.0182,  0.0770, -0.0265],\n                        [-0.0724, -0.0143,  0.0707],\n                        [-0.0114, -0.0502, -0.0472]]]])),\n             ('c3.0.bias',\n              tensor([-7.5764e-03, -2.0452e-02,  1.0613e-02, -1.8465e-02,  2.5722e-02,\n                      -3.5619e-02,  2.4753e-02, -3.3147e-02,  2.2821e-03, -3.2206e-02,\n                       2.1548e-02, -3.3207e-02, -3.8349e-02,  3.3728e-02,  1.3340e-02,\n                      -2.2717e-02,  2.5590e-02,  1.4257e-02,  3.1028e-02, -3.7618e-02,\n                       2.3594e-02, -1.3086e-02, -3.0545e-02,  3.1545e-02,  2.3183e-02,\n                      -5.9463e-03, -8.2472e-03,  2.5483e-02, -3.6474e-03,  4.5065e-06,\n                       9.8618e-03, -1.3698e-02,  3.0731e-02, -6.7290e-03, -2.5466e-02,\n                       1.7899e-02,  9.5309e-03,  1.4451e-02,  3.0557e-02, -3.9936e-02,\n                       2.9133e-02, -1.2813e-02, -1.2694e-02, -4.2074e-02, -2.3388e-02,\n                      -1.3899e-02,  3.3501e-02, -2.4353e-02, -4.2029e-02, -3.6809e-02,\n                       2.5778e-02, -1.3065e-02,  2.7365e-02, -2.6371e-02,  3.2120e-02,\n                       1.1927e-02,  4.1022e-02, -6.3977e-03,  3.7530e-02, -3.4949e-03,\n                      -5.8284e-03,  5.8612e-03,  3.3447e-02,  2.0490e-02,  3.6550e-02,\n                       3.5000e-02, -3.6755e-02, -2.8897e-02, -1.2912e-03, -2.1494e-02,\n                       1.5954e-02,  3.1043e-02,  2.6403e-02,  3.8934e-02, -3.1545e-02,\n                       3.5158e-02,  3.5162e-02,  4.1397e-02, -9.7847e-03,  1.2739e-02,\n                       3.2670e-02,  1.5206e-02, -2.2641e-02,  2.6197e-02,  6.7060e-03,\n                       3.8748e-02,  9.2690e-03,  1.4176e-02, -2.1100e-02,  8.0674e-03,\n                      -2.8413e-02, -2.9349e-02, -1.2839e-02, -2.4368e-02,  3.8552e-02,\n                       1.8547e-02,  2.1403e-02, -8.1359e-03,  4.2737e-03,  2.1055e-02,\n                       6.5797e-03, -3.0423e-02,  1.7041e-02, -1.2070e-02, -5.0100e-03,\n                      -2.8736e-02,  1.5528e-02,  2.8405e-02, -3.7613e-02,  8.9944e-03,\n                      -3.6839e-02,  1.6815e-02,  2.0838e-02,  1.9666e-02,  3.6586e-02,\n                      -1.2318e-02,  1.7993e-02,  8.2950e-03, -3.0493e-02, -1.6752e-03,\n                       3.0689e-02,  2.5524e-02,  2.1895e-02, -1.5388e-02, -4.0634e-02,\n                      -3.6460e-02, -1.2072e-02, -2.8207e-02])),\n             ('c3.1.weight',\n              tensor([0.9854, 0.9996, 1.0234, 1.0135, 1.0103, 0.9720, 0.9992, 1.0003, 1.0035,\n                      1.0157, 0.9974, 1.0000, 1.0262, 0.9838, 0.9918, 0.9945, 1.0100, 0.9790,\n                      1.0130, 1.0253, 1.0339, 1.0099, 1.0002, 0.9834, 1.0011, 0.9852, 0.9945,\n                      0.9736, 0.9810, 1.0112, 1.0078, 1.0083, 0.9947, 1.0011, 0.9973, 0.9946,\n                      1.0248, 1.0164, 0.9896, 0.9994, 0.9747, 1.0058, 1.0054, 0.9915, 0.9944,\n                      0.9922, 0.9874, 1.0212, 1.0269, 1.0007, 1.0072, 1.0201, 0.9919, 0.9883,\n                      1.0005, 1.0020, 1.0067, 1.0090, 0.9767, 1.0027, 1.0141, 0.9893, 0.9992,\n                      1.0060, 1.0007, 1.0137, 1.0224, 0.9794, 0.9908, 0.9805, 1.0098, 0.9995,\n                      0.9713, 1.0012, 1.0014, 0.9879, 0.9987, 0.9876, 1.0174, 0.9993, 0.9832,\n                      1.0050, 1.0117, 1.0222, 1.0039, 0.9996, 0.9834, 0.9928, 1.0034, 0.9932,\n                      1.0040, 1.0050, 1.0308, 0.9881, 1.0229, 1.0066, 1.0155, 0.9835, 1.0153,\n                      1.0068, 0.9663, 1.0015, 0.9869, 1.0267, 0.9928, 0.9899, 0.9946, 0.9888,\n                      0.9997, 0.9976, 1.0140, 1.0012, 1.0018, 0.9897, 1.0121, 1.0048, 0.9984,\n                      0.9976, 0.9901, 0.9915, 0.9988, 0.9713, 0.9842, 0.9934, 0.9750, 1.0017,\n                      1.0097, 1.0038])),\n             ('c3.1.bias',\n              tensor([-0.0190,  0.0168, -0.0395, -0.0696, -0.0026, -0.0431, -0.0405, -0.0410,\n                      -0.0469, -0.0265, -0.0385, -0.0033, -0.0201, -0.0251, -0.0432, -0.0041,\n                      -0.0046, -0.0416, -0.0101, -0.0101,  0.0094, -0.0308, -0.0447, -0.0519,\n                      -0.0196, -0.0637, -0.0029, -0.0373, -0.0436, -0.0241, -0.0274, -0.0325,\n                      -0.0584, -0.0463, -0.0332, -0.0334, -0.0321, -0.0301, -0.0401, -0.0556,\n                      -0.0496, -0.0402,  0.0062, -0.0568, -0.0551, -0.0328, -0.0210, -0.0511,\n                      -0.0154, -0.0386, -0.0636, -0.0316, -0.0815, -0.0342, -0.0025, -0.0636,\n                      -0.0145, -0.0256, -0.0614, -0.0385, -0.0092, -0.0570, -0.0476, -0.0311,\n                      -0.0146, -0.0259,  0.0102, -0.0194, -0.0610, -0.0315, -0.0307, -0.0099,\n                      -0.0347, -0.0306, -0.0304, -0.0328, -0.0338, -0.0169, -0.0285, -0.0325,\n                      -0.0413, -0.0497, -0.0397, -0.0281, -0.0265, -0.0391, -0.0437, -0.0504,\n                      -0.0737, -0.0182, -0.0308, -0.0316, -0.0225, -0.0485, -0.0252, -0.0184,\n                      -0.0350, -0.0353, -0.0042, -0.0255, -0.0380, -0.0634, -0.0748, -0.0345,\n                      -0.0418, -0.0234, -0.0309, -0.0586, -0.0523, -0.0518, -0.0128, -0.0506,\n                      -0.0559, -0.0303, -0.0587, -0.0331, -0.0551, -0.0261, -0.0813, -0.0129,\n                      -0.0279, -0.0876, -0.0388, -0.0333, -0.0280, -0.0428, -0.0232, -0.0341])),\n             ('c3.1.running_mean',\n              tensor([-1.8171e+00,  2.7042e+00, -2.1585e-01,  7.4549e-01, -1.7702e+00,\n                       1.9959e-01, -1.1697e+00, -6.1472e-01,  1.2477e+00, -6.9652e-01,\n                      -1.3322e+00,  4.0483e-01, -4.2855e-01,  1.4291e+00, -6.6393e-01,\n                      -2.2216e-01, -2.4094e-01, -2.8053e-03, -7.3032e-01, -9.1071e-02,\n                      -2.5287e-01,  1.1207e+00, -3.7107e-01, -5.4337e-01, -1.4272e+00,\n                       2.6747e-01, -3.8813e-01,  1.0477e+00, -9.4834e-02,  9.8402e-01,\n                      -3.3545e-01,  2.6176e+00,  1.9075e+00,  7.3510e-01, -7.2269e-01,\n                       4.7701e-01, -7.5910e-02, -7.0590e-01,  1.5789e+00, -7.4414e-02,\n                       1.0734e+00,  1.2194e+00, -1.1950e+00,  1.9285e+00, -3.2037e+00,\n                      -3.0793e-01, -9.9609e-01,  7.8400e-02,  1.0049e+00,  3.1815e-01,\n                      -2.0837e-02, -1.6427e+00,  1.3370e+00, -5.9113e-01, -3.8173e-01,\n                      -4.9024e-01,  6.3102e-01,  1.0731e+00, -4.6862e-01, -7.4922e-01,\n                      -7.7817e-01,  5.0727e-01,  1.2321e-01,  7.6969e-01,  4.7642e-01,\n                       2.9330e-01,  4.0216e-01, -9.5525e-01, -2.6861e-01, -1.8475e-01,\n                       4.8885e-01, -1.4822e+00,  9.1658e-01,  3.9128e-01,  7.4893e-01,\n                      -6.6623e-01,  1.5719e+00,  3.9928e-01, -1.1917e+00,  4.8008e-01,\n                      -1.2132e+00, -7.1604e-02, -1.1792e+00, -1.6289e+00,  5.1848e-01,\n                      -1.9110e-01, -1.2525e+00,  1.9854e+00, -2.4142e-02, -4.8637e-01,\n                       2.1490e-01, -2.7969e-02,  7.2215e-03,  1.0891e+00, -1.5296e+00,\n                       1.1408e+00, -1.2475e+00,  9.6449e-01, -6.2094e-01,  3.8436e-02,\n                      -2.7327e+00,  2.8018e-01,  9.7086e-01, -1.1778e+00,  1.1548e+00,\n                      -1.5050e+00, -8.5346e-01,  1.3535e+00,  3.0471e-01,  1.9585e+00,\n                      -8.2017e-01, -6.9729e-02,  2.0700e+00, -2.6731e-01,  4.8102e-01,\n                      -1.2127e+00, -2.4516e+00, -1.1221e+00, -4.9729e-01, -1.7707e+00,\n                       3.5616e-01,  4.1125e-01,  6.1824e-01, -1.3618e+00, -1.3083e+00,\n                      -1.1043e+00, -6.6087e-02, -2.1898e+00])),\n             ('c3.1.running_var',\n              tensor([1.4403, 1.0184, 1.5743, 1.0841, 1.2194, 1.4338, 0.7207, 1.0939, 1.0937,\n                      0.8640, 0.9579, 0.7646, 1.1054, 0.9541, 0.7442, 0.6797, 1.4386, 1.3664,\n                      0.8831, 1.0042, 1.3586, 0.9380, 1.2955, 0.9773, 0.7688, 0.9608, 1.3795,\n                      0.9138, 1.5148, 1.3310, 0.9581, 0.9437, 0.9426, 0.8061, 0.9562, 1.7846,\n                      0.7431, 1.2555, 1.2926, 1.1168, 1.0617, 0.7658, 1.0007, 0.5552, 1.3315,\n                      1.3189, 1.1619, 1.0194, 0.9983, 0.7787, 1.0784, 0.8608, 1.2638, 0.8564,\n                      0.9724, 1.1576, 1.3163, 0.9702, 0.7648, 1.1571, 1.1835, 0.9215, 1.2274,\n                      0.5802, 0.5578, 1.0850, 1.1133, 1.0436, 0.8558, 1.0788, 0.5295, 0.9243,\n                      1.2846, 1.0746, 0.7596, 0.9166, 1.7604, 0.8411, 0.7347, 1.0248, 0.9633,\n                      0.7573, 0.9555, 0.8879, 0.7397, 0.6534, 1.3537, 1.1117, 1.2904, 1.3121,\n                      0.9163, 1.0745, 0.8362, 1.1068, 1.0407, 0.7465, 0.9054, 0.9572, 0.9043,\n                      0.6364, 1.2663, 0.7865, 1.4882, 0.6784, 0.9265, 1.6247, 1.9149, 0.9138,\n                      1.3419, 0.9783, 1.1932, 0.7892, 0.7164, 1.2436, 0.7719, 0.8628, 1.5978,\n                      0.9926, 1.4916, 1.8804, 0.6124, 0.9862, 0.8123, 1.1381, 2.0695, 0.8075,\n                      1.3850, 0.9073])),\n             ('c3.1.num_batches_tracked', tensor(3700)),\n             ('c4.0.weight',\n              tensor([[[[ 5.7660e-02,  1.7575e-02, -3.0066e-02],\n                        [-9.8507e-02,  3.6424e-02,  1.3897e-02],\n                        [ 1.0847e-02,  2.3305e-02,  2.5871e-02]],\n              \n                       [[ 5.9002e-02, -7.3619e-02,  4.4316e-02],\n                        [-2.4286e-02,  2.6108e-02,  8.8523e-03],\n                        [-3.2689e-02,  1.0281e-02,  8.3472e-03]],\n              \n                       [[-9.4557e-03, -1.4637e-02, -5.7141e-02],\n                        [-1.4466e-02, -5.6913e-02, -6.3473e-02],\n                        [ 2.4832e-02, -6.5500e-02,  7.5149e-03]],\n              \n                       ...,\n              \n                       [[-3.5363e-03, -3.7070e-02, -4.2811e-02],\n                        [ 9.5517e-03,  3.6913e-02, -4.7902e-03],\n                        [ 2.4400e-02,  8.6994e-02, -2.9933e-02]],\n              \n                       [[-7.1576e-02,  7.0258e-02,  4.7726e-03],\n                        [-9.1363e-03,  6.2106e-02, -2.1383e-03],\n                        [-9.8214e-03,  2.5584e-02, -7.3186e-03]],\n              \n                       [[-2.9218e-03,  1.0102e-01,  2.4523e-02],\n                        [-1.9150e-02, -1.8205e-02, -2.5172e-02],\n                        [ 1.2278e-03,  5.4628e-03,  6.4662e-02]]],\n              \n              \n                      [[[ 2.4434e-02, -3.8644e-02, -9.0881e-02],\n                        [-3.1312e-02, -6.3424e-02, -7.4748e-02],\n                        [-4.4445e-02,  8.5250e-03,  3.7158e-02]],\n              \n                       [[ 7.9072e-03, -2.4897e-02,  5.0948e-02],\n                        [-9.3325e-03, -6.9566e-03, -2.9244e-02],\n                        [-6.0573e-02,  5.6186e-02,  3.9092e-02]],\n              \n                       [[ 2.8934e-02, -5.9803e-02, -1.4509e-01],\n                        [ 1.4418e-02, -2.0592e-02, -1.2834e-01],\n                        [-4.9910e-02,  7.0793e-04,  6.3518e-02]],\n              \n                       ...,\n              \n                       [[-1.0299e-02, -4.2583e-02, -1.8843e-02],\n                        [ 2.7264e-02,  4.8157e-02, -1.2457e-02],\n                        [ 3.6001e-03, -9.0400e-02, -6.4828e-02]],\n              \n                       [[-6.7389e-02,  7.9364e-03, -6.2182e-02],\n                        [-3.5193e-02, -5.9195e-02, -2.3971e-02],\n                        [-9.6847e-03,  1.8323e-02, -3.8810e-02]],\n              \n                       [[-4.6400e-02, -4.8143e-02, -2.5927e-02],\n                        [-1.2126e-01, -2.7723e-02,  1.5245e-02],\n                        [ 1.4815e-02, -3.1663e-02, -9.4011e-02]]],\n              \n              \n                      [[[ 5.1634e-04, -2.5876e-02, -7.2473e-02],\n                        [-1.0418e-02, -6.3394e-02, -7.0926e-02],\n                        [ 5.1355e-03, -2.4886e-02, -1.6481e-02]],\n              \n                       [[ 3.6181e-02,  1.4026e-02, -7.8810e-02],\n                        [ 5.2394e-02, -5.6866e-02, -7.5092e-03],\n                        [ 3.4734e-02,  1.1236e-02, -1.9529e-02]],\n              \n                       [[ 2.2675e-02,  8.7251e-02,  5.8944e-03],\n                        [-1.0330e-03,  1.8220e-04, -5.8224e-02],\n                        [-1.1756e-02,  1.2328e-01,  3.6046e-03]],\n              \n                       ...,\n              \n                       [[ 7.8644e-02,  1.4074e-02,  4.5230e-02],\n                        [ 7.0183e-02,  2.5313e-02,  4.0658e-02],\n                        [ 2.2881e-02,  2.0000e-02, -6.8320e-03]],\n              \n                       [[-1.2460e-02, -1.5046e-02, -3.9741e-02],\n                        [ 3.8752e-02, -2.7389e-02,  1.1942e-02],\n                        [ 8.9724e-03,  1.5489e-02, -6.2216e-02]],\n              \n                       [[ 4.3094e-02,  1.2766e-02,  4.1297e-02],\n                        [ 5.0848e-02, -6.0495e-02, -2.2984e-02],\n                        [ 3.6352e-02, -1.2802e-02,  3.0860e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-7.7098e-02, -6.7661e-05,  1.1388e-02],\n                        [-2.7522e-02,  9.6204e-03, -1.0604e-02],\n                        [ 8.2123e-03, -2.6684e-02, -1.7786e-02]],\n              \n                       [[ 1.7004e-02,  1.4935e-02, -1.9089e-02],\n                        [ 1.2845e-01, -4.9442e-02, -5.6607e-02],\n                        [ 3.9508e-03, -7.1348e-03,  3.8547e-02]],\n              \n                       [[ 7.8307e-03,  1.5761e-02, -2.7911e-02],\n                        [-2.6752e-02, -1.8141e-02, -4.3959e-02],\n                        [ 5.6511e-02, -2.7508e-02, -6.8125e-02]],\n              \n                       ...,\n              \n                       [[ 7.9145e-02,  5.6846e-02, -6.8337e-03],\n                        [ 9.2917e-02,  1.2758e-02,  6.3370e-02],\n                        [ 9.4810e-04,  3.5077e-03,  7.0550e-02]],\n              \n                       [[ 8.8063e-02,  5.6010e-02, -6.1404e-02],\n                        [-2.1962e-02,  3.9773e-02, -7.3988e-03],\n                        [ 4.9272e-02, -2.4511e-02, -8.5005e-02]],\n              \n                       [[ 2.4289e-02, -5.3509e-03,  2.1416e-02],\n                        [-1.3430e-02,  9.3080e-02, -3.9141e-02],\n                        [-4.3292e-02,  1.3963e-02,  3.2061e-02]]],\n              \n              \n                      [[[ 3.3328e-02,  2.0616e-02, -4.1012e-02],\n                        [ 1.0699e-02,  6.2978e-02,  8.8348e-03],\n                        [ 1.8885e-02,  7.7468e-02,  6.7676e-02]],\n              \n                       [[-3.7039e-02, -1.3360e-02,  1.1090e-02],\n                        [ 7.2065e-02,  2.1236e-02,  1.2060e-02],\n                        [ 5.2858e-02,  5.8154e-02,  8.1744e-03]],\n              \n                       [[-2.9763e-02, -1.9436e-02,  4.0040e-02],\n                        [-2.5679e-02,  1.4761e-02,  8.2106e-03],\n                        [-7.5791e-02,  2.6794e-02, -4.2432e-02]],\n              \n                       ...,\n              \n                       [[-6.7101e-02, -2.3574e-02, -4.6491e-02],\n                        [ 8.7092e-03,  8.8765e-02, -2.0212e-02],\n                        [ 4.1624e-02,  8.8484e-03, -3.6489e-02]],\n              \n                       [[-4.4028e-03,  1.9264e-02,  5.7613e-02],\n                        [ 3.9996e-02, -5.1738e-02, -3.4413e-02],\n                        [-2.1896e-04,  1.7663e-02,  4.0957e-02]],\n              \n                       [[ 1.5724e-02,  5.7431e-03,  1.0602e-02],\n                        [-1.1963e-02,  9.9183e-03, -9.1387e-03],\n                        [ 4.8857e-02, -3.7210e-02,  9.1119e-02]]],\n              \n              \n                      [[[-3.8867e-02, -2.6813e-03,  2.3189e-02],\n                        [-4.1852e-02, -4.2544e-02, -2.9414e-02],\n                        [ 6.7134e-02,  2.1632e-02,  1.8829e-02]],\n              \n                       [[ 4.9041e-02,  2.2612e-02,  1.2138e-01],\n                        [ 8.5745e-02,  2.0625e-02, -1.6011e-02],\n                        [ 1.0881e-02,  6.7332e-02,  3.9765e-02]],\n              \n                       [[-2.2888e-02, -7.6182e-02,  3.7063e-02],\n                        [ 2.0569e-02, -2.5983e-04, -7.3716e-02],\n                        [ 4.7479e-02,  5.3592e-04,  2.1230e-03]],\n              \n                       ...,\n              \n                       [[ 1.2866e-02,  5.5741e-04, -2.7985e-02],\n                        [-3.8820e-02,  3.4489e-02, -8.3910e-02],\n                        [-2.8908e-03, -5.1107e-03, -1.3108e-01]],\n              \n                       [[-4.8980e-02, -1.2273e-02,  3.2636e-02],\n                        [ 5.3104e-02,  1.0174e-02, -9.8290e-02],\n                        [-1.8984e-03,  2.5172e-03,  1.5492e-02]],\n              \n                       [[ 4.0367e-02,  1.0365e-02,  4.9879e-03],\n                        [ 1.4165e-02,  2.8385e-02,  8.4533e-03],\n                        [ 6.0689e-02, -1.2483e-02,  3.8387e-02]]]])),\n             ('c4.0.bias',\n              tensor([-0.0250,  0.0220, -0.0255, -0.0050, -0.0228, -0.0217,  0.0240, -0.0199,\n                      -0.0113, -0.0265, -0.0001,  0.0176,  0.0016,  0.0153, -0.0072, -0.0125,\n                       0.0060, -0.0194,  0.0242, -0.0233, -0.0257, -0.0085, -0.0119, -0.0163,\n                       0.0176,  0.0055, -0.0279,  0.0287,  0.0143, -0.0274, -0.0257, -0.0090,\n                       0.0084, -0.0052,  0.0101,  0.0281,  0.0105, -0.0281,  0.0034, -0.0032,\n                      -0.0096,  0.0120, -0.0252,  0.0038,  0.0071, -0.0154, -0.0206, -0.0084,\n                       0.0148, -0.0143,  0.0170, -0.0130, -0.0024,  0.0267,  0.0233, -0.0224,\n                       0.0153, -0.0226, -0.0214, -0.0010,  0.0112,  0.0187, -0.0185,  0.0028,\n                       0.0190, -0.0245, -0.0017, -0.0242, -0.0127,  0.0274, -0.0257,  0.0035,\n                      -0.0285,  0.0014, -0.0274, -0.0222, -0.0115,  0.0013,  0.0115, -0.0051,\n                      -0.0261,  0.0280,  0.0160, -0.0239,  0.0095, -0.0285, -0.0054, -0.0294,\n                      -0.0161, -0.0027, -0.0269,  0.0206, -0.0113,  0.0135,  0.0201,  0.0240,\n                       0.0074, -0.0102, -0.0076,  0.0275,  0.0135, -0.0278,  0.0027, -0.0032,\n                      -0.0104,  0.0075,  0.0222,  0.0140, -0.0005,  0.0130, -0.0152,  0.0027,\n                      -0.0228, -0.0184, -0.0192, -0.0231,  0.0153, -0.0039, -0.0058, -0.0165,\n                      -0.0059, -0.0160, -0.0045,  0.0038, -0.0072, -0.0256,  0.0254,  0.0192])),\n             ('c4.1.weight',\n              tensor([0.9816, 0.9884, 0.9897, 0.9833, 0.9912, 1.0102, 0.9983, 1.0038, 0.9967,\n                      1.0039, 0.9954, 0.9883, 0.9891, 0.9688, 0.9830, 0.9697, 1.0125, 1.0103,\n                      1.0000, 0.9978, 1.0108, 0.9576, 0.9788, 0.9811, 0.9813, 0.9922, 0.9741,\n                      0.9843, 0.9864, 0.9864, 0.9942, 1.0003, 0.9843, 0.9959, 1.0017, 0.9995,\n                      0.9870, 0.9920, 0.9827, 0.9919, 0.9911, 0.9906, 0.9961, 0.9889, 0.9659,\n                      0.9752, 0.9740, 0.9937, 0.9731, 0.9863, 0.9627, 1.0151, 0.9984, 0.9942,\n                      0.9771, 0.9905, 0.9951, 0.9873, 0.9917, 0.9836, 0.9913, 0.9724, 0.9954,\n                      0.9813, 1.0065, 1.0005, 0.9834, 0.9894, 1.0138, 0.9979, 1.0021, 0.9916,\n                      1.0006, 0.9878, 0.9720, 0.9722, 0.9847, 0.9950, 0.9714, 1.0062, 0.9777,\n                      0.9527, 0.9879, 0.9751, 0.9966, 0.9933, 0.9862, 0.9866, 0.9991, 0.9817,\n                      0.9876, 0.9865, 0.9917, 0.9915, 0.9537, 0.9814, 1.0032, 0.9933, 0.9922,\n                      0.9952, 0.9864, 0.9690, 0.9895, 1.0044, 0.9789, 1.0188, 0.9913, 0.9789,\n                      0.9994, 0.9681, 1.0038, 0.9765, 0.9860, 0.9785, 0.9940, 0.9829, 0.9895,\n                      0.9664, 0.9838, 0.9821, 0.9892, 1.0032, 0.9923, 1.0030, 0.9928, 0.9944,\n                      0.9719, 0.9832])),\n             ('c4.1.bias',\n              tensor([-0.0474, -0.0668, -0.0408, -0.0304, -0.0393, -0.0565, -0.0469, -0.0656,\n                      -0.0474, -0.0534, -0.0551, -0.0541, -0.0284, -0.0661, -0.0367, -0.0819,\n                      -0.0592, -0.0165, -0.0313, -0.0673, -0.0007, -0.0801, -0.0455, -0.0831,\n                      -0.0378, -0.0731, -0.0549, -0.0464, -0.0491, -0.0412, -0.0415, -0.0528,\n                      -0.0469, -0.0362, -0.0362, -0.0527, -0.0137, -0.0380, -0.0379, -0.0310,\n                      -0.0292, -0.0544, -0.0525, -0.0469, -0.0421, -0.0394, -0.0582, -0.0633,\n                      -0.0384, -0.0294, -0.0551, -0.0931, -0.0564, -0.0586, -0.0532, -0.0505,\n                      -0.0379, -0.0336, -0.0590, -0.0444, -0.0710, -0.0632, -0.0574, -0.0150,\n                      -0.0429, -0.0099, -0.0427, -0.0425, -0.0429, -0.0589, -0.0464, -0.0237,\n                       0.0087, -0.0492, -0.0824, -0.0473, -0.0711, -0.0467, -0.0395, -0.0252,\n                      -0.0416, -0.0815, -0.0459, -0.0560, -0.0991, -0.0115, -0.0440, -0.0685,\n                      -0.0578, -0.0609, -0.0098, -0.0491, -0.0128, -0.0714, -0.0203, -0.0266,\n                      -0.0407, -0.0094, -0.0454, -0.0607, -0.0456, -0.0601, -0.0373, -0.0418,\n                      -0.0659, -0.0504, -0.0641, -0.0549, -0.0306, -0.0198, -0.0673, -0.0664,\n                      -0.0118, -0.0829, -0.0470, -0.0755, -0.0428, -0.0792, -0.0400, -0.0745,\n                      -0.0598, -0.0299, -0.0233, -0.0478, -0.0403, -0.0665, -0.0152, -0.0995])),\n             ('c4.1.running_mean',\n              tensor([-1.3710, -0.1508, -1.0433, -0.4081,  0.4582, -0.2158, -1.1486,  1.5304,\n                      -1.2598,  0.3033, -1.4293, -1.4883, -0.7299,  0.2246, -0.4054, -1.1563,\n                       0.8359, -1.8803,  0.3048, -1.6295, -2.2285,  0.9493, -0.7482, -0.4499,\n                       0.0854,  1.0093, -1.1166, -0.3863,  0.5817, -1.8375, -0.8832, -1.1099,\n                      -0.1220, -0.9875, -1.3023, -0.9689, -0.5998, -1.0542, -0.9466, -0.5586,\n                      -1.6524,  0.9811, -0.2578, -1.9234, -0.7477, -0.1606,  0.1577,  0.3516,\n                       0.4217, -1.0279, -1.8422,  0.3616, -1.4779, -1.7331, -1.4436,  0.4369,\n                       0.0723,  1.3982,  0.5773, -0.9054,  1.8324,  0.3933,  0.7377, -1.5387,\n                       0.3232, -1.9397, -0.5071,  0.4117,  0.9874, -0.5478,  0.1795, -0.7727,\n                      -1.7850,  0.5397,  2.0417, -0.5625, -0.2514, -1.1806, -1.8709, -0.1970,\n                       0.2456, -0.7416,  0.5302, -1.3201, -0.5845, -0.1553, -0.9960,  0.7647,\n                      -1.8335, -0.8663, -1.2774,  0.5840, -1.7030,  0.4542,  0.7495, -1.4629,\n                       0.9146, -1.0437,  0.2526, -0.6282,  1.1658,  0.3979, -0.4992, -0.8439,\n                       1.0097,  0.6269,  0.2955,  0.4621, -2.6121, -1.9501, -0.0037, -1.4371,\n                      -0.6005,  0.3000, -1.5294, -1.1564, -2.0209, -0.1092, -0.4908, -0.6248,\n                      -1.9208, -0.4546, -0.1329,  1.5991, -1.0999, -0.9674, -2.3778,  1.5534])),\n             ('c4.1.running_var',\n              tensor([2.8314, 1.9574, 1.7575, 3.4252, 1.9010, 2.2398, 1.5469, 2.0311, 3.0005,\n                      2.1742, 2.0308, 1.0809, 1.8345, 2.0157, 2.8522, 1.8773, 2.5806, 1.2746,\n                      1.4532, 2.1676, 2.2047, 2.6467, 3.2098, 2.7425, 3.6173, 2.9399, 1.8078,\n                      1.5597, 1.2062, 2.4136, 2.4153, 1.7370, 2.1409, 2.4153, 1.7446, 1.6973,\n                      5.6006, 1.4017, 3.8156, 3.1997, 3.8936, 1.6140, 1.6814, 1.9067, 3.2127,\n                      2.5990, 1.8604, 1.8736, 1.9373, 2.4740, 2.9264, 1.9338, 2.7007, 2.9710,\n                      3.3631, 2.0992, 2.7798, 3.2036, 2.6915, 1.7141, 2.6984, 2.1613, 2.2891,\n                      1.7697, 2.1693, 2.1206, 1.5904, 2.0159, 1.8264, 1.4258, 1.6862, 2.4660,\n                      1.4266, 2.1229, 2.4258, 3.4134, 1.6775, 1.1610, 2.5931, 2.4602, 1.9451,\n                      1.4790, 1.8793, 1.6614, 2.1573, 1.5938, 1.5978, 1.5017, 2.5806, 2.8407,\n                      2.6251, 2.7685, 3.6341, 1.5294, 2.7647, 2.3879, 1.8906, 2.1014, 1.3711,\n                      2.2288, 2.1445, 3.7482, 2.2569, 1.8258, 4.1522, 2.3439, 1.5178, 2.3321,\n                      2.5017, 1.9759, 2.5760, 2.4014, 4.6034, 2.7829, 3.2496, 1.6458, 2.5710,\n                      2.9007, 2.7175, 1.9163, 1.9001, 2.3090, 2.0298, 2.1492, 1.3214, 2.0184,\n                      1.5561, 1.8245])),\n             ('c4.1.num_batches_tracked', tensor(3700)),\n             ('c5.0.weight',\n              tensor([[[[ 1.7084e-02, -2.2903e-02, -2.0165e-02],\n                        [-7.3493e-02, -4.3354e-02, -5.3454e-02],\n                        [-3.8754e-02, -5.1804e-02, -1.9257e-02]],\n              \n                       [[-1.6087e-04, -4.1628e-02, -4.4005e-02],\n                        [-2.3995e-02, -6.2241e-02, -1.4144e-02],\n                        [-2.1027e-02,  9.6706e-03,  3.1567e-02]],\n              \n                       [[-3.2000e-02, -3.5455e-02, -1.8748e-02],\n                        [-1.5194e-02, -5.3486e-05, -2.4401e-02],\n                        [ 3.6749e-02, -4.7501e-02, -4.7097e-02]],\n              \n                       ...,\n              \n                       [[ 6.9956e-02,  1.5138e-02,  1.4315e-02],\n                        [-1.0994e-03,  5.6020e-03, -3.9170e-02],\n                        [ 9.0739e-02,  1.5992e-02,  5.7499e-02]],\n              \n                       [[-4.9804e-03, -2.6808e-02,  5.0409e-03],\n                        [ 4.4597e-03, -1.5804e-02, -5.7713e-03],\n                        [ 1.3142e-02, -2.2807e-02,  6.6898e-03]],\n              \n                       [[-2.8498e-02, -2.5405e-02,  2.1861e-02],\n                        [-3.7058e-02, -6.1859e-03, -3.4416e-02],\n                        [-4.0805e-03, -1.2073e-02,  3.7816e-02]]],\n              \n              \n                      [[[-1.3919e-02,  2.5976e-02,  3.1282e-03],\n                        [ 3.2070e-02,  3.2858e-02,  7.2703e-02],\n                        [ 3.1031e-02,  4.2566e-02,  4.8640e-02]],\n              \n                       [[-1.9318e-02, -9.9746e-03,  4.0673e-02],\n                        [ 1.0932e-02,  5.4926e-02,  1.1315e-02],\n                        [-3.0118e-02, -1.2250e-02, -3.2941e-03]],\n              \n                       [[ 3.5074e-03, -1.3877e-02, -3.1112e-02],\n                        [-2.5406e-02,  8.5360e-03, -2.6496e-02],\n                        [-1.3302e-03, -2.5575e-02, -3.7930e-02]],\n              \n                       ...,\n              \n                       [[ 9.2400e-04,  1.0810e-02,  3.9883e-02],\n                        [-5.4676e-02, -1.0400e-02,  4.8059e-03],\n                        [-1.1459e-02, -5.6425e-04,  6.8524e-02]],\n              \n                       [[-9.2551e-04, -2.1054e-02, -5.3734e-03],\n                        [ 1.9575e-02,  2.8403e-02, -2.5128e-02],\n                        [-5.7397e-03, -2.3089e-02, -6.0531e-03]],\n              \n                       [[ 2.1095e-02,  2.5013e-02, -5.3706e-04],\n                        [ 5.0141e-02,  3.4383e-02,  3.0590e-03],\n                        [-8.6355e-03, -1.7599e-03,  5.9080e-03]]],\n              \n              \n                      [[[ 4.1051e-02,  1.4232e-02, -2.8574e-02],\n                        [-1.9509e-02,  2.8099e-02, -2.4641e-02],\n                        [ 2.0104e-02,  1.5957e-02, -4.0346e-03]],\n              \n                       [[ 7.0080e-02,  4.3298e-02,  2.1684e-02],\n                        [ 5.4950e-02,  1.9596e-02, -2.9879e-03],\n                        [ 7.5805e-02,  1.2202e-02, -1.6621e-02]],\n              \n                       [[-2.8043e-02,  3.3184e-02, -7.0400e-03],\n                        [ 7.4005e-02, -3.4682e-02, -1.3846e-02],\n                        [ 1.7947e-02, -6.4746e-03, -1.8955e-02]],\n              \n                       ...,\n              \n                       [[ 1.3517e-02, -1.2166e-02,  8.5238e-03],\n                        [-8.7519e-03,  3.9857e-02, -3.0537e-03],\n                        [-1.1697e-02,  7.1425e-03,  5.4197e-02]],\n              \n                       [[-2.8104e-02, -2.7901e-02,  2.0767e-02],\n                        [-4.8396e-02, -7.1420e-03, -7.6262e-03],\n                        [-2.7420e-02,  1.1658e-03,  3.5324e-02]],\n              \n                       [[-3.8001e-02,  1.4144e-02, -5.9471e-02],\n                        [ 1.1253e-02,  4.6889e-02, -1.2211e-02],\n                        [-1.9566e-02,  5.7255e-03,  6.7955e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.7787e-02, -1.0781e-02, -3.3890e-02],\n                        [ 1.0821e-02, -6.0768e-03, -1.2309e-02],\n                        [ 3.7120e-02,  9.2625e-03, -1.4946e-02]],\n              \n                       [[-3.4201e-02, -1.7760e-02, -1.1112e-02],\n                        [-2.2159e-02, -2.8693e-02, -5.6712e-03],\n                        [ 4.0543e-03, -2.6463e-02, -9.2775e-04]],\n              \n                       [[-2.8171e-02,  1.4804e-02, -3.9678e-02],\n                        [-2.3033e-02,  2.6026e-02,  2.7072e-02],\n                        [ 2.3093e-02, -8.1479e-04, -4.9422e-02]],\n              \n                       ...,\n              \n                       [[-4.5504e-02, -1.8034e-02,  1.9116e-02],\n                        [-4.6212e-03, -5.5502e-02, -2.3741e-02],\n                        [ 4.3244e-02, -4.4869e-02,  6.2731e-02]],\n              \n                       [[-1.0643e-03, -4.9267e-02,  2.7517e-02],\n                        [ 5.0409e-02, -4.1443e-02, -2.3738e-02],\n                        [-1.2782e-02,  2.6557e-02,  3.3538e-02]],\n              \n                       [[ 2.3107e-03,  3.2741e-02,  1.3720e-03],\n                        [ 1.2272e-02, -8.7193e-03, -1.3233e-02],\n                        [ 9.6662e-03,  4.8308e-02, -1.4472e-02]]],\n              \n              \n                      [[[ 4.0779e-02, -1.2781e-02,  2.3576e-02],\n                        [-4.4482e-02,  1.7649e-02,  3.0140e-02],\n                        [ 1.8728e-02,  5.0685e-02,  9.8831e-02]],\n              \n                       [[ 8.2760e-02,  6.0163e-02, -9.6250e-03],\n                        [ 6.6309e-03,  3.1496e-02,  1.3617e-02],\n                        [-2.2204e-02,  5.5327e-02,  2.8537e-02]],\n              \n                       [[-5.7345e-03, -4.1181e-02, -1.6640e-02],\n                        [ 2.4150e-02,  1.6884e-02,  9.8915e-03],\n                        [ 4.4574e-03,  4.4948e-02,  1.2168e-02]],\n              \n                       ...,\n              \n                       [[ 2.1006e-03,  7.1199e-03, -2.8530e-02],\n                        [ 2.5784e-02, -2.0983e-02, -1.8686e-02],\n                        [-2.0111e-02, -1.0502e-02, -7.6960e-03]],\n              \n                       [[ 1.7772e-02,  2.4502e-02,  3.4468e-02],\n                        [ 1.2162e-02,  5.6249e-02, -6.5176e-02],\n                        [-3.5909e-02, -1.7767e-02,  2.1095e-04]],\n              \n                       [[ 6.9450e-02, -8.0707e-03, -1.5519e-02],\n                        [-2.0842e-02,  2.3034e-02,  3.4302e-02],\n                        [ 2.0792e-02,  7.3282e-03,  1.0403e-02]]],\n              \n              \n                      [[[-1.6839e-03,  3.5059e-02,  6.5178e-03],\n                        [ 1.0406e-02,  2.1851e-02,  1.8993e-02],\n                        [ 4.7905e-02, -1.0856e-02,  9.1972e-03]],\n              \n                       [[ 5.6885e-03,  1.3263e-02,  1.6054e-02],\n                        [-2.7792e-02, -2.7041e-03, -3.4204e-02],\n                        [-2.2887e-02,  5.2276e-02, -2.7595e-02]],\n              \n                       [[-2.1823e-02, -2.9443e-02, -2.6499e-02],\n                        [ 1.5594e-02,  1.8501e-02,  9.7817e-02],\n                        [ 8.3154e-05,  3.0736e-03,  9.3099e-03]],\n              \n                       ...,\n              \n                       [[ 6.0018e-03, -2.7647e-02, -5.8807e-03],\n                        [-3.7906e-02, -1.1133e-02,  5.9957e-02],\n                        [ 8.6588e-02,  8.2048e-03, -1.2088e-03]],\n              \n                       [[-1.9609e-02,  2.3214e-02,  6.2588e-02],\n                        [ 1.2112e-02,  7.0847e-02,  6.4323e-02],\n                        [ 8.6301e-03, -2.8070e-02,  7.6946e-03]],\n              \n                       [[-2.5864e-02,  6.3585e-03, -6.9639e-02],\n                        [-3.4979e-02,  6.8941e-02,  3.8183e-03],\n                        [-6.5049e-03,  4.6494e-03, -2.1481e-02]]]])),\n             ('c5.0.bias',\n              tensor([-0.0109, -0.0071, -0.0276,  0.0233,  0.0038,  0.0256, -0.0216, -0.0019,\n                       0.0068, -0.0285,  0.0171, -0.0160, -0.0130,  0.0006,  0.0046, -0.0197,\n                       0.0246, -0.0106, -0.0093, -0.0069, -0.0243,  0.0081,  0.0243,  0.0226,\n                      -0.0025, -0.0236,  0.0254,  0.0262, -0.0212,  0.0013, -0.0033, -0.0095,\n                       0.0182, -0.0271, -0.0056,  0.0129,  0.0167,  0.0182,  0.0159, -0.0005,\n                       0.0036,  0.0188, -0.0069,  0.0282, -0.0076, -0.0041, -0.0111,  0.0111,\n                       0.0138,  0.0118, -0.0004, -0.0092,  0.0125, -0.0029,  0.0054,  0.0133,\n                       0.0068, -0.0025,  0.0285, -0.0082, -0.0130, -0.0080, -0.0228,  0.0034,\n                       0.0219,  0.0290, -0.0071,  0.0166, -0.0200, -0.0109, -0.0255,  0.0285,\n                      -0.0292, -0.0016,  0.0043,  0.0017, -0.0224, -0.0186,  0.0059, -0.0104,\n                      -0.0038, -0.0266, -0.0071, -0.0211, -0.0120,  0.0077, -0.0123, -0.0274,\n                       0.0204,  0.0255,  0.0233,  0.0222, -0.0113, -0.0288, -0.0285, -0.0136,\n                      -0.0142, -0.0094,  0.0104,  0.0159,  0.0281,  0.0238, -0.0021, -0.0118,\n                      -0.0259,  0.0143,  0.0108, -0.0164, -0.0129,  0.0138, -0.0275, -0.0259,\n                      -0.0045,  0.0288,  0.0034, -0.0224, -0.0063, -0.0197,  0.0181,  0.0249,\n                       0.0036,  0.0145,  0.0009,  0.0100,  0.0012,  0.0127,  0.0023,  0.0085,\n                      -0.0161,  0.0119,  0.0091, -0.0157, -0.0148,  0.0214,  0.0048,  0.0118,\n                      -0.0050,  0.0054,  0.0139, -0.0057,  0.0033, -0.0198, -0.0193, -0.0244,\n                       0.0059,  0.0221,  0.0251,  0.0084,  0.0275,  0.0136, -0.0060,  0.0196,\n                       0.0276,  0.0247, -0.0047, -0.0240,  0.0247,  0.0266, -0.0029,  0.0085,\n                       0.0166, -0.0158, -0.0111,  0.0203,  0.0233,  0.0282,  0.0165,  0.0200,\n                       0.0191, -0.0148, -0.0169,  0.0069, -0.0261, -0.0157, -0.0130, -0.0104,\n                      -0.0112, -0.0268,  0.0158,  0.0139,  0.0189,  0.0169,  0.0095, -0.0054,\n                      -0.0203,  0.0012,  0.0245, -0.0160,  0.0140, -0.0243, -0.0107, -0.0254,\n                      -0.0150, -0.0170, -0.0216,  0.0236,  0.0002,  0.0077, -0.0286, -0.0249,\n                       0.0020, -0.0004,  0.0150,  0.0240,  0.0014, -0.0008, -0.0076, -0.0164,\n                       0.0186,  0.0160, -0.0264,  0.0253, -0.0037,  0.0245,  0.0098, -0.0291,\n                      -0.0092, -0.0183,  0.0246,  0.0136,  0.0206, -0.0161,  0.0138, -0.0039,\n                      -0.0024, -0.0064, -0.0185,  0.0226, -0.0177,  0.0188, -0.0189, -0.0266,\n                       0.0290, -0.0095,  0.0178,  0.0128,  0.0184,  0.0086, -0.0133, -0.0155,\n                      -0.0098,  0.0025,  0.0086, -0.0220,  0.0272,  0.0037,  0.0290, -0.0174,\n                       0.0053, -0.0064,  0.0006,  0.0196,  0.0141,  0.0089,  0.0213,  0.0186])),\n             ('c5.1.weight',\n              tensor([0.9784, 1.0303, 1.0359, 0.9897, 1.0405, 0.9910, 0.9814, 1.0032, 0.9875,\n                      0.9959, 1.0470, 1.0045, 1.0541, 1.0352, 0.9848, 0.9930, 0.9842, 0.9772,\n                      1.0173, 0.9625, 1.0362, 0.9799, 1.0194, 1.0074, 1.0334, 1.0120, 0.9831,\n                      0.9863, 0.9831, 1.0511, 0.9797, 0.9812, 0.9990, 0.9845, 1.0068, 0.9876,\n                      1.0353, 0.9814, 0.9944, 0.9570, 1.0053, 1.0478, 0.9965, 0.9953, 1.0302,\n                      0.9605, 0.9746, 1.0067, 0.9865, 0.9869, 0.9968, 1.0317, 1.0060, 0.9900,\n                      0.9844, 1.0243, 1.0053, 0.9950, 0.9943, 1.0053, 0.9828, 0.9928, 0.9754,\n                      1.0335, 1.0415, 0.9583, 1.0025, 0.9891, 0.9944, 0.9764, 0.9826, 0.9859,\n                      1.0248, 0.9866, 1.0055, 1.0142, 1.0010, 1.0538, 0.9778, 1.0582, 1.0655,\n                      0.9891, 0.9821, 1.0893, 1.0019, 1.0399, 0.9684, 1.0701, 1.0139, 0.9797,\n                      0.9641, 1.0335, 1.0033, 1.0232, 1.0065, 1.0136, 0.9980, 0.9836, 1.0145,\n                      1.0198, 1.0072, 0.9760, 0.9674, 1.0358, 1.0127, 0.9984, 0.9815, 0.9971,\n                      1.0119, 0.9916, 1.0423, 1.0258, 1.0329, 1.0178, 1.0245, 1.0150, 0.9638,\n                      0.9900, 0.9813, 0.9878, 0.9790, 0.9936, 1.0138, 0.9620, 1.0095, 0.9969,\n                      1.0214, 1.0479, 1.0108, 0.9918, 0.9943, 0.9748, 0.9986, 0.9796, 0.9727,\n                      1.0074, 1.0176, 0.9970, 0.9749, 0.9804, 1.0027, 1.0123, 0.9811, 0.9749,\n                      0.9933, 1.0202, 0.9693, 0.9955, 0.9818, 0.9965, 1.0071, 0.9896, 0.9805,\n                      0.9395, 0.9990, 1.0051, 0.9891, 1.0027, 1.0098, 0.9950, 0.9987, 1.0095,\n                      1.0066, 0.9842, 1.0042, 0.9789, 1.0482, 0.9999, 1.0335, 1.0102, 0.9657,\n                      0.9947, 0.9642, 0.9839, 1.0058, 0.9854, 0.9776, 0.9880, 0.9834, 1.0075,\n                      0.9841, 1.0233, 0.9873, 1.0098, 1.0004, 1.0065, 1.0131, 0.9768, 1.0024,\n                      0.9842, 0.9888, 0.9648, 0.9772, 0.9810, 1.0538, 1.0855, 1.0217, 0.9844,\n                      0.9804, 0.9991, 1.0437, 0.9730, 1.0094, 0.9608, 0.9868, 0.9754, 0.9895,\n                      0.9992, 1.0138, 1.0276, 1.0133, 0.9709, 1.0071, 1.0446, 0.9841, 0.9816,\n                      0.9770, 1.0620, 1.0216, 1.0129, 1.0170, 0.9885, 0.9773, 1.0137, 0.9849,\n                      0.9796, 0.9924, 1.0051, 1.0018, 0.9972, 1.0387, 0.9749, 0.9861, 0.9627,\n                      1.0007, 0.9955, 0.9815, 1.0346, 0.9865, 1.0361, 1.0052, 0.9811, 0.9846,\n                      0.9847, 1.0058, 1.0141, 0.9833, 0.9540, 1.0134, 1.0018, 1.0020, 0.9896,\n                      0.9789, 0.9796, 0.9710, 0.9605])),\n             ('c5.1.bias',\n              tensor([-0.0712, -0.0794, -0.0448, -0.0688, -0.0157, -0.0975, -0.0691, -0.0260,\n                      -0.0392, -0.0424, -0.0766, -0.0641, -0.0450, -0.0424, -0.0493, -0.0785,\n                      -0.0620, -0.0320, -0.0268, -0.0838, -0.0465, -0.0592, -0.0547, -0.0283,\n                      -0.0636, -0.0626, -0.0442, -0.0421, -0.0678, -0.0414, -0.0277, -0.0466,\n                      -0.0486, -0.0597, -0.0586, -0.0690, -0.0401, -0.0484, -0.0867, -0.0495,\n                      -0.0543, -0.0245, -0.0597, -0.0819, -0.0834, -0.0785, -0.0708, -0.0681,\n                      -0.1073, -0.0626, -0.0432, -0.0416, -0.0375, -0.0705, -0.0639, -0.0688,\n                      -0.0685, -0.0623, -0.0633, -0.0590, -0.0679, -0.0523, -0.0898, -0.0333,\n                      -0.0477, -0.0476, -0.0402, -0.0567, -0.0411, -0.0392, -0.0588, -0.0783,\n                      -0.0334, -0.0481, -0.0342, -0.0560, -0.0489, -0.0509, -0.0763, -0.0684,\n                      -0.0047, -0.0421, -0.0619,  0.0019, -0.0371, -0.0693, -0.0530, -0.0501,\n                      -0.0792, -0.0512, -0.0563, -0.0436, -0.0653, -0.0436, -0.0144, -0.0601,\n                      -0.0198, -0.0577, -0.0595, -0.0450, -0.0244, -0.0732, -0.0682, -0.0156,\n                      -0.0729, -0.0232, -0.0619, -0.0502, -0.0344, -0.0308, -0.0492, -0.0626,\n                      -0.0470, -0.0168, -0.0230, -0.0475, -0.0452, -0.0488, -0.0617, -0.0387,\n                      -0.0942, -0.0481, -0.0645, -0.0781, -0.0679, -0.0906, -0.0521, -0.0278,\n                      -0.0739, -0.0381, -0.0529, -0.0993, -0.0629, -0.0656, -0.0457, -0.0339,\n                      -0.0465, -0.0659, -0.0703, -0.0625, -0.0545, -0.0454, -0.0748, -0.0698,\n                      -0.0517, -0.0476, -0.0964, -0.0647, -0.0616, -0.0697, -0.0807, -0.0703,\n                      -0.0531, -0.0902, -0.0604, -0.0566, -0.0607, -0.0536, -0.0579, -0.0609,\n                      -0.0327, -0.0388, -0.0341, -0.0664, -0.0261, -0.0392, -0.0655, -0.0558,\n                      -0.0679, -0.0521, -0.0540, -0.0670, -0.0692, -0.0862, -0.0754, -0.0615,\n                      -0.0710, -0.0549, -0.0193, -0.0646, -0.0669, -0.0386, -0.0713, -0.0641,\n                      -0.0686, -0.0597, -0.0208, -0.0500, -0.0472, -0.0683, -0.0581, -0.0637,\n                      -0.0613, -0.0712, -0.0369, -0.0232, -0.0619, -0.0672, -0.0702, -0.0557,\n                      -0.0750, -0.0317, -0.0308, -0.0724, -0.0750, -0.0810, -0.0334, -0.0265,\n                      -0.0233, -0.0573, -0.0507, -0.0468, -0.0428, -0.0208, -0.0967, -0.0570,\n                      -0.0763, -0.0178, -0.0224, -0.0679, -0.0369, -0.0635, -0.0813, -0.0373,\n                      -0.0275, -0.0847, -0.0401, -0.0487, -0.0564, -0.0934, -0.0274, -0.0572,\n                      -0.0746, -0.0583, -0.0596, -0.0592, -0.0459, -0.0353, -0.0457, -0.0650,\n                      -0.0524, -0.0592, -0.0461, -0.0725, -0.0608, -0.0239, -0.0621, -0.0761,\n                      -0.0430, -0.0557, -0.0737, -0.0603, -0.0720, -0.0648, -0.0303, -0.0264])),\n             ('c5.1.running_mean',\n              tensor([-1.4421,  0.9588, -2.0041, -0.8444, -0.0142,  1.2962, -0.9808,  1.7018,\n                      -2.1287, -2.1016, -0.9884,  1.7206,  0.3118,  0.3795, -1.6023,  1.6549,\n                      -2.2637, -2.0244,  0.6489,  0.7297, -0.3694,  0.0847,  1.0518, -0.7746,\n                       3.1066,  3.0563, -0.2242, -2.4889, -1.9654, -0.0437,  1.0197, -0.4513,\n                      -1.4427, -1.9417,  1.0683,  3.6514,  0.2091,  1.4452,  0.4692,  1.9629,\n                       0.1623, -1.0205, -1.3095, -1.1074, -0.4764, -2.7274, -1.9927,  1.1846,\n                      -1.6743, -1.3058, -3.8607,  0.4144, -1.1386,  1.0525,  1.7616,  2.4047,\n                      -2.1430, -0.4756,  0.9182,  0.8848, -1.3968, -0.0123,  2.8792, -1.7109,\n                       0.5551, -1.9909,  2.4841, -0.9090,  0.1476, -0.0997, -0.6118,  1.7555,\n                      -0.6405,  1.3658, -1.8468,  0.3641,  2.7887, -0.0601, -0.7354,  1.9887,\n                      -0.1684, -0.2075, -0.2395, -0.6103,  1.3483, -0.6662, -0.0768, -0.9005,\n                      -3.2522,  2.3197, -2.2964,  1.9530,  0.0219,  1.0366, -0.7765,  0.4196,\n                      -1.3715, -0.7182,  0.9338, -0.1796,  0.9374, -2.2306, -0.5380, -2.5608,\n                      -1.7467,  0.4035,  0.3664, -0.0735, -0.8237, -1.6557, -1.3517,  2.0859,\n                      -0.9231,  1.4076,  0.2848, -2.0301, -2.5669, -1.0198,  0.4625, -1.6326,\n                      -0.6498,  0.4128,  3.7454,  1.4729, -1.1449, -0.3680,  3.1364, -0.8606,\n                      -0.1159, -3.0216,  3.2885, -0.5679, -1.5283,  0.2232, -1.9262,  0.8783,\n                      -1.0917, -0.4112, -1.5158, -0.2859, -0.4769, -2.1227,  1.0227, -0.6842,\n                      -1.6215,  0.3942,  0.3919, -0.6268,  1.6013, -1.7048, -2.3534,  2.0111,\n                      -0.0910, -0.3043,  0.9120,  4.3762,  0.3524,  1.1569,  0.0335, -1.1620,\n                      -3.0985,  3.0711,  2.0974, -0.9704,  2.3694, -1.5578,  0.1985,  0.7175,\n                       0.6373, -2.4867, -0.3855,  0.6901, -2.0749, -0.3411, -1.0899, -0.3115,\n                       0.5286,  1.9429, -0.6560, -0.0873, -0.8254, -0.4055, -0.6660, -0.7202,\n                       0.7063,  1.7418,  0.7304, -1.1567, -0.3535, -1.7920, -0.3340,  0.2041,\n                      -1.1995, -0.9292, -1.4141, -1.9184, -1.1817,  2.6548, -3.5953,  0.8287,\n                      -0.8412, -1.9147,  0.9861, -0.6260, -2.1507,  0.8749, -0.6156,  0.4039,\n                       0.6983, -0.0099, -0.1795,  1.5658, -0.6698,  1.3118, -0.6879,  3.6292,\n                      -0.1857, -1.2856, -1.0212, -0.5290,  1.7328, -0.8172,  1.0298,  1.3943,\n                       0.4583, -0.4287,  0.3401,  4.3562,  2.7597, -0.3871, -1.4342, -0.4639,\n                       0.8391, -1.5146,  1.3693,  1.4347, -2.4331,  0.6940,  1.9184,  1.7705,\n                      -2.1914, -0.4392,  3.3099,  0.5725, -1.0552, -1.3834,  4.0077,  0.5148,\n                      -2.6391,  1.3447, -1.5139,  1.1775, -0.2003, -0.2372,  0.8112, -1.6052])),\n             ('c5.1.running_var',\n              tensor([0.9412, 0.8000, 1.1703, 1.2622, 0.7958, 0.9468, 1.7386, 1.1757, 3.9408,\n                      2.1393, 0.9202, 1.5456, 0.7789, 1.4193, 1.9427, 1.5747, 2.2422, 4.9712,\n                      3.1089, 2.0676, 1.0050, 1.2712, 2.1390, 1.7916, 1.2447, 2.1348, 1.5619,\n                      1.3447, 1.6696, 1.1554, 1.9825, 1.7965, 1.8249, 0.9300, 1.5375, 1.7713,\n                      1.7684, 1.2607, 1.2321, 2.6009, 1.0469, 1.0638, 1.8773, 1.0945, 1.2674,\n                      1.5457, 1.7525, 1.3968, 1.4499, 2.4473, 2.9605, 0.8372, 1.7854, 1.1932,\n                      2.6641, 0.8283, 1.8252, 2.9302, 1.5385, 1.3211, 1.7939, 1.7359, 1.1909,\n                      1.0138, 1.3526, 2.7941, 1.1726, 1.8437, 1.5826, 1.9079, 2.0511, 1.7460,\n                      1.0228, 1.3628, 2.2449, 1.0891, 1.9875, 0.9862, 3.8067, 0.9682, 1.0425,\n                      1.8591, 1.1630, 0.8982, 1.6620, 0.8455, 3.7104, 1.1755, 1.3407, 2.7343,\n                      1.7188, 1.2431, 1.5709, 1.8484, 1.8446, 1.0439, 2.7623, 1.5549, 1.0370,\n                      1.1053, 1.5844, 2.2290, 1.7198, 1.5880, 1.4053, 1.8420, 2.6480, 1.1787,\n                      1.2981, 2.5682, 1.3545, 1.1796, 1.6269, 1.5284, 1.5386, 1.1967, 2.6993,\n                      2.3031, 1.9211, 2.1274, 1.3919, 1.7982, 1.3795, 1.7764, 1.8517, 0.9803,\n                      1.2188, 1.1520, 1.2165, 1.6176, 3.0376, 2.5636, 1.7316, 1.7531, 3.2574,\n                      2.2429, 1.7384, 1.6609, 3.1593, 2.4726, 1.6704, 1.4093, 1.6995, 2.1341,\n                      2.1726, 1.0185, 1.5768, 1.5688, 1.8551, 1.0882, 1.4286, 0.9410, 1.8362,\n                      2.1263, 1.0524, 2.3837, 1.0217, 1.6428, 1.1264, 1.6923, 1.9336, 1.5211,\n                      1.2745, 1.1673, 1.1543, 2.0061, 0.7459, 1.7525, 1.0324, 1.7674, 2.6286,\n                      0.9658, 2.6581, 2.1792, 1.5852, 2.1273, 1.7525, 1.9918, 3.3789, 2.3689,\n                      2.0205, 1.6425, 1.3640, 1.7416, 1.2057, 1.6266, 2.0464, 1.5962, 1.3831,\n                      1.8151, 1.5453, 3.3696, 1.7196, 1.3437, 0.8900, 0.9801, 1.0282, 1.7254,\n                      2.4954, 1.3604, 0.7670, 1.8181, 1.9402, 2.8759, 0.9924, 1.5937, 2.7246,\n                      2.3013, 1.5268, 0.8680, 1.2039, 0.8897, 1.4021, 0.9512, 1.4904, 2.1111,\n                      1.1840, 1.0373, 1.3629, 1.3844, 0.8785, 1.1890, 2.2942, 1.5594, 2.5865,\n                      1.0638, 2.6105, 1.8002, 1.2436, 1.8194, 1.3012, 1.2567, 1.7187, 1.9080,\n                      1.0159, 1.1681, 2.4156, 1.1878, 1.3554, 0.9790, 1.6187, 1.6442, 2.0008,\n                      1.6956, 1.6382, 0.7955, 1.4787, 1.9175, 1.2218, 1.0169, 1.2954, 1.8726,\n                      1.5841, 1.4645, 2.3579, 2.7465])),\n             ('c5.1.num_batches_tracked', tensor(3700)),\n             ('c6.0.weight',\n              tensor([[[[-3.4109e-02, -5.1231e-02, -2.3484e-02],\n                        [-1.2317e-02, -2.9217e-02,  3.1841e-03],\n                        [ 3.5225e-02, -2.2893e-03, -2.3948e-02]],\n              \n                       [[-1.5737e-01, -7.6003e-02, -2.7335e-04],\n                        [-6.0462e-02, -1.3182e-03, -5.5193e-02],\n                        [-6.2271e-02, -3.7376e-02,  9.3551e-02]],\n              \n                       [[ 3.4113e-02, -5.6093e-02,  2.6887e-02],\n                        [-7.2826e-02, -6.3402e-02, -1.7990e-02],\n                        [-2.2350e-02, -1.1793e-02, -4.9840e-03]],\n              \n                       ...,\n              \n                       [[ 8.7427e-02,  1.5304e-02,  1.0324e-02],\n                        [ 8.4815e-03, -1.9622e-02, -5.0794e-02],\n                        [-2.7534e-02,  8.5932e-03,  1.2981e-02]],\n              \n                       [[ 2.0269e-02, -1.9294e-02,  4.6825e-02],\n                        [ 3.8545e-02, -9.4175e-02,  8.4832e-03],\n                        [ 1.3954e-02, -3.4857e-02, -6.8713e-02]],\n              \n                       [[ 3.4805e-02,  4.4326e-02,  1.4452e-02],\n                        [ 5.3149e-03, -1.1438e-02, -9.1247e-03],\n                        [ 1.6163e-02,  4.8558e-03, -8.5871e-03]]],\n              \n              \n                      [[[-4.8309e-02, -5.1131e-02, -3.0562e-03],\n                        [-1.5306e-02, -2.9057e-02, -4.3972e-02],\n                        [-1.4528e-02,  1.0232e-02, -7.4127e-02]],\n              \n                       [[ 4.5434e-02,  2.8396e-02, -4.9445e-02],\n                        [ 1.0981e-02,  7.1213e-02, -4.2911e-02],\n                        [ 4.7239e-02, -4.9682e-02,  9.6908e-03]],\n              \n                       [[-2.8926e-02,  1.1002e-02,  1.1488e-01],\n                        [-9.3996e-02, -1.0086e-02,  5.0172e-02],\n                        [-1.9156e-02, -1.8413e-02, -4.8497e-02]],\n              \n                       ...,\n              \n                       [[ 8.5039e-02,  4.8683e-02,  6.3399e-03],\n                        [ 2.4529e-02,  6.5191e-02,  2.7876e-02],\n                        [ 5.3617e-02,  4.0349e-02,  1.2774e-02]],\n              \n                       [[ 1.0629e-02, -8.4602e-02, -8.4910e-02],\n                        [-6.7168e-02,  2.1032e-02, -3.1785e-02],\n                        [-1.2795e-02, -2.5525e-02,  2.6679e-02]],\n              \n                       [[-4.5566e-02, -3.5119e-02, -3.3754e-02],\n                        [ 3.6433e-02,  4.5959e-02, -3.2348e-02],\n                        [ 3.1087e-02, -7.0063e-03, -1.4172e-02]]],\n              \n              \n                      [[[ 9.1798e-03, -1.9938e-02,  7.4981e-02],\n                        [-4.9483e-02, -4.1896e-02, -4.8830e-02],\n                        [-8.4914e-02, -3.0453e-02,  5.4649e-02]],\n              \n                       [[-5.6107e-03,  2.7740e-03, -3.5551e-02],\n                        [-1.5243e-02, -5.1405e-05, -5.5770e-02],\n                        [-3.1675e-02,  5.7801e-02, -6.4280e-02]],\n              \n                       [[-3.3168e-02, -2.6386e-02,  3.1395e-02],\n                        [-6.9053e-02,  6.7815e-02, -3.7309e-02],\n                        [-1.1344e-02, -3.3035e-02,  2.5005e-03]],\n              \n                       ...,\n              \n                       [[ 3.5381e-03,  3.8006e-02,  2.7430e-02],\n                        [ 1.0113e-02, -1.3575e-02,  8.0460e-03],\n                        [ 5.3339e-02, -8.3910e-02, -1.6407e-02]],\n              \n                       [[-3.9026e-02,  5.1243e-02, -5.8107e-02],\n                        [ 7.9422e-02,  3.1133e-02,  2.3774e-02],\n                        [-5.4358e-02, -8.6377e-03,  1.4124e-02]],\n              \n                       [[ 5.9401e-02,  4.6771e-02,  3.8154e-02],\n                        [-4.6389e-02, -3.9870e-02, -1.8954e-03],\n                        [ 2.8161e-02, -3.4307e-02, -2.2172e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.1201e-02,  3.8785e-02,  7.9268e-02],\n                        [-6.4770e-02, -4.2541e-02,  1.5149e-03],\n                        [-3.9019e-02, -4.5781e-02, -1.8311e-02]],\n              \n                       [[-5.6423e-03,  5.1176e-02, -2.4032e-02],\n                        [ 7.3877e-02,  4.6853e-02,  3.8927e-03],\n                        [-1.4135e-02, -4.2345e-02,  3.0549e-02]],\n              \n                       [[-8.8908e-02, -3.4915e-02,  3.9058e-02],\n                        [ 2.0778e-02, -1.0946e-02, -4.0985e-02],\n                        [-8.4219e-02, -2.2325e-02,  3.8357e-03]],\n              \n                       ...,\n              \n                       [[ 6.6218e-03, -3.8999e-02, -1.8197e-02],\n                        [ 1.7657e-02,  6.1048e-03,  6.1813e-02],\n                        [ 1.4427e-02, -4.1450e-02,  5.5723e-02]],\n              \n                       [[ 1.7560e-02,  4.1780e-02,  5.7318e-02],\n                        [ 9.0520e-03,  3.6710e-02,  9.2453e-02],\n                        [ 1.7068e-02,  1.0709e-02,  3.9663e-02]],\n              \n                       [[-4.5289e-02, -3.9844e-03,  2.7770e-02],\n                        [-7.8382e-02,  2.6687e-02, -2.5984e-02],\n                        [-3.5188e-02, -5.1953e-02, -2.1414e-02]]],\n              \n              \n                      [[[ 2.8886e-02, -7.2618e-03,  6.6188e-02],\n                        [-1.7925e-02, -4.5010e-02,  6.6013e-03],\n                        [ 3.6041e-02,  4.2524e-02,  1.0800e-02]],\n              \n                       [[ 9.3415e-03, -1.6818e-02, -5.6889e-02],\n                        [-7.6762e-02, -4.2841e-02,  6.9782e-04],\n                        [-4.1223e-02,  2.0305e-02, -3.2275e-02]],\n              \n                       [[ 1.6392e-02,  1.9322e-02,  1.0422e-02],\n                        [ 2.0220e-02, -1.9508e-02,  2.2293e-04],\n                        [-1.1217e-02, -6.9773e-02, -7.2536e-02]],\n              \n                       ...,\n              \n                       [[ 5.1264e-02,  2.3736e-02,  1.4226e-02],\n                        [-2.6160e-02,  1.9929e-02,  6.3993e-03],\n                        [ 1.3793e-02, -6.5307e-03,  3.8083e-02]],\n              \n                       [[-8.4121e-02,  5.3141e-02, -2.8077e-02],\n                        [ 3.7398e-02, -3.0805e-02,  6.6655e-02],\n                        [-7.2761e-02, -5.5142e-02,  2.7960e-02]],\n              \n                       [[ 1.9545e-02,  2.7088e-03,  2.1220e-02],\n                        [-2.3317e-02, -2.6352e-02,  7.4332e-03],\n                        [-4.0744e-02,  3.8039e-02, -5.4131e-02]]],\n              \n              \n                      [[[ 4.2193e-02, -2.2693e-02, -3.8514e-02],\n                        [ 2.1886e-02,  5.2722e-02,  7.8457e-04],\n                        [-2.2630e-02, -1.0772e-02, -3.7006e-02]],\n              \n                       [[-1.2769e-02, -5.6986e-02, -1.0648e-02],\n                        [-1.3312e-02, -3.3679e-02,  8.4241e-02],\n                        [-5.7692e-02, -2.5001e-02,  8.8871e-03]],\n              \n                       [[-2.8873e-03, -2.9335e-02, -2.0564e-03],\n                        [ 5.6373e-02, -1.9984e-02, -2.6840e-02],\n                        [-3.4057e-02,  3.2558e-03,  5.1432e-03]],\n              \n                       ...,\n              \n                       [[ 8.8324e-02, -3.4487e-02,  1.0248e-02],\n                        [-1.9201e-02,  1.5816e-02,  5.7326e-03],\n                        [ 5.2216e-02,  9.1253e-03,  6.0964e-02]],\n              \n                       [[ 3.9039e-02,  2.9538e-02, -2.7166e-02],\n                        [ 2.4268e-02,  7.7776e-02, -2.3697e-03],\n                        [-3.2812e-02,  6.1050e-02,  9.5571e-02]],\n              \n                       [[ 2.9532e-02, -8.0642e-03,  6.4947e-02],\n                        [-3.8763e-02,  7.3030e-02,  1.5519e-02],\n                        [-2.1118e-04, -4.3136e-02,  1.8899e-02]]]])),\n             ('c6.0.bias',\n              tensor([-0.0120,  0.0071,  0.0094, -0.0005,  0.0116,  0.0050,  0.0045,  0.0040,\n                      -0.0024, -0.0082,  0.0025, -0.0018,  0.0047,  0.0053,  0.0052,  0.0059,\n                       0.0118, -0.0199, -0.0183,  0.0106, -0.0198, -0.0181, -0.0199,  0.0177,\n                       0.0095, -0.0033,  0.0196,  0.0172,  0.0203,  0.0102,  0.0127,  0.0125,\n                      -0.0192,  0.0120,  0.0025,  0.0186, -0.0072,  0.0068,  0.0195,  0.0081,\n                      -0.0087,  0.0144,  0.0051,  0.0187, -0.0034,  0.0020,  0.0138,  0.0110,\n                       0.0032,  0.0089,  0.0102,  0.0165, -0.0068, -0.0147, -0.0166, -0.0115,\n                       0.0157,  0.0177, -0.0164, -0.0102, -0.0029, -0.0150,  0.0118, -0.0038,\n                       0.0137,  0.0100,  0.0174, -0.0087,  0.0188, -0.0081, -0.0114,  0.0057,\n                      -0.0018,  0.0027,  0.0113, -0.0199,  0.0136, -0.0062, -0.0047, -0.0005,\n                      -0.0168,  0.0169,  0.0141,  0.0040, -0.0142,  0.0176,  0.0090,  0.0101,\n                      -0.0046, -0.0044,  0.0101, -0.0078, -0.0197,  0.0192,  0.0051, -0.0037,\n                       0.0198,  0.0020,  0.0146, -0.0202,  0.0104,  0.0044,  0.0156,  0.0165,\n                      -0.0151,  0.0109,  0.0137,  0.0148,  0.0165, -0.0070, -0.0163, -0.0085,\n                       0.0128, -0.0205,  0.0023,  0.0088, -0.0162, -0.0071,  0.0181, -0.0154,\n                       0.0119,  0.0047, -0.0152,  0.0050, -0.0043,  0.0197,  0.0060,  0.0070])),\n             ('c6.1.weight',\n              tensor([1.0043, 1.0477, 1.0224, 1.0541, 1.0379, 1.0340, 1.0383, 1.0314, 1.0207,\n                      1.0188, 1.0843, 1.0165, 1.0205, 1.0110, 1.0244, 1.0022, 1.0453, 1.0014,\n                      1.0329, 1.0408, 1.0326, 1.0313, 1.0306, 1.0327, 1.0566, 1.0936, 1.0622,\n                      1.0425, 1.0244, 1.0646, 1.0006, 1.0189, 0.9848, 1.0114, 1.0242, 1.0281,\n                      1.0120, 1.0519, 1.0057, 1.0343, 1.0281, 1.0577, 1.0112, 0.9961, 1.0128,\n                      1.0390, 1.0096, 1.0332, 1.0394, 1.0150, 1.0263, 1.0465, 1.0310, 0.9982,\n                      1.0506, 1.0176, 1.0323, 1.0224, 1.0597, 1.0861, 1.0479, 1.0228, 1.0493,\n                      1.0192, 1.0453, 1.0296, 1.0198, 1.0341, 1.0562, 1.0262, 1.0132, 1.0076,\n                      1.0154, 1.0468, 1.0129, 1.0184, 1.0601, 1.0426, 1.0233, 1.0484, 1.0728,\n                      1.0253, 1.0514, 1.0780, 1.0507, 1.0500, 1.0535, 1.0368, 1.0425, 1.0569,\n                      1.0096, 1.0415, 1.0373, 1.0066, 1.0185, 1.0446, 1.0390, 1.0721, 1.0208,\n                      1.0142, 1.0493, 1.0492, 1.0482, 1.0295, 1.0282, 1.0222, 1.0204, 1.0309,\n                      1.0383, 1.0213, 1.0337, 1.0210, 1.0429, 1.0143, 1.0452, 1.0356, 1.0606,\n                      1.0442, 1.0598, 1.0254, 1.0427, 1.0258, 1.0224, 1.0481, 1.0225, 1.0308,\n                      1.0415, 1.0146])),\n             ('c6.1.bias',\n              tensor([-0.0190, -0.0524, -0.0693, -0.0178, -0.0470, -0.0550, -0.0426, -0.0665,\n                      -0.0560, -0.0721, -0.0158, -0.0854, -0.0772, -0.0460, -0.0459, -0.0878,\n                      -0.0753, -0.0162, -0.0323, -0.0406, -0.0557, -0.0451, -0.0480, -0.0191,\n                      -0.0314, -0.0343, -0.0449, -0.0315, -0.0496, -0.0138, -0.0551, -0.0416,\n                      -0.1024, -0.0333, -0.0442, -0.0302, -0.0503,  0.0056, -0.0493, -0.0307,\n                      -0.0370, -0.0281, -0.0669, -0.0843, -0.0837, -0.0042, -0.0666, -0.0061,\n                      -0.0256, -0.0247, -0.0523,  0.0014, -0.0431, -0.0214, -0.0464, -0.0372,\n                      -0.0182, -0.0276, -0.0683,  0.0044, -0.0204, -0.0455, -0.0395, -0.0879,\n                      -0.0313, -0.0276, -0.0048, -0.0367, -0.0557, -0.0527, -0.0210, -0.0168,\n                      -0.0330, -0.0456, -0.0322, -0.0786, -0.0430, -0.0508, -0.0508,  0.0077,\n                      -0.0396, -0.0259, -0.0550,  0.0057, -0.0150, -0.0275, -0.0202, -0.0138,\n                       0.0009, -0.0450, -0.0486, -0.0122, -0.0423, -0.0511, -0.0384, -0.0165,\n                      -0.0586, -0.0007, -0.0585, -0.0553, -0.0096, -0.0173, -0.0409, -0.0460,\n                      -0.0297, -0.0312, -0.0645, -0.0145, -0.0289, -0.0302,  0.0103, -0.0527,\n                      -0.0354, -0.0142, -0.0528, -0.0286, -0.0341, -0.0147,  0.0106, -0.0693,\n                      -0.0477, -0.0285, -0.0031, -0.0209, -0.0137, -0.0246, -0.0175, -0.0375])),\n             ('c6.1.running_mean',\n              tensor([-2.8095, -0.3043,  1.4131, -2.4199,  0.6670, -2.6104,  1.8848, -0.9204,\n                      -2.3269, -3.8548, -1.4693,  1.5164,  0.4200,  2.8122, -0.6328,  2.1837,\n                       0.2796, -2.1109,  0.5354, -0.7416, -0.3012, -0.7475, -1.9983, -0.2620,\n                       0.1115, -0.8132, -2.9883, -0.7043,  4.0843, -1.5157,  0.3616, -1.6192,\n                       0.5365, -0.5048,  0.3366, -0.2187, -3.7298, -1.8043, -1.3301,  1.5474,\n                       1.1210,  0.8959, -0.6470, -1.2977,  1.2483, -0.8663,  3.5078, -0.4035,\n                      -0.0820,  0.3396, -0.2270, -3.6042,  0.8148, -0.7053,  0.4108, -0.3914,\n                       0.1704, -0.3445,  1.8105, -0.9119, -2.6512,  1.0518, -0.3410,  2.5044,\n                      -1.8009, -0.3555, -0.9895, -0.5351, -0.6799, -1.2987, -1.6860,  2.8017,\n                       3.0489,  0.6417, -2.3505,  2.1254,  0.7982, -2.0012,  4.8806, -2.8314,\n                       0.9051, -2.1763, -1.4913, -1.5308,  0.3384,  0.1971, -1.7013, -2.2567,\n                      -2.6202,  0.5086,  2.3705,  0.0569,  1.1344, -0.2227, -0.9835, -0.5713,\n                       0.4220, -3.6169, -0.6623, -0.6104,  0.2509, -0.5812,  1.7284,  0.1640,\n                      -2.0664, -3.0698,  0.4413, -4.2964, -0.5337,  3.8119, -1.5806,  0.9706,\n                      -1.4184, -1.2516,  2.6261, -0.8636, -3.1999,  1.6758, -1.3646,  0.6975,\n                       1.0592,  0.8966, -0.1105, -0.0920,  2.7215, -0.0074, -2.7584,  2.3284])),\n             ('c6.1.running_var',\n              tensor([5.6611, 3.4424, 2.8366, 3.0103, 3.8697, 3.4553, 4.1391, 3.4172, 3.1283,\n                      4.2197, 4.6560, 3.9635, 2.9002, 3.6557, 3.1530, 4.1835, 2.6161, 7.4100,\n                      4.4582, 3.8848, 2.7197, 4.9864, 3.3502, 4.5886, 3.0624, 2.7135, 3.5654,\n                      4.1983, 4.4524, 4.2677, 4.9071, 3.3980, 4.3709, 3.6463, 3.2607, 3.8940,\n                      3.8112, 5.4250, 2.9295, 5.0960, 4.7649, 2.8586, 3.8662, 2.8919, 3.8522,\n                      3.8255, 3.3861, 3.0370, 2.9320, 4.9961, 4.1410, 4.9569, 4.2138, 5.0075,\n                      3.7864, 3.1629, 3.5294, 3.2426, 2.9005, 3.6771, 4.5624, 3.9198, 2.9946,\n                      3.7414, 3.5926, 3.9979, 5.2427, 3.2960, 3.8554, 3.1154, 4.7832, 7.4303,\n                      4.4043, 3.8217, 3.5320, 4.1712, 3.2864, 3.2469, 4.4715, 2.6478, 4.1247,\n                      3.4927, 3.1566, 4.2901, 4.0055, 3.2675, 3.7104, 4.3101, 4.1032, 3.6994,\n                      3.8608, 3.3157, 3.5349, 3.2107, 2.7734, 2.9871, 3.3161, 3.3873, 3.6146,\n                      3.5073, 6.4777, 3.2369, 2.7756, 3.0474, 2.7770, 4.8288, 3.2317, 4.6254,\n                      3.3332, 3.5010, 6.2717, 3.5485, 3.3707, 3.6919, 3.0679, 4.0431, 3.6580,\n                      6.8583, 4.3854, 4.2890, 3.3007, 5.1774, 6.4783, 3.4922, 4.8958, 5.5864,\n                      3.1861, 8.2657])),\n             ('c6.1.num_batches_tracked', tensor(3700)),\n             ('c7.0.weight',\n              tensor([[[[-4.9342e-03, -7.3633e-02, -2.5082e-02],\n                        [-4.7779e-02, -3.0038e-02, -1.0863e-01],\n                        [ 3.2238e-02, -5.9456e-02, -5.9371e-02]],\n              \n                       [[ 3.8367e-02,  1.0672e-01, -6.2141e-03],\n                        [ 9.3163e-04,  8.9780e-02,  1.8998e-02],\n                        [ 2.0185e-02, -7.5334e-03,  1.0393e-01]],\n              \n                       [[-2.7139e-02,  4.1266e-02, -3.8642e-03],\n                        [ 1.5544e-01,  2.4641e-02, -1.9729e-03],\n                        [ 1.9946e-02,  7.0317e-02,  6.2962e-02]],\n              \n                       ...,\n              \n                       [[-2.0997e-02, -1.0694e-02,  2.9037e-02],\n                        [ 6.3102e-02, -4.0487e-02,  3.2732e-02],\n                        [ 2.2858e-02,  3.6201e-02,  4.8957e-02]],\n              \n                       [[ 5.8594e-02, -4.6450e-02,  3.2083e-02],\n                        [-5.8783e-02, -2.2693e-02,  1.9857e-02],\n                        [ 3.3324e-03, -7.5567e-02,  5.7448e-02]],\n              \n                       [[ 9.1875e-03, -4.1665e-02, -2.2008e-02],\n                        [-2.1841e-02, -1.3141e-01, -1.7834e-02],\n                        [ 4.4225e-02,  3.1687e-02,  1.0518e-02]]],\n              \n              \n                      [[[-4.5160e-02, -3.8480e-02, -3.0599e-02],\n                        [ 9.0329e-02, -7.5121e-04, -1.5855e-02],\n                        [ 3.0492e-03, -2.2919e-02,  7.6059e-02]],\n              \n                       [[-3.2362e-02, -1.2102e-02,  2.5296e-02],\n                        [-2.9097e-02, -1.7856e-02,  4.5990e-02],\n                        [ 5.1988e-02,  6.5943e-02, -1.0540e-02]],\n              \n                       [[-8.2279e-02, -7.8462e-02,  4.8289e-02],\n                        [-1.5350e-02, -4.2307e-02, -9.3304e-02],\n                        [-2.5906e-03, -9.1802e-02,  1.5966e-02]],\n              \n                       ...,\n              \n                       [[-6.4651e-02, -5.5532e-02,  1.1412e-02],\n                        [-8.9157e-03, -4.2524e-02,  6.9399e-03],\n                        [ 6.0843e-03,  1.2355e-03,  2.1919e-02]],\n              \n                       [[-3.4052e-04, -6.7658e-02, -3.1676e-02],\n                        [-5.2125e-02,  3.6051e-02, -2.8154e-02],\n                        [-3.6834e-02, -1.2776e-02,  4.8920e-03]],\n              \n                       [[ 3.7333e-02, -9.6490e-02, -4.1092e-02],\n                        [-2.6082e-02,  7.9413e-03, -4.1153e-04],\n                        [ 8.3327e-02,  3.6655e-02,  2.7450e-02]]],\n              \n              \n                      [[[ 7.3213e-03, -5.7582e-02,  4.4378e-02],\n                        [ 5.6722e-02,  3.1981e-02,  3.5853e-02],\n                        [-3.0989e-02,  2.1317e-02,  4.6545e-02]],\n              \n                       [[ 1.0690e-02,  3.6081e-03,  4.1007e-02],\n                        [-2.5396e-02, -5.0007e-02,  2.8829e-02],\n                        [ 5.7347e-02,  6.6903e-02, -3.2999e-02]],\n              \n                       [[-2.5605e-03, -9.7700e-03, -1.7200e-02],\n                        [-2.1275e-02,  1.5612e-02, -3.5743e-02],\n                        [ 5.7171e-02, -3.3436e-02,  1.0982e-02]],\n              \n                       ...,\n              \n                       [[-6.8364e-02,  3.7643e-02, -3.6236e-03],\n                        [-3.7828e-02, -6.8187e-02,  6.2886e-03],\n                        [-9.1007e-02, -1.0307e-01, -2.1826e-02]],\n              \n                       [[ 3.1905e-02, -6.7484e-03, -3.7700e-02],\n                        [ 4.1561e-03,  6.0422e-02,  3.7145e-02],\n                        [ 1.5654e-02,  1.3376e-02, -2.4106e-02]],\n              \n                       [[ 6.9029e-02,  9.4438e-02,  1.6959e-02],\n                        [ 1.3750e-02, -3.5139e-02,  5.9290e-02],\n                        [ 1.6790e-02,  2.8719e-02,  3.7408e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.2662e-03,  3.7809e-02,  6.4058e-03],\n                        [-1.8871e-02, -6.3298e-02, -4.7679e-02],\n                        [ 3.3260e-03, -9.5571e-03, -1.8528e-02]],\n              \n                       [[ 7.0169e-02,  1.9997e-02,  1.8788e-02],\n                        [-2.7133e-02,  7.5079e-02,  5.3935e-02],\n                        [ 2.8947e-02,  5.9866e-02,  4.6810e-02]],\n              \n                       [[-5.3103e-02, -2.3975e-03,  8.3324e-02],\n                        [-3.6958e-02, -1.7970e-02, -2.5037e-02],\n                        [ 4.6774e-02,  9.1214e-02,  1.1072e-01]],\n              \n                       ...,\n              \n                       [[-5.6402e-03,  3.8650e-02, -4.0025e-02],\n                        [-6.8111e-02,  1.7267e-02, -2.1407e-02],\n                        [ 4.5570e-02,  3.0586e-02,  5.8745e-02]],\n              \n                       [[ 1.1312e-02,  1.0879e-01, -8.4369e-02],\n                        [-1.8206e-02,  3.7718e-02,  3.5670e-02],\n                        [ 1.6536e-02,  8.4715e-03,  1.6129e-02]],\n              \n                       [[-3.2306e-02,  4.1509e-02,  2.1749e-02],\n                        [-2.3708e-02,  8.3612e-03,  2.3358e-03],\n                        [-4.3126e-02,  1.0453e-02, -1.6209e-02]]],\n              \n              \n                      [[[ 6.6637e-02,  7.0807e-02, -3.5046e-02],\n                        [-2.5050e-03, -1.2632e-02, -3.2839e-02],\n                        [ 3.7772e-02,  1.0419e-02, -3.0317e-02]],\n              \n                       [[ 2.5754e-03,  1.7623e-02, -1.4613e-02],\n                        [ 2.1622e-02,  1.4156e-02,  1.6292e-02],\n                        [-8.1205e-03,  1.8034e-02,  2.3665e-02]],\n              \n                       [[-4.7249e-02,  2.2618e-02,  1.8672e-02],\n                        [ 9.2326e-02, -2.8971e-02, -7.4309e-03],\n                        [ 1.9754e-02,  1.0383e-02,  2.6420e-02]],\n              \n                       ...,\n              \n                       [[-4.7254e-02, -4.1802e-02,  5.6582e-02],\n                        [-2.6130e-02, -8.4238e-03, -5.0128e-02],\n                        [-3.3740e-02, -3.5942e-02,  3.5360e-02]],\n              \n                       [[ 1.3273e-03,  4.9281e-02, -8.8201e-03],\n                        [ 9.9374e-02,  1.7210e-02,  2.4680e-02],\n                        [ 3.2689e-02,  6.8978e-02,  7.2970e-03]],\n              \n                       [[-8.3953e-02, -9.2570e-03, -8.5396e-02],\n                        [ 3.5872e-02, -3.3504e-02, -1.5986e-02],\n                        [-9.9466e-02,  6.8246e-02, -2.8075e-02]]],\n              \n              \n                      [[[ 4.0647e-02,  3.2984e-03, -5.8396e-02],\n                        [-7.0363e-05,  5.1667e-02,  5.7882e-02],\n                        [-4.1534e-02, -7.2167e-03,  1.0828e-02]],\n              \n                       [[ 1.4719e-02, -5.6780e-02, -4.1513e-02],\n                        [ 5.5061e-02,  5.1020e-02, -6.4791e-02],\n                        [ 3.3523e-02,  4.4740e-02, -2.4631e-02]],\n              \n                       [[-5.2200e-02, -3.1140e-02,  9.5953e-03],\n                        [-3.7332e-02, -3.3104e-02, -2.5137e-02],\n                        [-7.2177e-02, -3.0045e-02,  5.5225e-02]],\n              \n                       ...,\n              \n                       [[ 2.0930e-02,  4.3701e-02,  2.2923e-02],\n                        [-7.0302e-02, -4.6932e-02, -2.8659e-02],\n                        [ 4.3747e-02, -2.2424e-02,  1.8399e-02]],\n              \n                       [[-4.7729e-02, -4.0207e-02,  3.0537e-02],\n                        [-5.2836e-02,  1.6550e-02,  8.1945e-02],\n                        [ 1.0432e-01,  9.0281e-02,  2.3803e-02]],\n              \n                       [[-4.6860e-02,  8.3680e-02,  3.8278e-02],\n                        [ 2.5559e-02,  4.8454e-02,  4.1557e-03],\n                        [-2.3125e-03, -4.0689e-02,  2.8707e-02]]]])),\n             ('c7.0.bias',\n              tensor([ 0.0120,  0.0039, -0.0122,  0.0076,  0.0151,  0.0124, -0.0026, -0.0147,\n                      -0.0206, -0.0010,  0.0120,  0.0078, -0.0026, -0.0184,  0.0196, -0.0108,\n                      -0.0193, -0.0060, -0.0155,  0.0006, -0.0081, -0.0190,  0.0187, -0.0170,\n                      -0.0142, -0.0080,  0.0029,  0.0081,  0.0191, -0.0015,  0.0038, -0.0056,\n                       0.0035,  0.0077, -0.0160,  0.0071,  0.0166, -0.0053,  0.0022, -0.0100,\n                       0.0080, -0.0090, -0.0208, -0.0023,  0.0069, -0.0051, -0.0009,  0.0172,\n                       0.0006, -0.0098, -0.0088, -0.0028, -0.0108, -0.0141,  0.0206,  0.0029,\n                       0.0096, -0.0161,  0.0028, -0.0169, -0.0092, -0.0132, -0.0050, -0.0038,\n                       0.0040,  0.0125,  0.0118,  0.0053,  0.0078,  0.0203, -0.0027,  0.0054,\n                       0.0039, -0.0167,  0.0202, -0.0183, -0.0143,  0.0123, -0.0169,  0.0055,\n                      -0.0100,  0.0006,  0.0111,  0.0112, -0.0116,  0.0041, -0.0159, -0.0060,\n                       0.0104, -0.0185,  0.0014, -0.0190,  0.0098, -0.0068, -0.0085, -0.0162,\n                       0.0133,  0.0186, -0.0159,  0.0084, -0.0014,  0.0111, -0.0204,  0.0035,\n                       0.0175,  0.0021,  0.0004,  0.0192, -0.0116,  0.0014,  0.0077,  0.0111,\n                       0.0081, -0.0177, -0.0123,  0.0146, -0.0171, -0.0060,  0.0194, -0.0152,\n                       0.0163, -0.0038,  0.0122,  0.0114, -0.0157, -0.0053, -0.0186,  0.0049])),\n             ('c7.1.weight',\n              tensor([0.9853, 1.0075, 0.9977, 1.0179, 1.0020, 1.0030, 1.0108, 1.0063, 0.9895,\n                      1.0323, 1.0167, 0.9969, 1.0157, 0.9844, 0.9940, 1.0187, 1.0044, 0.9957,\n                      1.0029, 0.9954, 0.9853, 0.9841, 0.9978, 1.0106, 0.9916, 1.0031, 1.0013,\n                      1.0045, 0.9992, 1.0069, 0.9982, 0.9785, 0.9786, 0.9843, 0.9943, 1.0132,\n                      0.9750, 1.0007, 0.9869, 1.0146, 1.0057, 1.0114, 1.0089, 1.0003, 1.0055,\n                      1.0047, 1.0090, 1.0050, 0.9869, 0.9933, 1.0234, 1.0214, 0.9892, 1.0199,\n                      1.0056, 1.0026, 0.9700, 0.9888, 0.9838, 0.9943, 0.9936, 0.9884, 1.0018,\n                      0.9912, 1.0168, 0.9751, 1.0081, 0.9771, 1.0044, 1.0216, 0.9961, 1.0161,\n                      0.9828, 1.0035, 0.9756, 1.0185, 1.0216, 0.9898, 0.9937, 1.0161, 1.0063,\n                      0.9997, 1.0197, 0.9937, 1.0083, 1.0241, 1.0159, 0.9969, 0.9946, 0.9929,\n                      1.0016, 0.9783, 0.9725, 1.0100, 1.0009, 1.0095, 1.0023, 0.9932, 0.9740,\n                      1.0089, 1.0366, 0.9978, 1.0058, 1.0060, 1.0100, 0.9926, 1.0043, 0.9841,\n                      0.9978, 1.0005, 0.9785, 1.0051, 1.0001, 0.9988, 1.0115, 1.0084, 1.0047,\n                      0.9889, 1.0191, 0.9954, 0.9912, 1.0064, 1.0077, 0.9878, 1.0028, 0.9967,\n                      1.0025, 0.9969])),\n             ('c7.1.bias',\n              tensor([-6.8893e-02, -4.1342e-02, -1.3281e-02, -1.8146e-02, -2.6975e-02,\n                      -3.0458e-02, -1.5754e-02, -2.8756e-02, -9.5897e-03, -9.4067e-03,\n                      -2.3900e-02, -2.9289e-02, -2.6726e-02, -3.3409e-02, -3.4407e-02,\n                      -2.6226e-05, -2.5340e-02, -4.1157e-02, -3.2222e-02, -1.9437e-02,\n                      -2.0943e-02, -2.2927e-02, -4.3012e-02, -3.9966e-02, -3.7682e-02,\n                      -3.4229e-02, -3.8937e-02, -1.9388e-02, -4.5502e-02, -2.3130e-02,\n                      -7.4219e-03, -5.8587e-02, -2.6741e-02, -2.5105e-02, -1.4898e-02,\n                      -3.9524e-02, -3.3093e-02, -2.9791e-02, -4.9975e-02, -4.1518e-02,\n                      -1.3626e-02, -5.8912e-02, -3.0538e-02, -3.5061e-02, -1.9314e-02,\n                      -1.3414e-02, -2.7898e-02, -4.3307e-03, -7.9246e-03, -1.1944e-02,\n                      -1.2995e-02,  2.6390e-03, -1.6381e-02, -1.3377e-02, -2.7083e-02,\n                      -4.3659e-02, -7.6081e-02, -1.4450e-02, -5.6841e-02, -1.4708e-02,\n                      -3.7045e-02, -1.4370e-02,  5.0627e-03, -3.2568e-02, -3.3489e-02,\n                      -3.9768e-02, -3.5822e-02, -1.2834e-02, -2.6174e-02, -1.2490e-02,\n                      -2.0040e-03, -3.3903e-02, -3.3634e-02, -6.4779e-02, -3.0190e-02,\n                      -3.3173e-02, -9.0075e-03, -5.2407e-02, -2.3709e-02, -4.4041e-02,\n                      -2.9999e-02, -3.7823e-02, -2.2202e-02, -8.4240e-03, -1.2962e-02,\n                      -2.2149e-02, -1.4259e-02, -4.2333e-02, -1.5012e-02, -3.2168e-02,\n                      -8.6240e-03, -5.2501e-02, -5.5106e-02, -3.7508e-02, -2.4088e-02,\n                      -1.3881e-02, -4.3377e-02, -6.2838e-02, -2.0229e-02, -1.5322e-02,\n                      -1.4681e-02, -1.8462e-02, -1.3396e-02, -1.6665e-02, -5.2953e-02,\n                      -9.6745e-03, -3.1302e-02, -2.5268e-02, -4.8807e-02, -9.4926e-03,\n                      -2.4349e-02, -1.3860e-02, -5.7737e-02, -6.5360e-02, -5.3270e-02,\n                      -3.8208e-02, -2.8427e-02, -4.0796e-02, -2.5423e-02, -4.8875e-02,\n                      -2.9159e-02, -1.6264e-02,  1.0550e-02, -2.3669e-02, -2.8549e-02,\n                      -2.6332e-02,  1.5534e-02, -2.4517e-02])),\n             ('c7.1.running_mean',\n              tensor([ 0.0672, -1.3685,  0.5166, -0.0225, -0.0921,  0.2225,  0.1994, -1.0420,\n                       0.7081, -2.3248,  0.2684, -0.5433,  0.7277,  1.1150,  0.1564, -2.2654,\n                      -1.4610,  0.2212,  0.9205, -2.3841, -0.8220, -0.4790,  0.7043, -0.0931,\n                       0.9455, -2.7159, -0.4507,  0.5154,  1.7770, -0.9914, -0.6868,  0.9453,\n                      -0.5198, -1.4002, -1.1411, -0.0554, -0.5826, -0.7522,  0.1955, -1.3199,\n                      -0.2243,  0.2559,  0.1027, -0.3486,  1.1137, -1.2722,  3.0841, -1.3189,\n                      -1.2838,  0.4545, -0.1891, -2.0205, -0.0606, -1.3171,  0.4457, -0.1953,\n                      -0.0955,  0.8232, -0.0679, -1.3922, -0.3527, -1.1101, -1.9613,  0.8073,\n                       1.2736,  0.3308, -0.9046, -2.2968, -1.4345,  0.1618, -0.7771, -1.4072,\n                       0.8019,  0.2033, -0.1408, -0.3431,  0.0741,  1.2268, -0.7329, -1.6769,\n                      -0.4922,  0.4893, -1.4670, -1.3671, -1.2597,  0.0388, -1.5691, -0.8061,\n                      -0.9505, -1.0732,  0.2806, -0.1003,  1.3325, -0.3980, -0.8526, -0.2040,\n                       0.0673,  0.6795,  0.3948, -1.6942, -0.4857, -0.7535, -0.7880,  0.9365,\n                       0.8180, -0.2305,  0.2937, -1.1559, -1.0994, -2.0341,  0.0040, -0.6813,\n                       0.5170, -0.5775, -1.0979,  0.6520,  0.0242,  0.1935,  0.3960, -0.5902,\n                      -0.0178,  1.7426, -3.0486,  0.8073, -0.0234, -1.1453, -2.7574,  2.1706])),\n             ('c7.1.running_var',\n              tensor([2.4998, 2.4952, 2.8946, 1.7656, 2.5588, 2.2205, 3.2606, 2.1256, 2.9832,\n                      2.4598, 2.3328, 3.2519, 2.0123, 2.0836, 1.9582, 6.0704, 2.8209, 2.6850,\n                      2.6066, 5.9372, 2.8372, 2.6498, 1.9826, 2.2986, 3.3148, 2.3615, 2.9438,\n                      2.8081, 1.7928, 1.7249, 3.2574, 1.8879, 3.3191, 2.3360, 2.9620, 2.5783,\n                      2.1597, 2.6415, 1.6853, 2.5888, 2.9085, 2.4523, 2.2477, 2.3571, 3.2856,\n                      2.6009, 3.1110, 2.5643, 2.8051, 3.3243, 3.3432, 2.0709, 2.2455, 3.1853,\n                      2.2983, 1.9129, 1.6895, 2.0619, 3.2817, 3.1221, 2.0010, 3.2451, 3.8430,\n                      2.7212, 2.5351, 3.4535, 2.6451, 2.9901, 3.0167, 2.5883, 2.4730, 2.3743,\n                      2.0096, 2.1048, 3.8144, 1.8835, 2.4182, 3.3190, 2.1905, 2.2870, 2.0832,\n                      2.0702, 2.8889, 2.1785, 2.6666, 2.3516, 2.1487, 2.7217, 5.1590, 2.6931,\n                      2.3291, 2.8298, 2.6095, 2.4319, 3.0329, 1.8112, 2.8789, 2.2186, 2.8467,\n                      6.2840, 2.3680, 2.4887, 2.8226, 2.7632, 2.1326, 5.1460, 3.7219, 2.6172,\n                      2.0708, 3.4292, 3.1266, 1.9669, 2.0980, 2.7806, 2.3227, 1.9057, 2.8916,\n                      2.1344, 3.0267, 3.7169, 3.5340, 2.6396, 4.0051, 2.6622, 3.1068, 2.7903,\n                      3.4209, 3.2905])),\n             ('c7.1.num_batches_tracked', tensor(3700)),\n             ('c8.0.weight',\n              tensor([[[[-3.3203e-02,  1.9328e-02, -8.9099e-02],\n                        [-5.1095e-02, -7.5371e-02, -3.9261e-02],\n                        [-6.6271e-02, -4.4234e-02, -8.9768e-02]],\n              \n                       [[ 1.0603e-01,  3.5797e-02,  8.8930e-02],\n                        [ 1.7291e-02,  1.0253e-01, -4.3698e-05],\n                        [ 7.2785e-02, -2.1523e-02,  3.5975e-02]],\n              \n                       [[ 6.6593e-02, -1.7112e-02,  1.0999e-02],\n                        [-3.2642e-02, -1.7342e-02, -1.0066e-02],\n                        [-1.0943e-01, -2.7589e-02,  5.1462e-03]],\n              \n                       ...,\n              \n                       [[ 1.0602e-01, -3.4441e-02,  5.8941e-03],\n                        [ 2.5340e-02, -6.3867e-02, -8.2345e-03],\n                        [ 4.6218e-02, -2.2394e-02, -3.1949e-02]],\n              \n                       [[ 2.2211e-02, -1.3237e-02, -1.4321e-02],\n                        [ 2.4761e-02, -2.0154e-02, -5.7335e-03],\n                        [ 5.4033e-02,  1.0226e-01,  7.0159e-02]],\n              \n                       [[-1.1335e-01,  2.8569e-02,  5.5228e-02],\n                        [-1.3761e-01,  1.1744e-01, -7.1937e-02],\n                        [ 9.0242e-03,  1.7349e-02, -2.0515e-02]]],\n              \n              \n                      [[[-2.3002e-02, -1.4203e-03, -2.6172e-02],\n                        [ 9.0537e-04, -4.3657e-02, -1.2892e-01],\n                        [ 1.6156e-02, -6.2574e-02,  2.2875e-02]],\n              \n                       [[-7.8853e-02,  6.8718e-02,  6.0378e-02],\n                        [ 6.6462e-02,  2.4526e-02, -6.3071e-02],\n                        [ 3.2444e-02,  1.1072e-01,  2.0391e-02]],\n              \n                       [[-3.6893e-02, -6.8975e-02,  5.4696e-02],\n                        [-3.4512e-02, -3.9294e-02, -3.3483e-02],\n                        [-9.1563e-02, -6.1872e-02, -4.1641e-02]],\n              \n                       ...,\n              \n                       [[-1.7274e-01, -1.4177e-02, -1.0554e-02],\n                        [ 6.4896e-02, -5.6769e-02,  1.0596e-01],\n                        [-1.8105e-02, -6.2546e-02,  4.3923e-03]],\n              \n                       [[-7.8349e-02, -6.1379e-02, -8.2682e-03],\n                        [-8.3897e-02, -9.6062e-02, -1.4177e-02],\n                        [-2.4213e-02, -1.1048e-01, -5.2715e-03]],\n              \n                       [[-2.7340e-03,  5.3416e-02, -3.0306e-03],\n                        [ 3.5051e-02,  2.9542e-02, -1.0632e-02],\n                        [-3.5724e-02,  8.9232e-02,  1.2969e-01]]],\n              \n              \n                      [[[-1.0372e-02, -6.1134e-04, -8.5094e-02],\n                        [-1.1193e-01, -3.6721e-02,  3.9005e-02],\n                        [ 3.4235e-02, -5.7986e-02,  7.4949e-02]],\n              \n                       [[ 6.3856e-02,  8.7742e-02,  4.2264e-02],\n                        [-1.7337e-03,  9.9641e-03, -4.7131e-02],\n                        [-6.9053e-02, -3.9556e-02,  1.4398e-02]],\n              \n                       [[-2.1072e-02, -2.4537e-02,  7.5436e-03],\n                        [-9.8254e-02, -4.4917e-03, -6.9790e-02],\n                        [-3.2181e-02, -4.6169e-02, -2.6706e-02]],\n              \n                       ...,\n              \n                       [[ 6.2242e-02,  1.3017e-02,  1.9952e-03],\n                        [ 5.4420e-02,  1.9686e-02, -2.4756e-02],\n                        [ 1.0101e-01, -8.9010e-03,  4.1205e-02]],\n              \n                       [[ 1.2082e-02, -8.0623e-02, -6.4553e-02],\n                        [-1.1184e-01, -4.4825e-02, -5.2670e-02],\n                        [-4.0049e-02,  3.2846e-02,  2.2617e-02]],\n              \n                       [[-4.6840e-02,  5.7086e-03,  7.1214e-02],\n                        [-1.2604e-02, -3.4818e-02, -3.0825e-02],\n                        [ 1.1588e-01,  6.8221e-03,  5.7010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.4769e-02,  4.4830e-02, -1.1197e-01],\n                        [-8.4699e-02,  5.5073e-02, -9.8267e-02],\n                        [-3.4728e-02,  4.1424e-03, -1.5014e-02]],\n              \n                       [[-3.4339e-02,  4.0525e-02, -3.4202e-02],\n                        [ 1.0732e-02,  3.8984e-02,  2.3835e-02],\n                        [ 1.8616e-02,  4.1027e-02, -6.0799e-03]],\n              \n                       [[-1.0134e-02,  1.2473e-02, -4.8652e-03],\n                        [-6.8260e-02, -4.7498e-02, -4.5790e-02],\n                        [-1.2701e-01, -9.3487e-02, -1.1455e-01]],\n              \n                       ...,\n              \n                       [[ 5.4950e-02,  4.6406e-02,  9.7101e-02],\n                        [ 5.0952e-02,  2.5915e-02,  6.5818e-02],\n                        [ 3.3739e-02, -5.3473e-02, -1.8611e-02]],\n              \n                       [[ 1.1151e-01,  7.9521e-03,  3.3321e-02],\n                        [ 2.4265e-02,  6.1016e-02,  1.4907e-01],\n                        [ 3.5667e-02,  4.1552e-02, -4.5313e-03]],\n              \n                       [[-1.6649e-02, -1.5939e-01,  4.2286e-02],\n                        [-6.1933e-02, -1.6211e-02,  2.5830e-02],\n                        [-1.0004e-01, -1.3267e-02, -6.2153e-02]]],\n              \n              \n                      [[[ 3.9065e-02, -8.5624e-02,  5.7931e-02],\n                        [ 8.3235e-02,  3.2061e-02,  2.9269e-02],\n                        [-4.1710e-04, -4.5526e-02,  4.7209e-02]],\n              \n                       [[-5.5075e-02,  3.4656e-02, -1.0259e-01],\n                        [-4.2272e-02,  2.0320e-02, -2.7235e-02],\n                        [-4.2013e-03,  2.5331e-02,  5.7062e-02]],\n              \n                       [[ 2.1327e-02, -5.6738e-03,  2.4214e-02],\n                        [-3.0551e-02, -4.7839e-02,  5.6032e-02],\n                        [-2.0006e-02,  1.2520e-02,  4.9326e-02]],\n              \n                       ...,\n              \n                       [[-2.6866e-02,  3.3474e-02, -1.7176e-03],\n                        [-9.9897e-02, -7.0353e-02, -1.1146e-01],\n                        [-9.1111e-02,  3.0471e-02,  3.0361e-03]],\n              \n                       [[-6.5768e-02, -3.6298e-02, -2.3906e-02],\n                        [-1.6202e-02, -2.9722e-02, -1.6942e-02],\n                        [ 2.7506e-03, -2.3683e-02,  2.6882e-02]],\n              \n                       [[ 1.6009e-01, -1.0219e-01, -7.7504e-02],\n                        [ 5.4617e-04, -2.3395e-02,  1.8199e-02],\n                        [-9.4497e-03,  4.0804e-02,  7.8396e-03]]],\n              \n              \n                      [[[-1.5062e-02, -5.2146e-02, -8.2106e-02],\n                        [-4.5777e-02,  2.6402e-02,  4.5825e-02],\n                        [ 9.1887e-03, -4.0498e-02, -4.6720e-02]],\n              \n                       [[ 1.4376e-02, -1.4388e-01, -1.5014e-02],\n                        [ 1.1933e-02, -6.0665e-02,  2.4498e-02],\n                        [ 2.1752e-02, -3.8432e-02,  6.7861e-02]],\n              \n                       [[-8.1831e-02, -6.5040e-02,  3.3918e-02],\n                        [ 1.4636e-02,  5.1795e-02, -4.2463e-02],\n                        [ 4.2507e-02,  1.3747e-03, -6.3689e-02]],\n              \n                       ...,\n              \n                       [[-2.5699e-02, -5.3493e-03,  4.9803e-02],\n                        [ 1.8572e-02,  6.6365e-03,  2.2045e-02],\n                        [ 1.0412e-01,  3.9512e-02,  1.0269e-02]],\n              \n                       [[ 2.9677e-03,  4.7069e-02, -2.2166e-02],\n                        [ 7.2426e-02, -8.9027e-02, -2.1068e-02],\n                        [ 1.3655e-02,  2.1149e-02, -4.8898e-02]],\n              \n                       [[ 1.2445e-02, -9.5230e-02,  2.9909e-02],\n                        [ 1.1616e-01, -4.8136e-02, -1.0362e-02],\n                        [-4.7153e-02, -3.7755e-02,  8.2177e-02]]]])),\n             ('c8.0.bias',\n              tensor([ 0.0135, -0.0266,  0.0178, -0.0056, -0.0137, -0.0192, -0.0123, -0.0044,\n                       0.0017,  0.0266,  0.0179, -0.0205, -0.0255,  0.0286, -0.0002, -0.0121,\n                      -0.0234,  0.0124, -0.0090, -0.0060,  0.0199,  0.0126, -0.0103,  0.0152,\n                      -0.0251,  0.0252, -0.0130,  0.0227, -0.0067,  0.0193, -0.0147,  0.0044,\n                      -0.0088,  0.0236, -0.0294,  0.0115,  0.0148, -0.0148,  0.0259, -0.0093,\n                       0.0254,  0.0039, -0.0282,  0.0087, -0.0264, -0.0272,  0.0201,  0.0093,\n                      -0.0172,  0.0181,  0.0208, -0.0207, -0.0123,  0.0225, -0.0076, -0.0100,\n                      -0.0052,  0.0097,  0.0229, -0.0241, -0.0069, -0.0046, -0.0161, -0.0236])),\n             ('c8.1.weight',\n              tensor([1.0299, 0.9965, 1.0014, 1.0195, 1.0045, 1.0340, 1.0175, 1.0234, 0.9985,\n                      1.0430, 1.0001, 1.0294, 0.9656, 1.0227, 1.0300, 0.9738, 0.9920, 0.9850,\n                      1.0156, 0.9955, 1.0260, 1.0103, 1.0036, 1.0159, 1.0288, 1.0097, 0.9811,\n                      1.0083, 0.9831, 1.0215, 1.0035, 0.9859, 0.9945, 0.9890, 1.0337, 0.9927,\n                      1.0463, 1.0098, 0.9891, 0.9878, 1.0032, 0.9923, 0.9815, 1.0294, 1.0332,\n                      1.0015, 0.9738, 1.0110, 1.0217, 0.9853, 1.0302, 1.0017, 1.0139, 1.0190,\n                      1.0043, 0.9958, 0.9895, 1.0137, 1.0353, 0.9998, 1.0031, 1.0385, 1.0170,\n                      0.9916])),\n             ('c8.1.bias',\n              tensor([-4.6060e-03, -3.0788e-02, -6.5173e-03,  1.1747e-02,  5.4760e-03,\n                      -3.8925e-03,  1.8713e-02, -1.5993e-02, -1.7791e-02,  1.3476e-02,\n                      -1.6329e-02,  3.1454e-03, -5.1038e-02, -6.2979e-02, -2.5789e-02,\n                      -5.9403e-02, -2.1658e-02, -2.6207e-02, -7.2635e-03, -4.0167e-02,\n                       1.1701e-04, -2.1233e-02, -8.8954e-02, -1.2592e-02,  6.9660e-03,\n                      -2.0474e-03, -6.4445e-02, -3.1054e-02, -6.8547e-02, -1.1731e-02,\n                      -6.2106e-02, -5.4339e-02, -6.3011e-02, -5.8148e-02, -1.8493e-02,\n                      -5.3882e-02,  2.3375e-02, -3.3855e-02, -5.8480e-02, -2.8599e-02,\n                       1.3070e-03, -3.2666e-02, -3.2567e-02, -1.9052e-03,  3.1853e-02,\n                      -1.8188e-02, -5.2554e-02, -4.6579e-02, -1.2478e-02, -4.1952e-02,\n                      -1.2868e-02, -2.6300e-02, -1.7183e-02, -2.7755e-02, -3.8171e-02,\n                      -2.3220e-02, -4.8779e-02, -3.3703e-02, -1.1279e-02, -9.2484e-03,\n                      -9.1810e-04, -4.7726e-03,  1.3495e-05, -4.4504e-02])),\n             ('c8.1.running_mean',\n              tensor([-0.9457, -0.7312, -2.1195, -1.6843, -1.5140, -0.0178, -0.6937, -0.5715,\n                       1.4074, -0.8530, -0.0999, -0.5785, -2.2758, -0.3785,  0.1324, -0.2624,\n                      -1.1276,  0.1362, -1.6879,  1.0184,  0.4752,  0.4136,  0.4758,  0.1667,\n                      -0.0174,  0.5151, -0.4232,  0.2541,  0.8388, -0.7913,  2.2206,  0.4264,\n                      -0.4200,  0.9839,  0.1212, -0.0166, -0.2808,  1.7108,  1.3422, -0.1721,\n                       0.3899,  1.0332, -1.0639, -0.6162, -0.9319,  2.0036, -0.1377,  0.7434,\n                       0.9839,  1.3910, -0.3544, -1.3223,  1.0127, -0.0430, -1.3595, -0.6312,\n                      -0.1122,  0.0284,  0.0051,  1.4948, -0.4903,  0.2325, -0.7360,  1.7754])),\n             ('c8.1.running_var',\n              tensor([2.4415, 3.1398, 3.1434, 2.4394, 4.4797, 2.8163, 4.4165, 2.8234, 4.2623,\n                      4.3772, 3.6774, 2.4286, 2.5554, 2.0172, 2.7751, 2.0086, 2.7575, 2.9401,\n                      3.0308, 2.3360, 2.9208, 3.9489, 2.1309, 2.4664, 2.6504, 4.4836, 3.4096,\n                      2.3308, 2.7955, 3.1042, 2.1949, 2.7582, 2.8512, 3.0816, 3.3150, 2.5111,\n                      2.3773, 2.4649, 2.8259, 3.0074, 3.2103, 2.5924, 3.4504, 3.2430, 3.5025,\n                      2.5382, 3.6869, 4.1234, 3.4483, 2.7889, 2.8835, 2.3657, 2.9396, 2.7785,\n                      2.8656, 4.2642, 2.3329, 3.9566, 2.8349, 2.4987, 2.4138, 2.6133, 3.0130,\n                      3.5772])),\n             ('c8.1.num_batches_tracked', tensor(3700)),\n             ('c9.0.weight',\n              tensor([[[[-3.7685e-02, -5.8820e-02,  5.2716e-02],\n                        [-6.4305e-02, -3.5950e-02, -5.0258e-02],\n                        [-8.5118e-03, -3.1462e-03, -4.0184e-02]],\n              \n                       [[ 7.7924e-03,  3.6853e-02,  5.5167e-02],\n                        [ 1.6511e-02, -1.2815e-02,  6.0766e-02],\n                        [-4.2259e-02, -1.8024e-02, -1.4796e-02]],\n              \n                       [[-2.8359e-02, -3.3498e-03,  2.6903e-02],\n                        [ 1.7165e-02, -9.1642e-03,  1.0844e-02],\n                        [-3.1918e-03, -3.5285e-02, -5.2257e-02]],\n              \n                       ...,\n              \n                       [[-4.7532e-02,  5.2084e-02, -3.1144e-02],\n                        [ 1.3369e-01, -3.7130e-03,  3.9419e-02],\n                        [-1.1616e-02, -2.7751e-03,  2.4027e-03]],\n              \n                       [[ 2.9618e-02,  5.8024e-02,  4.8728e-02],\n                        [ 1.6974e-03, -2.2715e-04, -3.2690e-02],\n                        [ 2.8723e-02,  3.4672e-02,  2.6194e-02]],\n              \n                       [[ 2.8114e-02,  3.4345e-02,  5.9398e-02],\n                        [ 1.0146e-01,  2.3799e-02, -1.4450e-04],\n                        [-4.5387e-02,  4.1157e-02,  5.0182e-02]]],\n              \n              \n                      [[[-7.5491e-02, -6.5586e-02, -1.7811e-02],\n                        [ 1.9802e-02, -2.2899e-02, -3.6866e-02],\n                        [-4.1220e-02, -2.2595e-02, -4.7959e-02]],\n              \n                       [[-8.0688e-02, -1.0465e-02,  1.1485e-02],\n                        [-4.4357e-03, -3.2091e-02, -5.4697e-02],\n                        [-1.0212e-02,  3.6312e-02, -6.6114e-02]],\n              \n                       [[-4.4025e-02, -3.9471e-02, -1.6619e-02],\n                        [-7.9394e-02, -3.2355e-02,  1.0778e-02],\n                        [-6.2142e-02, -7.0177e-02,  9.1336e-03]],\n              \n                       ...,\n              \n                       [[-9.2896e-03,  3.9917e-02,  1.1596e-02],\n                        [ 7.0564e-03,  2.1367e-02,  8.1181e-03],\n                        [-2.5171e-02, -4.8984e-02, -3.6533e-02]],\n              \n                       [[ 1.1508e-01,  6.3632e-02,  7.4487e-02],\n                        [-5.3803e-03, -1.9328e-02, -3.9033e-02],\n                        [ 9.0604e-03,  1.4870e-02,  3.2300e-03]],\n              \n                       [[ 2.8686e-02,  3.8950e-02,  8.4569e-03],\n                        [-2.2250e-02,  5.3220e-02,  2.3553e-02],\n                        [ 2.5159e-02,  3.7357e-02,  2.2166e-02]]],\n              \n              \n                      [[[-8.9305e-02,  2.0843e-02,  1.1200e-02],\n                        [ 1.1874e-02, -5.9548e-03, -3.0153e-02],\n                        [ 1.4565e-02, -1.0766e-01,  3.7650e-02]],\n              \n                       [[ 6.0182e-02,  3.4919e-02,  2.3258e-02],\n                        [-1.0857e-02,  6.1520e-02,  3.5332e-02],\n                        [ 8.7039e-02,  7.6412e-02,  3.0370e-02]],\n              \n                       [[ 5.0536e-02,  2.3493e-02,  3.5350e-02],\n                        [ 1.0212e-01,  7.9976e-02,  7.3731e-02],\n                        [ 2.8010e-03,  4.7671e-02,  4.8889e-02]],\n              \n                       ...,\n              \n                       [[-3.0677e-03, -2.5399e-02, -3.1056e-02],\n                        [-4.4259e-02,  4.3126e-02, -3.5506e-02],\n                        [ 8.7490e-02, -4.0665e-02,  4.2664e-02]],\n              \n                       [[ 5.8141e-02,  2.3933e-03,  5.3039e-02],\n                        [-9.8749e-04,  5.0380e-02, -9.3197e-03],\n                        [-4.5461e-02,  4.7598e-02, -6.6931e-02]],\n              \n                       [[-5.7433e-02, -4.6793e-02,  3.4462e-02],\n                        [-2.6380e-03, -4.3142e-02, -2.4059e-02],\n                        [-3.2285e-02,  1.7821e-03,  8.2213e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.0030e-02, -1.8516e-02, -5.2135e-02],\n                        [-3.5131e-02,  8.0306e-03, -7.4241e-03],\n                        [ 2.5431e-02,  2.5796e-02, -3.3932e-02]],\n              \n                       [[-4.7470e-02, -8.3195e-02, -2.6717e-02],\n                        [-4.6854e-02, -2.1650e-02,  3.1301e-03],\n                        [ 2.8201e-02, -3.0887e-02, -1.4267e-02]],\n              \n                       [[ 3.6523e-02, -1.3086e-02, -3.7986e-02],\n                        [-3.5778e-02, -1.1662e-02, -4.4269e-02],\n                        [-5.4575e-02, -3.8389e-02,  8.3497e-04]],\n              \n                       ...,\n              \n                       [[-5.4720e-02,  7.0103e-03,  3.9598e-02],\n                        [ 9.1650e-02,  3.3409e-02,  2.9386e-02],\n                        [ 2.9126e-02, -1.5797e-02,  1.5748e-02]],\n              \n                       [[ 5.5587e-02,  1.4760e-02, -1.9292e-02],\n                        [-3.9797e-02, -6.8659e-03,  1.4609e-02],\n                        [ 2.2047e-02, -1.5954e-03, -1.5935e-02]],\n              \n                       [[-1.5875e-02, -8.8851e-03, -3.6401e-03],\n                        [-2.9807e-03,  6.3152e-02,  2.1140e-02],\n                        [ 1.8217e-02,  2.5896e-02, -7.3023e-02]]],\n              \n              \n                      [[[-5.6044e-02, -5.7200e-02, -7.4171e-02],\n                        [-5.7155e-02, -2.6525e-02, -5.4253e-02],\n                        [-6.8424e-02, -3.1671e-02, -9.8644e-02]],\n              \n                       [[ 4.3135e-03, -1.1588e-02,  4.6037e-02],\n                        [-8.1055e-03, -2.8327e-02,  4.5250e-03],\n                        [-2.6670e-03,  1.8379e-02, -4.2871e-02]],\n              \n                       [[ 5.4641e-02,  1.6880e-01,  1.2498e-02],\n                        [ 3.9749e-02,  6.8314e-02,  9.0560e-02],\n                        [ 3.7839e-02,  2.9546e-02,  1.0657e-01]],\n              \n                       ...,\n              \n                       [[ 7.2466e-02,  7.0459e-03,  3.1373e-02],\n                        [ 1.7390e-03,  3.6492e-02,  1.9175e-02],\n                        [-2.4448e-02,  7.0045e-02,  3.9666e-02]],\n              \n                       [[ 3.9577e-02, -1.1742e-02,  1.9213e-02],\n                        [ 5.7887e-04,  1.4169e-03,  7.6892e-02],\n                        [ 3.3353e-02,  3.0009e-02, -1.7052e-02]],\n              \n                       [[-4.0615e-02, -3.4625e-03, -6.6102e-02],\n                        [ 6.1951e-02, -3.5361e-03, -6.4588e-02],\n                        [ 9.7675e-02,  9.3301e-02,  2.4006e-02]]],\n              \n              \n                      [[[ 3.8805e-02, -1.0608e-01, -5.6160e-02],\n                        [-3.4248e-03, -7.2465e-02, -2.3396e-02],\n                        [-4.4685e-02, -5.2861e-02, -1.7929e-02]],\n              \n                       [[ 5.4697e-03,  9.6924e-03,  5.0395e-02],\n                        [ 5.2368e-02,  7.6298e-02,  4.8471e-02],\n                        [-8.9140e-03,  8.8769e-02,  3.7697e-02]],\n              \n                       [[ 2.7538e-02, -1.6436e-02,  3.6665e-02],\n                        [-2.4885e-02, -1.4741e-02, -6.6199e-02],\n                        [ 4.6397e-02, -2.4633e-02,  4.7798e-02]],\n              \n                       ...,\n              \n                       [[-1.2633e-02,  3.3013e-02,  7.9878e-03],\n                        [ 4.5976e-02,  3.7950e-02,  5.9152e-02],\n                        [ 4.3665e-03, -1.8696e-02,  2.1340e-02]],\n              \n                       [[-4.3402e-02, -1.5719e-02, -2.0674e-02],\n                        [-4.7005e-02,  1.1825e-02, -4.3588e-02],\n                        [-6.6484e-03, -7.3466e-02, -4.5078e-02]],\n              \n                       [[-1.6005e-02, -6.6055e-03, -1.9322e-02],\n                        [-6.5225e-02,  3.1784e-02, -1.7610e-02],\n                        [-6.9328e-02,  2.1465e-02,  3.5644e-02]]]])),\n             ('c9.0.bias',\n              tensor([-0.0171, -0.0110, -0.0134,  0.0175, -0.0303,  0.0285,  0.0249, -0.0160,\n                      -0.0296, -0.0108,  0.0288, -0.0069,  0.0268, -0.0248,  0.0006, -0.0052,\n                       0.0132,  0.0096, -0.0198, -0.0222, -0.0128,  0.0145, -0.0177,  0.0039,\n                      -0.0180, -0.0169,  0.0264,  0.0010,  0.0078,  0.0175,  0.0042, -0.0151,\n                      -0.0273, -0.0203, -0.0069,  0.0265, -0.0164, -0.0282,  0.0100, -0.0185,\n                      -0.0159, -0.0233,  0.0063,  0.0211,  0.0108, -0.0206, -0.0059, -0.0175,\n                      -0.0110,  0.0131, -0.0131, -0.0132, -0.0152, -0.0273, -0.0235,  0.0240,\n                       0.0106, -0.0122,  0.0250, -0.0049,  0.0131, -0.0143,  0.0046, -0.0211,\n                       0.0161,  0.0163,  0.0209,  0.0105,  0.0221,  0.0181, -0.0144, -0.0142,\n                       0.0162,  0.0233,  0.0155, -0.0175,  0.0238, -0.0159,  0.0007, -0.0117,\n                       0.0199,  0.0296, -0.0185,  0.0241, -0.0283,  0.0016,  0.0264,  0.0179,\n                      -0.0219, -0.0195, -0.0270, -0.0075, -0.0135, -0.0207, -0.0213, -0.0288,\n                      -0.0038, -0.0274, -0.0123,  0.0048, -0.0099, -0.0096,  0.0204,  0.0138,\n                       0.0278,  0.0148,  0.0176,  0.0010,  0.0207, -0.0117,  0.0012, -0.0174,\n                       0.0094, -0.0176, -0.0121,  0.0190,  0.0039, -0.0023, -0.0207,  0.0204,\n                      -0.0110, -0.0030, -0.0075,  0.0031,  0.0178,  0.0265,  0.0056,  0.0019])),\n             ('c9.1.weight',\n              tensor([0.9855, 1.0178, 1.0088, 0.9843, 0.9903, 0.9965, 0.9740, 0.9838, 1.0291,\n                      1.0110, 0.9910, 1.0063, 1.0116, 1.0113, 1.0069, 0.9934, 0.9955, 1.0072,\n                      0.9918, 0.9818, 0.9820, 0.9741, 1.0177, 0.9952, 1.0155, 0.9901, 0.9861,\n                      1.0008, 0.9963, 0.9912, 0.9963, 0.9893, 1.0031, 0.9947, 0.9997, 0.9786,\n                      0.9958, 0.9978, 1.0001, 1.0099, 1.0027, 1.0207, 1.0178, 1.0043, 0.9977,\n                      1.0025, 0.9951, 1.0069, 0.9871, 1.0007, 1.0137, 0.9786, 1.0133, 0.9985,\n                      1.0254, 0.9926, 1.0045, 0.9974, 1.0171, 0.9931, 1.0102, 0.9647, 1.0025,\n                      1.0072, 1.0264, 1.0023, 0.9778, 1.0039, 1.0143, 1.0005, 0.9932, 1.0159,\n                      1.0123, 1.0002, 0.9615, 1.0142, 0.9925, 1.0000, 0.9872, 0.9878, 1.0166,\n                      1.0084, 1.0299, 1.0033, 0.9993, 1.0095, 0.9927, 0.9917, 0.9983, 1.0232,\n                      1.0044, 1.0047, 0.9916, 0.9828, 0.9985, 0.9860, 0.9990, 0.9933, 0.9860,\n                      0.9976, 1.0200, 1.0027, 1.0151, 0.9791, 1.0184, 0.9817, 0.9887, 1.0130,\n                      0.9879, 0.9866, 1.0078, 0.9933, 1.0013, 1.0135, 1.0145, 0.9833, 1.0280,\n                      0.9995, 1.0020, 1.0048, 1.0370, 0.9915, 0.9940, 0.9875, 0.9923, 1.0140,\n                      0.9716, 0.9916])),\n             ('c9.1.bias',\n              tensor([-1.3427e-02,  1.7878e-02,  1.4525e-02, -2.3270e-02, -3.2592e-02,\n                      -4.8375e-02, -1.0062e-02, -3.6577e-02, -9.3547e-04, -3.3559e-02,\n                      -8.5245e-03,  2.1010e-02,  2.5466e-02, -1.3262e-02, -2.1130e-02,\n                      -2.0295e-02,  2.1118e-02, -4.2197e-02, -1.6439e-02, -2.1491e-02,\n                      -2.5959e-03, -3.3066e-02,  1.7648e-02, -3.0440e-02,  4.2335e-03,\n                      -3.4551e-02, -1.7985e-02, -3.1146e-02, -4.0402e-02, -3.1452e-03,\n                       5.7634e-03, -3.0197e-02,  7.6224e-03, -8.4630e-03, -1.7209e-02,\n                      -1.4330e-02, -1.1983e-02, -9.9465e-03, -8.5070e-03, -2.7375e-02,\n                      -1.7342e-02, -1.2016e-02, -3.8531e-02,  2.1437e-02, -4.4863e-02,\n                      -1.2277e-02, -2.2921e-03,  5.8816e-03, -2.4899e-02,  3.8202e-03,\n                       2.7685e-03, -3.1538e-02, -5.7960e-02, -9.3054e-03, -2.1446e-03,\n                      -1.8286e-02, -2.9471e-04, -1.4439e-02, -1.7896e-02, -7.8949e-03,\n                      -4.9533e-04, -4.1069e-02, -1.9433e-02, -2.4432e-02, -2.4618e-02,\n                      -1.6344e-02, -5.7787e-02,  2.2095e-03, -1.7079e-02, -1.5388e-02,\n                      -4.1621e-02, -3.5076e-02,  4.5596e-03, -1.4194e-02, -2.6246e-02,\n                      -2.2684e-02, -3.9929e-02, -1.3460e-02, -1.3898e-03, -1.4982e-02,\n                       7.4833e-03, -2.0695e-02,  1.4614e-02, -2.8685e-03,  7.5803e-03,\n                      -1.8573e-02, -1.9616e-02, -5.7220e-02, -9.9284e-03, -1.8386e-02,\n                       5.3408e-03, -8.9446e-03, -3.3370e-02, -2.3045e-02, -3.7640e-03,\n                      -2.5088e-02, -4.2216e-02, -1.7110e-02, -2.8287e-02,  1.0358e-02,\n                      -1.0356e-02, -5.5245e-03, -4.1250e-03, -2.0185e-02, -2.0285e-02,\n                      -3.5031e-02, -4.3588e-02,  4.0520e-03, -2.6054e-02,  4.1927e-05,\n                      -2.7910e-02, -1.8478e-02,  1.0645e-03,  1.6866e-02, -3.0725e-03,\n                      -1.2822e-02, -2.1792e-02,  3.1027e-03, -2.3068e-02, -3.9281e-02,\n                      -3.8328e-02, -2.5239e-02, -2.4419e-02, -5.1983e-02, -3.0070e-02,\n                      -7.5279e-03, -4.2962e-02, -1.8668e-02])),\n             ('c9.1.running_mean',\n              tensor([ 0.4712, -0.9025, -1.1608, -0.9595, -0.2580,  0.4080, -2.1393,  0.0314,\n                       0.0890,  0.0169, -0.9859, -1.0464, -1.2793,  0.6360,  0.5534,  0.6218,\n                      -0.0745,  0.3363, -0.4287,  1.1710, -0.4171, -0.4846,  0.2457,  0.5145,\n                      -1.0195, -0.3419, -0.4928, -0.4053,  0.2097,  0.6962, -0.2094,  0.1781,\n                      -0.0052, -0.4663,  0.3477, -0.1427, -0.3562, -0.3438, -0.3108,  0.8052,\n                       0.4783, -0.2924,  0.3287, -0.4759,  0.5733, -0.2589, -0.4867, -1.2304,\n                       0.5477, -0.0991,  0.4839,  0.0098,  0.7153, -0.4870,  0.2943, -0.3661,\n                       0.8166,  0.0843, -0.0830, -0.8192,  0.9380, -0.1089, -0.2171, -0.6417,\n                       0.3880,  0.7299,  0.5010,  0.5154,  0.0222,  0.7902,  0.6314,  0.4311,\n                      -0.3746, -0.7235, -1.1680, -0.3336,  0.9365,  1.0410, -0.1887, -0.2586,\n                      -0.5184,  0.7250, -0.1796, -0.0809, -1.4082, -0.2661, -0.8490, -0.2864,\n                      -0.3937, -0.3434, -2.1498, -0.1538, -0.5245, -0.5361, -0.2809, -0.2729,\n                       0.9030,  1.0097, -0.2179,  0.7301, -0.0155, -0.3108,  0.1960, -0.1097,\n                       0.0138,  0.9274, -0.2250, -0.1990,  0.2301,  0.2799,  0.2643, -0.9797,\n                       0.6787,  0.2361, -0.2139,  0.7668,  0.4177, -0.9310,  0.6752,  0.1395,\n                      -0.0642, -0.5450, -0.0529,  0.1629, -0.8796, -0.2428,  0.1873,  0.1037])),\n             ('c9.1.running_var',\n              tensor([0.9984, 1.0900, 1.6491, 1.8079, 1.3034, 0.9430, 1.8823, 1.0846, 1.0116,\n                      1.1452, 1.5665, 1.2652, 1.3332, 0.9208, 1.0181, 0.9732, 1.5074, 1.3908,\n                      1.0058, 1.2346, 1.2672, 1.8895, 1.1451, 1.0935, 1.6815, 0.7740, 1.1444,\n                      1.3551, 1.2662, 1.1916, 1.3211, 0.8612, 1.1935, 1.1062, 1.1813, 1.0585,\n                      1.3984, 1.4904, 1.5345, 1.2689, 1.0067, 0.8793, 0.8377, 0.9634, 1.1583,\n                      1.3070, 0.9369, 1.2921, 0.9882, 1.6239, 0.8645, 1.4143, 1.1395, 1.1469,\n                      1.5220, 1.6811, 0.8849, 1.6597, 1.1684, 1.1178, 0.8513, 1.1192, 1.1917,\n                      1.0891, 0.9292, 0.9255, 0.7300, 1.3204, 0.9398, 0.9013, 1.6884, 0.8234,\n                      0.9462, 0.9038, 1.1184, 0.9373, 1.1521, 1.1295, 1.4083, 1.3113, 1.4564,\n                      0.9498, 0.9715, 1.1888, 1.4134, 1.2788, 1.4925, 0.8448, 0.9440, 0.7855,\n                      2.0587, 1.5174, 1.2152, 2.0568, 1.9789, 1.7987, 0.9990, 1.1133, 1.2958,\n                      1.1568, 0.9249, 1.0542, 1.3417, 0.7983, 1.6130, 1.3162, 1.0831, 1.4654,\n                      1.1092, 1.0243, 1.0943, 1.5483, 0.8733, 1.0398, 1.2159, 1.3326, 0.8307,\n                      1.8416, 0.8668, 0.8298, 1.1237, 0.9342, 0.9714, 1.0419, 1.7016, 1.1145,\n                      0.9908, 1.2125])),\n             ('c9.1.num_batches_tracked', tensor(3700)),\n             ('c10.0.weight',\n              tensor([[[[-5.9923e-02,  5.4026e-02,  2.1243e-02],\n                        [ 6.9165e-02,  1.1333e-02,  3.6595e-02],\n                        [ 6.6944e-02, -2.6939e-02,  6.9001e-02]],\n              \n                       [[ 1.5017e-02,  1.0824e-01, -8.0409e-02],\n                        [-8.8728e-02, -6.8352e-03,  2.2319e-02],\n                        [-5.0765e-02, -1.6122e-02, -4.6919e-02]],\n              \n                       [[ 4.4904e-02,  2.4997e-02, -2.5235e-02],\n                        [-6.8812e-02,  7.5464e-02, -8.2162e-02],\n                        [-2.0485e-02, -1.5877e-01, -1.0406e-01]],\n              \n                       ...,\n              \n                       [[ 3.6235e-02,  2.9985e-02, -6.0020e-02],\n                        [-1.3670e-02,  5.9810e-02,  1.3965e-01],\n                        [-3.0385e-02,  1.2194e-02,  2.6791e-02]],\n              \n                       [[ 2.1076e-02,  5.4037e-03, -1.5086e-02],\n                        [ 8.8789e-02,  1.1895e-02,  5.3539e-02],\n                        [-2.9396e-02, -1.4188e-02, -6.8762e-02]],\n              \n                       [[-1.0329e-02, -1.1172e-02, -1.0391e-01],\n                        [ 1.1122e-01,  4.6209e-02,  5.2623e-02],\n                        [ 1.0741e-01,  1.0269e-02, -2.9598e-03]]],\n              \n              \n                      [[[ 2.3712e-02,  3.9743e-02, -3.8870e-02],\n                        [ 1.0142e-01,  2.7913e-02, -2.3349e-02],\n                        [ 5.3218e-02,  2.6866e-02,  4.7981e-03]],\n              \n                       [[ 9.4026e-02,  6.4427e-02,  4.5071e-02],\n                        [ 1.0137e-01,  6.5927e-04,  1.2412e-01],\n                        [ 9.8063e-02,  9.1461e-02,  8.2821e-02]],\n              \n                       [[ 5.4861e-03,  3.1290e-02, -7.3681e-02],\n                        [-3.4717e-02,  1.1101e-01,  1.2012e-01],\n                        [-6.5078e-02,  4.8463e-02, -1.6930e-02]],\n              \n                       ...,\n              \n                       [[ 5.4568e-04,  1.1159e-01,  5.5183e-02],\n                        [ 6.9356e-02, -7.2762e-02,  1.0561e-01],\n                        [-7.8848e-02,  5.3244e-02, -4.3735e-02]],\n              \n                       [[ 3.4394e-02, -1.6976e-02, -1.7022e-01],\n                        [ 7.7161e-02, -3.9825e-02, -1.2640e-01],\n                        [-9.4222e-03, -6.4316e-03,  3.4776e-02]],\n              \n                       [[ 1.1376e-02, -3.0475e-02, -5.0406e-02],\n                        [ 6.4188e-03,  5.9333e-02, -4.2754e-02],\n                        [ 1.5411e-02,  1.6078e-02, -2.9842e-02]]],\n              \n              \n                      [[[-2.1803e-02,  3.4240e-02,  5.6223e-03],\n                        [-1.3321e-01, -4.3742e-02, -4.3016e-02],\n                        [ 2.2015e-02,  1.2407e-01, -4.9970e-03]],\n              \n                       [[-6.9523e-03,  2.1475e-02, -7.6024e-03],\n                        [-1.1000e-01, -6.6887e-03, -1.4611e-02],\n                        [ 1.7567e-02, -1.2469e-02,  1.3491e-02]],\n              \n                       [[-6.1695e-02, -9.0178e-02, -2.6609e-02],\n                        [ 5.6712e-03, -3.3104e-02,  3.4470e-03],\n                        [ 8.6375e-02, -5.2581e-03, -5.5950e-02]],\n              \n                       ...,\n              \n                       [[ 1.0370e-02,  3.3786e-02, -5.6205e-02],\n                        [-2.8433e-02,  4.2406e-02,  7.6094e-03],\n                        [-4.6533e-02, -1.7435e-02, -3.6945e-02]],\n              \n                       [[-1.2701e-01,  6.5332e-02, -7.2522e-03],\n                        [ 6.1265e-02,  6.7296e-03, -4.7279e-02],\n                        [ 5.1449e-02,  5.9155e-02, -1.1975e-01]],\n              \n                       [[ 3.0892e-02,  1.3906e-02, -1.3032e-01],\n                        [-1.4362e-01,  3.3699e-02,  2.4106e-02],\n                        [ 5.8449e-03,  3.4678e-02, -1.4755e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4769e-02, -5.0484e-03,  2.7615e-02],\n                        [-4.1872e-02, -3.2401e-02,  3.3720e-02],\n                        [-6.8874e-02, -4.8416e-02, -1.6602e-01]],\n              \n                       [[ 1.0019e-01,  1.6625e-03, -1.2016e-02],\n                        [-6.3073e-02,  1.0688e-01, -6.2852e-02],\n                        [ 3.2094e-02,  1.6121e-02,  1.0163e-01]],\n              \n                       [[ 1.7567e-02, -7.9649e-02, -4.5896e-02],\n                        [-1.3693e-02,  3.1599e-03, -9.6999e-02],\n                        [-8.0210e-02, -1.1866e-02, -4.3055e-02]],\n              \n                       ...,\n              \n                       [[ 2.9098e-02,  7.7575e-02,  7.8607e-02],\n                        [ 8.2704e-02,  5.9548e-02, -2.3653e-03],\n                        [ 2.2840e-02,  2.0312e-02, -1.1870e-02]],\n              \n                       [[ 6.9559e-02,  1.0966e-01, -8.8126e-02],\n                        [ 1.5996e-03,  5.3293e-02, -6.1238e-02],\n                        [-1.8225e-01,  5.1375e-02, -7.4145e-02]],\n              \n                       [[ 3.3737e-02,  3.6286e-02,  1.4552e-01],\n                        [ 5.5152e-03,  1.6347e-02,  1.0642e-02],\n                        [ 6.7681e-02,  8.0830e-02,  1.0836e-01]]],\n              \n              \n                      [[[ 3.0330e-02, -8.6212e-02, -3.3278e-02],\n                        [-2.3455e-02, -6.4576e-02, -7.3300e-02],\n                        [ 5.5252e-03, -1.4812e-02, -2.9356e-02]],\n              \n                       [[-6.4000e-02,  2.5039e-02,  6.2662e-02],\n                        [-3.5919e-02,  1.6853e-02,  1.2235e-01],\n                        [ 1.5174e-01,  1.4642e-01,  7.6113e-02]],\n              \n                       [[-2.0448e-02, -3.6828e-02,  3.1875e-02],\n                        [-7.6595e-02,  3.9788e-02, -4.5064e-02],\n                        [ 5.5408e-02,  6.5735e-02, -6.1540e-02]],\n              \n                       ...,\n              \n                       [[ 2.7462e-02, -5.5708e-02,  1.2992e-01],\n                        [-4.0110e-02, -5.3180e-02, -1.0959e-02],\n                        [-2.5508e-02, -3.4205e-02, -1.3885e-02]],\n              \n                       [[ 3.2591e-02,  4.5988e-02, -1.3250e-02],\n                        [ 1.2488e-01,  9.1625e-03,  3.6131e-02],\n                        [ 1.2812e-02,  1.1018e-04,  6.4809e-02]],\n              \n                       [[ 6.9099e-02, -5.8697e-02, -1.9002e-02],\n                        [ 7.8295e-02,  5.4220e-02, -1.7995e-02],\n                        [ 3.9177e-02, -4.1946e-02,  1.5332e-02]]],\n              \n              \n                      [[[-4.1994e-02, -3.3790e-02,  1.6530e-03],\n                        [ 8.2865e-03,  6.9975e-02, -1.4122e-02],\n                        [-1.0098e-01,  7.6045e-03,  4.2786e-02]],\n              \n                       [[-1.7346e-02,  1.6299e-02,  4.0365e-03],\n                        [ 6.8348e-02,  4.4370e-03,  1.0922e-01],\n                        [ 1.2473e-01, -8.4516e-02,  2.9303e-02]],\n              \n                       [[ 2.7802e-02, -3.7245e-02,  2.1063e-02],\n                        [-1.0669e-01,  3.8965e-02, -3.6471e-02],\n                        [-1.7484e-02, -1.2087e-01, -4.5034e-02]],\n              \n                       ...,\n              \n                       [[-7.1660e-02, -1.3375e-02, -4.9756e-02],\n                        [ 5.9365e-02, -1.0508e-02, -1.3339e-01],\n                        [-5.1300e-02, -1.2448e-03, -8.4269e-02]],\n              \n                       [[-4.5650e-02,  6.0341e-02, -3.2335e-02],\n                        [-1.7538e-02, -5.0251e-02, -1.0204e-01],\n                        [-4.3591e-02, -4.3695e-02,  4.0817e-02]],\n              \n                       [[ 1.2238e-02, -9.0733e-03, -1.6348e-02],\n                        [-2.5358e-02,  5.1304e-02,  2.8078e-02],\n                        [ 6.5810e-02, -8.5772e-02,  1.9840e-02]]]])),\n             ('c10.0.bias',\n              tensor([-0.0227,  0.0155,  0.0099,  0.0234,  0.0045, -0.0037,  0.0220,  0.0046,\n                      -0.0178, -0.0116, -0.0218,  0.0153, -0.0108, -0.0024, -0.0134,  0.0204,\n                       0.0058, -0.0278, -0.0146, -0.0284,  0.0130, -0.0219, -0.0129, -0.0242,\n                      -0.0037,  0.0295,  0.0028,  0.0042, -0.0218,  0.0163,  0.0224, -0.0010,\n                       0.0151, -0.0251,  0.0004,  0.0057, -0.0038, -0.0149, -0.0103,  0.0146,\n                       0.0190, -0.0236, -0.0117, -0.0196,  0.0201,  0.0078, -0.0155,  0.0164,\n                      -0.0150,  0.0166,  0.0302, -0.0139,  0.0267, -0.0206,  0.0245, -0.0018,\n                      -0.0121,  0.0301,  0.0011, -0.0182,  0.0074, -0.0124,  0.0013,  0.0101])),\n             ('c10.1.weight',\n              tensor([1.1094, 1.1028, 1.0456, 1.0851, 1.0708, 1.1120, 1.0593, 1.1179, 1.0866,\n                      1.1074, 1.1133, 1.0754, 1.0454, 1.0541, 1.0744, 1.1149, 1.0717, 1.1156,\n                      1.0672, 1.1244, 1.0827, 1.0589, 1.0893, 1.1066, 1.1166, 1.0390, 1.1212,\n                      1.0987, 1.0706, 1.0986, 1.0569, 1.0756, 1.0568, 1.0737, 1.0750, 1.0953,\n                      1.0666, 1.0371, 1.0660, 1.0866, 1.0759, 1.0854, 1.1009, 1.1149, 1.0770,\n                      1.0944, 1.0652, 1.1207, 1.1134, 1.0844, 1.0921, 1.1004, 1.0755, 1.1149,\n                      1.0526, 1.0694, 1.0538, 1.0806, 1.0789, 1.0546, 1.1220, 1.0810, 1.0567,\n                      1.0685])),\n             ('c10.1.bias',\n              tensor([0.0637, 0.0798, 0.0518, 0.0701, 0.0622, 0.1072, 0.0444, 0.0794, 0.0990,\n                      0.0611, 0.0830, 0.0651, 0.0612, 0.0466, 0.0857, 0.1130, 0.0675, 0.0528,\n                      0.0886, 0.1291, 0.0759, 0.0611, 0.0673, 0.0866, 0.0624, 0.0309, 0.0561,\n                      0.0398, 0.0862, 0.0660, 0.0385, 0.0454, 0.0307, 0.0383, 0.0653, 0.0628,\n                      0.0909, 0.0548, 0.0600, 0.0900, 0.0583, 0.0726, 0.0575, 0.0795, 0.0396,\n                      0.0848, 0.0514, 0.0954, 0.0866, 0.0645, 0.0664, 0.0706, 0.0693, 0.0959,\n                      0.0620, 0.0625, 0.0682, 0.0503, 0.0426, 0.0593, 0.0905, 0.0817, 0.0488,\n                      0.0746])),\n             ('c10.1.running_mean',\n              tensor([-0.1662,  2.3157, -0.3801, -0.0861, -0.6763,  0.7791, -0.1054,  0.3376,\n                      -0.4977, -0.6029, -0.8716, -1.8523, -2.0036, -0.7065, -0.2532, -0.4265,\n                       1.0116,  0.3280, -1.9032, -1.3236, -0.6278,  0.2794, -1.7016, -1.9263,\n                       1.2526, -0.2009,  1.3926,  1.3165, -2.2821, -0.0358,  0.1896,  0.5078,\n                       1.2092,  0.7392, -0.8846,  0.6940, -1.3573, -0.7162, -1.9406, -2.7928,\n                      -1.7818, -0.6159, -0.1942, -1.4386,  0.1870,  0.6191,  0.5192, -0.4108,\n                      -1.1286, -1.1009, -0.8768, -0.1797, -2.1505, -1.1915, -0.6732, -0.7662,\n                      -2.1979,  0.6949,  0.4889, -0.8183, -1.3912,  0.1790,  0.7328, -1.3598])),\n             ('c10.1.running_var',\n              tensor([2.5761, 4.0651, 3.2200, 3.5602, 3.5145, 4.1390, 4.1826, 3.7649, 5.3748,\n                      3.0829, 3.2731, 4.3583, 4.4216, 4.6591, 3.3987, 4.4367, 2.4367, 3.0504,\n                      5.0064, 3.4189, 3.5471, 4.8731, 3.5452, 5.8263, 2.9777, 2.8107, 3.4357,\n                      3.6197, 6.0210, 2.8288, 3.1644, 3.3620, 3.9351, 3.1117, 4.2914, 3.6362,\n                      7.7472, 4.5835, 3.1558, 3.9694, 2.7776, 3.1644, 2.6855, 3.0740, 2.5167,\n                      2.8614, 4.1057, 3.5146, 3.3990, 5.4011, 4.0266, 3.3947, 3.9315, 3.2272,\n                      4.2001, 5.7038, 4.8860, 3.5248, 4.0597, 4.9147, 5.2194, 3.9457, 3.6730,\n                      4.1410])),\n             ('c10.1.num_batches_tracked', tensor(3700)),\n             ('classifier.weight',\n              tensor([[[[-1.9342e-02, -2.3847e-02,  5.9736e-02],\n                        [-7.8515e-02,  6.4920e-02, -3.9103e-02],\n                        [-1.7710e-02, -6.9680e-02, -8.0353e-02]],\n              \n                       [[-8.6077e-03, -1.4073e-03, -8.2513e-02],\n                        [-4.1329e-02, -6.8389e-02, -6.9946e-03],\n                        [-7.6736e-02, -4.7486e-02, -9.4548e-02]],\n              \n                       [[-6.2878e-02,  7.3957e-03, -4.1728e-02],\n                        [-2.8294e-02, -2.7636e-02, -5.9882e-04],\n                        [-9.1601e-02, -1.3385e-02, -1.9117e-02]],\n              \n                       ...,\n              \n                       [[-8.6521e-02, -5.5341e-02, -5.2533e-02],\n                        [-2.3744e-02,  6.2979e-03, -6.1617e-02],\n                        [-9.8980e-02, -4.4241e-02, -8.1608e-02]],\n              \n                       [[-3.5696e-02, -3.5348e-02, -2.0169e-02],\n                        [-1.0156e-02, -1.7905e-01,  3.0644e-02],\n                        [ 1.3859e-02,  6.9365e-02, -3.4570e-02]],\n              \n                       [[-3.0219e-02,  1.3989e-02, -5.6459e-02],\n                        [-6.7173e-02, -1.6149e-01, -7.9199e-02],\n                        [-1.0198e-02, -4.4511e-02, -6.0133e-02]]],\n              \n              \n                      [[[-1.4128e-02,  6.9529e-02, -3.5340e-02],\n                        [ 3.1828e-02, -2.7275e-02, -6.3052e-02],\n                        [-4.7580e-02,  4.1992e-02,  4.9630e-02]],\n              \n                       [[-1.1500e-01, -5.0972e-02, -1.1401e-01],\n                        [-9.8984e-02,  9.0025e-03, -1.0862e-01],\n                        [-6.8905e-02, -1.5854e-01, -1.3349e-01]],\n              \n                       [[ 3.3812e-02,  5.8901e-02, -1.3615e-02],\n                        [ 6.5697e-02, -4.5933e-02,  5.4276e-02],\n                        [-2.6759e-02,  9.7597e-02,  6.8402e-02]],\n              \n                       ...,\n              \n                       [[-4.8872e-02, -5.2326e-02,  3.2742e-02],\n                        [ 1.2403e-02,  9.7855e-02,  6.5872e-02],\n                        [ 1.0784e-01, -3.0397e-02,  6.7144e-02]],\n              \n                       [[ 1.0376e-01,  3.9123e-03,  3.5694e-02],\n                        [-9.5898e-03, -1.6893e-02, -4.0487e-02],\n                        [-5.4778e-02,  1.0775e-01, -7.8215e-02]],\n              \n                       [[ 8.8255e-02,  3.3712e-02,  2.3192e-02],\n                        [ 9.5407e-03,  1.0149e-01, -5.2792e-02],\n                        [ 2.2609e-02,  8.6180e-02,  1.6402e-02]]],\n              \n              \n                      [[[ 9.5888e-02, -3.0514e-02,  1.7942e-02],\n                        [ 1.4050e-01,  4.4058e-02,  9.4541e-02],\n                        [-2.4587e-03,  2.9451e-02, -9.0349e-03]],\n              \n                       [[-2.0185e-02, -1.7198e-02, -8.3792e-02],\n                        [-2.2366e-02, -1.2457e-01, -9.0793e-02],\n                        [-3.8611e-02, -3.5049e-02, -1.2510e-01]],\n              \n                       [[-2.9702e-02, -3.8955e-02, -1.2764e-02],\n                        [-5.0700e-02, -5.0196e-02,  1.3767e-02],\n                        [-2.7992e-02,  5.7672e-02,  5.1330e-02]],\n              \n                       ...,\n              \n                       [[ 1.1575e-02,  6.4882e-02,  8.4170e-02],\n                        [ 9.0710e-03, -2.8708e-02,  8.3522e-02],\n                        [-1.3943e-02,  3.9595e-02,  1.0941e-01]],\n              \n                       [[ 2.7071e-02,  1.6129e-02,  7.0127e-02],\n                        [ 8.0951e-04, -9.1876e-03,  8.6226e-02],\n                        [-2.4693e-02,  1.5047e-01,  1.2296e-03]],\n              \n                       [[ 1.7302e-02,  7.9704e-02, -1.5519e-02],\n                        [ 2.7822e-02, -4.6223e-02,  8.5856e-02],\n                        [ 1.5663e-02,  4.9021e-02, -5.2135e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.6754e-03, -1.2387e-01, -7.1323e-02],\n                        [-1.4662e-01,  1.3699e-02, -1.3082e-01],\n                        [-1.3202e-01,  7.5499e-02,  1.0697e-02]],\n              \n                       [[-5.2024e-02, -1.0903e-01,  6.6354e-02],\n                        [ 1.0864e-02,  4.2136e-02, -5.6644e-02],\n                        [-6.6852e-02, -4.6788e-02, -5.9661e-02]],\n              \n                       [[-4.5748e-02,  2.6974e-02, -6.0722e-02],\n                        [ 8.5717e-02,  7.3185e-02, -8.1731e-02],\n                        [ 3.3716e-05,  1.0229e-02,  5.8249e-02]],\n              \n                       ...,\n              \n                       [[-1.4301e-01, -1.0878e-01, -8.7348e-02],\n                        [-3.5634e-02, -3.4958e-02, -1.1217e-01],\n                        [-8.2898e-02, -1.0470e-01, -1.4451e-01]],\n              \n                       [[ 4.8077e-04,  8.0918e-02,  9.3242e-02],\n                        [-1.7655e-02, -2.3746e-02,  1.1661e-02],\n                        [ 4.4869e-02,  2.8805e-02,  6.9251e-02]],\n              \n                       [[-4.0937e-03, -1.2291e-02, -6.7892e-02],\n                        [-1.3292e-02, -3.7055e-02, -5.1997e-02],\n                        [ 4.9614e-02,  8.2488e-02,  4.4857e-02]]],\n              \n              \n                      [[[-6.0292e-02, -3.8204e-02,  2.3466e-02],\n                        [-9.2743e-02,  9.1738e-03, -3.9848e-03],\n                        [-3.6631e-02, -7.4590e-02,  1.1042e-02]],\n              \n                       [[ 1.1612e-01,  1.3016e-02, -3.5892e-02],\n                        [ 5.6944e-03, -7.6026e-02,  2.5121e-02],\n                        [ 2.8582e-02,  2.4505e-02, -3.9413e-02]],\n              \n                       [[ 5.8413e-02,  1.0323e-02,  1.4865e-01],\n                        [ 4.0877e-02,  6.0048e-02,  2.5260e-02],\n                        [ 2.4972e-02,  3.0462e-02,  6.3645e-03]],\n              \n                       ...,\n              \n                       [[-8.4372e-02, -6.9802e-03, -3.2961e-02],\n                        [ 2.6840e-02,  7.0434e-02,  1.5018e-02],\n                        [-4.8434e-02, -1.0892e-01,  5.4624e-02]],\n              \n                       [[-8.8984e-02, -4.9794e-02, -1.2254e-01],\n                        [-9.3863e-02, -4.3967e-02, -3.8561e-02],\n                        [-3.3272e-02,  1.3450e-02, -8.2708e-02]],\n              \n                       [[-5.3024e-02, -8.3383e-02, -5.1764e-02],\n                        [-1.9735e-02, -6.6805e-02, -4.1219e-05],\n                        [-6.9051e-02,  2.9414e-02, -2.2088e-02]]],\n              \n              \n                      [[[-2.6683e-02, -4.8939e-02, -6.8845e-02],\n                        [-1.3341e-01, -1.0875e-01, -5.4463e-02],\n                        [-2.2319e-02, -4.7281e-02, -6.9313e-02]],\n              \n                       [[ 2.4488e-02, -1.8914e-02, -1.4648e-01],\n                        [-7.8806e-02, -1.1768e-02, -4.9160e-02],\n                        [-8.2040e-02, -1.0668e-01, -6.1227e-02]],\n              \n                       [[-1.8054e-02,  3.9525e-02,  6.8161e-02],\n                        [ 9.3615e-02,  2.5468e-02,  1.0261e-02],\n                        [-7.5075e-02, -4.2376e-02,  2.7002e-02]],\n              \n                       ...,\n              \n                       [[-1.5538e-02, -1.6631e-01, -1.0732e-01],\n                        [-8.6561e-02, -1.0079e-01, -1.2303e-01],\n                        [-3.6863e-02, -1.1239e-01, -7.0980e-02]],\n              \n                       [[ 3.6551e-02, -9.1412e-03,  1.4900e-02],\n                        [-3.6278e-02, -3.8438e-02,  5.5135e-02],\n                        [-7.2924e-04, -1.0000e-02,  1.3678e-02]],\n              \n                       [[ 5.4553e-02, -1.4630e-01, -1.0104e-01],\n                        [ 2.4288e-02, -9.1980e-02, -8.0216e-03],\n                        [ 1.2271e-02,  1.0427e-01, -5.2949e-02]]]])),\n             ('classifier.bias',\n              tensor([-3.9848e-02,  1.4861e-02,  9.0180e-03, -9.1016e-03,  2.7302e-02,\n                       1.6259e-02, -2.5265e-02, -2.8788e-02, -4.1594e-02,  1.4549e-02,\n                      -2.8556e-02,  9.5131e-03,  1.3410e-02, -4.5443e-02,  9.9764e-03,\n                       8.4609e-03, -9.0347e-05, -1.7850e-02, -1.8639e-02,  2.2500e-02,\n                       1.0947e-02, -2.3657e-02, -1.1657e-02, -1.5427e-02, -3.2259e-02,\n                      -3.3125e-02, -1.4474e-02,  2.8126e-02, -4.8647e-02, -9.4222e-03,\n                       5.0848e-03,  9.0123e-04,  1.8162e-02, -1.8319e-02,  2.8615e-02,\n                       4.6471e-03, -3.9188e-02, -3.6395e-02, -3.7204e-02, -2.9310e-02,\n                      -3.2509e-02, -3.0694e-02,  2.4371e-02, -4.2887e-03,  2.4692e-02,\n                       2.8395e-02, -6.3831e-02, -2.1788e-02, -5.9079e-03,  2.5031e-02,\n                      -4.5999e-02, -1.5160e-02, -4.8164e-02,  7.6481e-03, -4.2318e-02,\n                       3.2674e-03,  2.7308e-02, -3.3159e-02, -4.0706e-02, -3.3833e-02,\n                      -5.4300e-02, -1.1270e-02, -5.5991e-03, -4.4568e-03, -6.7557e-03,\n                      -5.2885e-02, -5.7473e-02, -4.4880e-02,  1.2466e-02, -1.8425e-02,\n                      -9.6867e-03, -2.0872e-02,  2.6954e-02,  3.2018e-02, -2.4609e-04,\n                       9.6188e-03,  5.6340e-03]))])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# save the traced TorchScript model (can be loaded into libtorch (C++))\n",
    "traced_model.save(torchscript_trained_model_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ONNX Conversion\n",
    "See: https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html#convert-model-to-onnx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator ::max_unpool2d to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [37], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m tile_t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(tile)\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# export it (can take a while...) -- currently causes some issues ...\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtile_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monnx_trained_model_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTrainingMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEVAL\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/__init__.py:350\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03mExports a model into ONNX format. If ``model`` is not a\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03m    model to the file ``f`` even if this is raised.\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/utils.py:163\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexport\u001B[39m(\n\u001B[1;32m    146\u001B[0m     model,\n\u001B[1;32m    147\u001B[0m     args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    160\u001B[0m     export_modules_as_functions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    161\u001B[0m ):\n\u001B[0;32m--> 163\u001B[0m     \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/utils.py:1074\u001B[0m, in \u001B[0;36m_export\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001B[0m\n\u001B[1;32m   1071\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1072\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[0;32m-> 1074\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1085\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1087\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[1;32m   1088\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1089\u001B[0m     export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n\u001B[1;32m   1090\u001B[0m )\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/utils.py:731\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[1;32m    728\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 731\u001B[0m     graph \u001B[38;5;241m=\u001B[39m \u001B[43m_optimize_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    732\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    733\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    734\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    735\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    738\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    740\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    742\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch IR graph at exception: \u001B[39m\u001B[38;5;124m\"\u001B[39m, graph)\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/utils.py:308\u001B[0m, in \u001B[0;36m_optimize_graph\u001B[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001B[0m\n\u001B[1;32m    306\u001B[0m     _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001B[1;32m    307\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[0;32m--> 308\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jit_pass_onnx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[1;32m    310\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_lint(graph)\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/__init__.py:416\u001B[0m, in \u001B[0;36m_run_symbolic_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_symbolic_function\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[0;32m--> 416\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_symbolic_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/mambaforge/envs/fairicube/lib/python3.10/site-packages/torch/onnx/utils.py:1421\u001B[0m, in \u001B[0;36m_run_symbolic_function\u001B[0;34m(g, block, n, inputs, env, operator_export_type)\u001B[0m\n\u001B[1;32m   1417\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m g\u001B[38;5;241m.\u001B[39mat(  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m   1418\u001B[0m             op_name, \u001B[38;5;241m*\u001B[39minputs, overload_name\u001B[38;5;241m=\u001B[39m_get_aten_op_overload_name(n), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattrs\n\u001B[1;32m   1419\u001B[0m         )\n\u001B[1;32m   1420\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1421\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m symbolic_registry\u001B[38;5;241m.\u001B[39mUnsupportedOperatorError(\n\u001B[1;32m   1422\u001B[0m             domain, op_name, opset_version\n\u001B[1;32m   1423\u001B[0m         )\n\u001B[1;32m   1424\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m:\n\u001B[1;32m   1425\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m operator_export_type \u001B[38;5;241m==\u001B[39m _C_onnx\u001B[38;5;241m.\u001B[39mOperatorExportTypes\u001B[38;5;241m.\u001B[39mONNX_FALLTHROUGH:\n",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m: Exporting the operator ::max_unpool2d to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub."
     ]
    }
   ],
   "source": [
    "# export the trained model to ONNX\n",
    "from torch.onnx import TrainingMode\n",
    "\n",
    "# create a tensor with example input data\n",
    "tile_index = 1\n",
    "tile = train_obs[tile_index,:,:,:].transpose(2,0,1)\n",
    "tile_t = torch.from_numpy(tile).unsqueeze(dim=0)\n",
    "\n",
    "# export it (can take a while...) -- currently causes some issues ...\n",
    "torch.onnx.export(\n",
    "    model, tile_t, onnx_trained_model_file,\n",
    "    opset_version=16,\n",
    "    verbose=True,\n",
    "    training=TrainingMode.EVAL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
